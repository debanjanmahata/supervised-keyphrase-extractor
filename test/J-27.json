{"abstract": "A sequence of prices and demands are rationalizable if there exists a concave , continuous and monotone utility function such that the demands are the maximizers of the utility function over the budget set corresponding to the price .Afriat 1 presented necessary and sufficient conditions for a finite sequence to be rationalizable .Varian 20 and later Blundell et al. 3 , 4 continued this line of work studying nonparametric methods to forecasts demand .Their results essentially characterize learnability of degenerate classes of demand functions and therefore fall short of giving a general degree of confidence in the forecast .The present paper complements this line of research by introducing a statistical model and a measure of complexity through which we are able to study the learnability of classes of demand functions and derive a degree of confidence in the forecasts .Our results show that the class of all demand functions has unbounded complexity and therefore is not learnable , but that there exist interesting and potentially useful classes that are learnable from finite samples .We also present a learning algorithm that is an adaptation of a new proof of Afriat 's theorem due to Teo and Vohra 17 .", "id": "J-27", "reader_keywords_stem": ["learn from reveal prefer", "complex problem", "forecast", "probabl approxim correct", "monoton concav util function", "demand function", "rationaliz", "finit set of observ", "observ finit set", "incom-lipschitz", "fat shatter dimens"], "combined_keywords_stem": ["learn from reveal prefer", "complex problem", "forecast", "probabl approxim correct", "monoton concav util function", "demand function", "rationaliz", "finit set of observ", "observ finit set", "incom-lipschitz", "fat shatter dimens", "reveal prefer", "machin learn", "fat shatter"], "introduction": "A market is an institution by which economic agents meet and make transactions .Classical economic theory explains the incentives of the agents to engage in this behavior through the agents ' preference over the set of available bundles indicating that agents attempt to replace their current bundle with bundles that are both more preferred and attainable if such bundles exist .The preference relation is therefore the key factor in understanding consumer behavior .One of the common assumptions in this theory is that the preference relation is represented by a utility function and that agents strive to maximize their utility given a budget constraint .This pattern of behavior is the essence of supply and demand , general equilibria and other aspects of consumer theory .Furthermore , as we elaborate in section 2 , basic observations on market demand behavior suggest that utility functions are monotone and concave .This brings us to the question , first raised by Samuelson 18 , to what degree is this theory refutable ?Given observations of price and demand , under what circumstances can we conclude that the data is consistent with the behavior of a utility maximizing agent equipped with a monotone concave utility function and subject to a budget constraint ?Samuelson gave a necessary but insufficient condition on the underlying preference known as the weak axiom of revealed preference .Uzawa 16 and Mas-Colell 10 , 11 introduced a notion of income-Lipschitz and showed that demand functions with this property are rationalizable .These properties do not require any parametric assumptions and are technically refutable , but they do assume knowledge of the entire demand function and rely heavily on the differential properties of demand functions .Hence , an infinite amount of information is needed to refute the theory .It is often the case that apart form the demand observations there is additional information on the system and it is sensible to make parametric assumptions , namely , to stipulate some functional form of utility .Consistency with utility maximization would then depend on fixing the parameters of the utility function to be consistent with the observations and with a set of equations called the Slutski equations .If such parameters exist , we conclude that the stipulated utility form is consistent with the observations .This approach is useful when there is reason to make these stipulations , it gives an explicit utility function which can be used to make precise forecasts on demand for unobserved prices .The downside of this approach is that real life data is often inconsistent with convenient functional forms .Moreover , if the observations are inconsistent it is unclear whether this is a refutation of the stipulated functional form or of utility maximization .Addressing these issues Houthakker 7 noted that an observer can see only finite quantities of data .He askes when can it be determined that a finite set of observations is consistent with utility maximization without making parametric assumptions ?He showes that rationalizability of a finite set of observations is equivalent to the strong axiom of revealed preference .Richter 15 showes that strong axiom of revealed preference is equivalent to rationalizability by a strictly concave monotone utility function .Afriat 1 gives another set of rationalizability conditions the observations must satisfy .Varian 20 introduces the generalized axiom of revealed preference GARP , an equivalent form of Afriat 's consistency condition that is easier to verify computationally .It is interesting to note that these necessary and sufficient conditions for rationalizability are essentially versions of the well known Farkas lemma 6 see also 22 .Afriat 1 proved his theorem by an explicit construction of a utility function witnessing consistency .Varian 20 took this one step further progressing from consistency to forecasting .Varian 's forecasting algorithm basically rules out bundles that are revealed inferior to observed bundles and finds a bundle from the remaining set that together with the observations is consistent with GARP .Furthermore , he introduces Samuelson 's '' money metric '' as a canonical utility function and gives upper and lower envelope utility functions for the money metric .Knoblauch 9 shows these envelopes can be computed efficiently .Varian 21 provides an up to date survey on this line of research .A different approach is presented by Blundell et al. 3 , 4 .These papers introduce a model where an agent observes prices and Engel curves for these prices .This gives an improvement on Varian 's original bounds , though the basic idea is still to rule out demands that are revealed inferior .This model is in a sense a hybrid between Mas-Colell and Afriat 's aproaches .The former requires full information for all prices , the latter for a finite number of prices .On the other hand the approach taken by Blundell et al. requires full information only on a finite number of price trajectories .The motivation for this crossover is to utilize income segmentation in the population to restructure econometric information .Different segments of the population face the same prices with different budgets , and as much as aggregate data can testify on individual preferences , show how demand varies with the budget .Applying non parametric statistical methods , they reconstruct a trajectory from the observed demands of different segments and use it to obtain tighter bounds .Both these methods would most likely give a good forecast for a fixed demand function after sufficiently many observations assuming they were spread out in a reasonable manner .However , these methods do not consider the complexity of the demand functions and do not use any probabilistic model of the observations .Therefore , they are unable to provide any estimate of the number of observations that would be sufficient for a good forecast or the degree of confidence in such a forecast .In this paper we examine the feasibility of demand forecasting with a high degree of confidence using Afriat 's conditions .We formulate the question in terms of whether the class of demand functions derived from monotone concave utilities is efficiently PAC-learnable .Our first result is negative .We show , by computing the fat shattering dimension , that without any prior assumptions , the set of all demand functions induced by monotone concave utility functions is too rich to be efficiently PAC-learnable .However , under some prior assumptions on the set of demand functions we show that the fat shattering dimension is finite and therefore the corresponding sets are PAC-learnable .In these cases , assuming the probability distribution by which the observed price-demand pairs are generated is fixed , we are in a position to offer a forecast and a probabilistic estimate on its accuracy .In section 2 we briefly discuss the basic assumptions of demand theory and their implications .In section 3 we present a new proof to Afriat 's theorem incorporating an algorithm for efficiently generating a forecasting function due to Teo and Vohra 17 .We show that this algorithm is computationally efficient and can be used as a learning algorithm .In section 4 we give a brief introduction to PAC learning including several modifications to learning real vector valued functions .We introduce the notion of fat shattering dimension and use it to devise a lower bound on the sample complexity .We also sketch results on upper bounds .In section 5 we study the learnability of demand functions and directly compute the fat shattering dimension of the class of all demand functions and a class of income-Lipschitzian demand functions with a bounded global income-Lipschitz constant .", "title": "Learning From Revealed Preference", "method": "A utility function u : Rn + + R is a function relating bundles of goods to a cardinal in a manner reflecting the preferences over the bundles .A rational agent with a budget that w.l.g equals 1 facing a price vector p E Rn + will choose from her budget set B p = x E Rn + : p \u00b7 x < 1 a bundle x E Rn + that maximizes her private utility .The first assumption we make is that the function is monotone increasing , namely , if x > y , in the sense that the inequality holds coordinatewise , then u x > u y .This reflects the assumption that agents will always prefer more of any one good .This , of course , does not necessarily hold in practice , as in many cases excess supply may lead to storage expenses or other externalities .However , in such cases the demand will be an interior point of the budget set and the less preferred bundles wo n't be observed .The second assumption we make on the utility is that all the marginals partial derivatives are monotone decreasing .This is the law of diminishing marginal utility which assumes that the larger the excess of one good over the other the less we value each additional good of one kind over the other .These assumptions imply that the utility function is concave and monotone on the observations .The demand function of the agent is the correspondence fu : Rn + + Rn + satisfying f p = argmax u x : p \u00b7 x < I In general this correspondence is not necessarily single valued , but it is implicit in the proof of Afriat 's theorem that any set of observations can be rationalized by a demand function that is single valued for unobserved prices .Since large quantities of any good are likely to create utility decreasing externalities , we assume the prices are limited to a compact set .W.l.g. we assume u has marginal utility zero outside 0 , 1 d. Any budget set that is not a subset of the support is maximized on any point outside the support and it is therefore difficult to forecast for these prices .We are thus interested in forecasts for prices below the simplex \u2206 d = conv 0 , ... , 1 , ... , 0 .For these prices we take the metricfor any p , p ~ \u2208 \u2206 d .This property reflects an assumption that preferences and demands have some sort of stability .It rules out different demands for the similar prices .We may therefore assume from here on that demand functions are single valued .A sequence of prices and demands p1 , x1 , ... , pn , xn is rationalizable if there exists a utility function u such that xi = fu pi for i = 1 , ... , n .We begin with a trivial observation , if pi \u00b7 xj \u2264 pi \u00b7 xi and xi = f pi then xi is preferred over xj since the latter is in the budget set when the former was chosen .It is therefore revealed that u xj \u2264 u xi implying pj \u00b7 xj \u2264 pj \u00b7 xi .Suppose there is a sequence pi1 , xi1 , ... , pik , xik such that pij \u00b7 xij \u2212 xij +1 \u2264 0 for j = 1 ... k \u2212 1 and pik \u00b7 xik \u2212 xi1 \u2264 0 .Then the same reasoning shows that u xi1 = u xi2 = ... = u xik implying pi1 \u00b7 xi1 \u2212 xi2 = pi2 \u00b7 xi2 \u2212 xi3 = ... = pik \u2212 1 \u00b7 xik \u2212 1 \u2212 xik = 0 .We call the latter condition the Afriat condition AC .This argument shows that AC is necessary for rationalizability ; the surprising result in Afriat 's theorem is that this condition is also sufficient .Let A be an n \u00d7 n matrix with entries aij = pi \u00b7 xj \u2212 xi aij and aji are independent , aii = 0 and let D A be the weighted digraph associated with A .The matrix satisfies AC if every cycle with negative total weight includes at least one edge with positive weight .is a concave utility function that is consistent with the observations , and from our previous remark it follows that D A satisfies AC .In the other direction it is shown by explicit construction that Afriat 's condition for D A implies L A is feasible .The construction provides a utility function that is consistent with the observations .Teo and Vohra 17 give a strongly polynomial time algorithm for this construction which will be the heart of our learning algorithm .The construction is executed in two steps .First , the algorithm finds s \u2208 Rn + such that the weighted digraph D A , s defined by the matrix \u02dcaij = siaij has no cycle with negative total weight if D A satisfies AC and returns a negative cycle otherwise .The dual of a shortest path problem is given by the constraints : yj \u2212 yi \u2264 siaij i = ~ j It is a standard result see 14 p 109 that the system is feasible iff D A , s has no negative cycles .Thus , in the second step , if D A satisfies AC , the algorithm calls a SHORTEST PATH algorithm to find y \u2208 Rn satisfying the constraints .Now we describe how to choose the si 's .Define S = i , j : aij < 0 , E = i , j : aij = 0 and T = i , j : aij > 0 and let G = n , S \u222a E be a digraph with weights wij = \u2212 1 if i , j \u2208 S and wij = 0 otherwise .D A has no negative cycles , hence G is acyclic and breadth first search can assign potentials \u03c6i such that \u03c6j \u2264 \u03c6i + wij for i , j \u2208 S \u222a E .We relabel the vertices so that \u03c61 \u2265 \u03c62 \u2265 ... \u2265 \u03c6n .LetWe show that for this choice of s , D A , s contains no negative weight cycle .Suppose C = i1 , ... , ik is a cycle in D A , s .If \u03c6 is constant on C then aijij +1 = 0 for j = 1 , ... , k and we are done .Otherwise let iv \u2208 C be the vertex with smallest potential satisfying w.l.o.g. \u03c6 iv < \u03c6 iv +1 .For any cycle C in the digraph D A , s , let v , u be an edge in C such that i v has the smallest potential among all vertices in C , and ii \u03c6u > \u03c6v .Such an edge exists , otherwise \u03c6i is identical for all vertices i in C .In this case , all edges in C have non-negative edge weight in D A , s .If iv , iv +1 \u2208 S \u222a E , then we havea contradiction .Hence iv , iv +1 \u2208 T. Now , note that all vertices q in C with the same potential as iv must be incident to an edge q , t in C such that \u03c6 t \u2265 \u03c6 q .Hence the edge q , t must have non-negative weight .i.e. , aq , t \u2265 0 .Let p denote a vertex in C with the second smallest potential .Now , C has weightAlgorithm 1 returns in polynomial time a hypothesis that is a piecewise linear function and agrees with the labeling of the observation namely sample error zero .To use this function to forecast demand for unobserved prices we need algorithm 2 which maximizes the function on a given budget set .Since u x = mini yi + sipi x \u2212 xi this is a linear program and can be solved in time polynomial in d , n as well as the size of the largest number in the input .while there exist unvisited vertices do visit new vertex j assign potential to \u03c6jreorder indices so \u03c61 \u2264 \u03c62 ... \u2264 \u03c6n for all 1 \u2264 i \u2264 n doIn a supervised learning problem , a learning algorithm is given a finite sample of labeled observations as input and is required to return a model of the functional relationship underlying the labeling .This model , referred to as a hypothesis , is usually a computable function that is used to forecast the labels of future observations .The labels are usually binary values indicating the membership of the observed points in the set that is being learned .However , we are not limited to binary values and , indeed , in the demand functions we are studying the labels are real vectors .The learning problem has three major components : estimation , approximation and complexity .The estimation problem is concerned with the tradeoff between the size of the sample given to the algorithm and the degree of confidence we have in the forecast it produces .The approximation problem is concerned with the ability of hypotheses from a certain class to approximate target functions from a possibly different class .The complexity problem is concerned with the computational complexity of finding a hypothesis that approximates the target function .A parametric paradigm assumes that the underlying functional relationship comes from a well defined family , such as the Cobb-Douglas production functions ; the system must learn the parameters characterizing this family .Suppose that a learning algorithm observes a finite set of production data which it assumes comes from a Cobb-Douglas production function and returns a hypothesis that is a polynomial of bounded degree .The estimation problem in this case would be to assess the sample size needed to obtain a good estimate of the coefficients .The approximation problem would be to assess the error sustained from approximating a rational function by a polynomial .The complexity problem would be the assessment of the time required to compute the polynomial coefficients .In the probably approximately correct PAC paradigm , the learning of a target function is done by a class of hypothesis functions , that does or does not include the target function itself ; it does not necessitate any parametric assumptions on this class .It is also assumed that the observations are generated independently by some distribution on the domain of the relation and that this distribution is fixed .If the class of target functions has finite 'd imensionality ' then a function in the class is characterized by its values on a finite number of points .The basic idea is to observe the labeling of a finite number of points and find a function from a class of hypotheses which '' tends to agree '' with this labeling .The theory tells us that if the sample is large enough then any function that '' tends to agree '' with the labeling will , with high probability , be a good approximation of the target function for future observations .The prime objective of PAC theory is to develop the relevant notion of dimensionality and to formalize the tradeoff between dimensionality , sample size and the level of confidence in the forecast .In the revealed preference setting , our objective is to use a set of observations of prices and demand to forecast demand for unobserved prices .Thus the target function is a mapping from prices to bundles , namely f : Rd + \u2192 Rd + .The theory of PAC learning for real valued functions is concerned predominantly with functions from Rd to R .In this section we introduce modifications to the classical notions of PAC learning to vector valued functions and use them to prove a lower bound for sample complexity .An upper bound on the sample complexity can also be proved for our definition of fat shattering , but we do not bring it here as the proof is much more tedious and analogous to the proof of theorem 4 .Before we can proceed with the formal definition , we must clarify what we mean by forecast and tend to agree .In the case of discrete learning , we would like to obtain a function h that with high probability agrees with f .We would then take the probability lo\u03c3 f x = h x as the measure of the quality of the estimation .Demand functions are real vector functions and we therefore do not expect f and h to agree with high probability .Rather we are content with having small mean square errors on all coordinates .Thus , our measure of estimation error is given by :For given observations S = p1 , x1 , ... , pn , xn we measure the agreement by the sample errorA sample error minimization SEM algorithm is an algorithm that finds a hypothesis minimizing erS S , h .In the case of revealed preference , there is a function that takes the sample error to zero .Nevertheless , the upper bounds theorem we use does not require the sample error to be zero .there exists an algorithm L that for a set of observations of length mL = mL \u03b5 , \u03b4 = Poly \u03b41 , 1\u03b5 finds a function h from H such that er\u03c3 f , h < \u03b5 with probability 1 \u2212 \u03b4 .There may be several learning algorithms for C with different sample complexities .The minimal mL is called the sample complexity of C. Note that in the definition there is no mention of the time complexity to find h in H and evaluating h p .A set C is efficiently PAC-learnable if there is a Poly \u03b41 , 1\u03b5 time algorithm for choosing h and evaluating h p .For discrete function sets , sample complexity bounds may be derived from the VC-dimension of the set see 19 , 8 .An analog to this notion of dimension for real functions is the fat shattering dimension .We use an adaptation of this notion to real vector valued function sets .Let \u0393 \u2282 Rd + and let C be a set of real functions from \u0393 to Rd + .We define the \u03b3 fat shattering dimension of C , denoted fatC \u03b3 as the maximal size of a \u03b3-shattered set in \u0393 .If this size is unbounded then the dimension is infinite .To demonstrate the usefulness of the this notion we use it to derive a lower bound on the sample complexity .LEMMA 2 .Suppose the functions fb : b \u2208 0 , 1 n witness the shattering of p1 , ... , pn .Then , for any x \u2208 Rd + and labels b , b ~ \u2208 0 , 1 n such that bi = ~ b ~ i either | | fb pi \u2212 x | | \u221e > \u03b32d or | | fb , pi \u2212 x | | \u221e > \u03b32d .Proof : Since the max exceeds the mean , it follows that if fb and fb , correspond to labels such that bi = ~ b ~ i thenThis implies that for any x \u2208 Rd + either | | fb pi \u2212 x | | \u221e > \u03b32d or | | fb , pi \u2212 x | | \u221e > \u03b32d \u2737 THEOREM 3 .Suppose that C is a class of functions mapping from \u0393 to Rd + .Then any learning algorithm L for C has sample complexity satisfying mL \u03b5 , \u03b4 \u2265 21 fatC 4d\u03b5 An analog of this theorem for real valued functions with a tighter bound can be found in 2 , this version will suffice for our needs .Proof : Suppose n = 21 fatC 4d\u03b5 then there exists a set \u0393S = p1 , ... , p2n that is shattered by C .It suffices to show that at least one distribution requires large sample .We construct such a distribution .Let \u03c3 be the uniform distribution on \u0393S and CS = fb : b \u2208 0 , 1 2n be the set of functions that witness the shattering of p1 ... , pn .Let fb be a function chosen uniformly at random from CS .It follows from lemma 2 with \u03b3 = 2d + that for any fixed function h the probability that | | fb p \u2212 h p | | \u221e > 2\u03b5 for p \u2208 \u0393S is at least as high as getting heads on a fair coin toss .Therefore Eb | | fb p \u2212 h p | | \u221e > 2\u03b5 .Suppose for a sequence of observations z = pi1 , x1 , ... , pin , xn a learning algorithm L finds a function h .The observation above and Fubini imply Eb er\u03c3 h , fb > \u03b5 .Randomizing on the sample space we get Eb , z er\u03c3 h , fb > \u03b5 .This shows Eh , z er\u03c3 h , fb , , > \u03b5 for some fb , , .W.l.g we may assume the error is bounded since we are looking at what is essentially a finite set therefore the probability that er\u03c3 h , fb , , > \u03b5 can not be too small , hence fb , , is not PAClearnable with a sample of size n \u2737 The following theorem gives an upper bound on the sample complexity required for learning a set of functions with finite fat shattering dimension .The theorem is proved in 2 for real valued functions , the proof for the real vector case is analogous and so omitted .Algorithm 1 is an efficient learning algorithm in the sense that it finds a hypothesis with sample error zero in time polynomial in the number of observations .As we have seen in section 4 the number of observations required to PAC learn the demand depends on the fat shattering dimension of the class of demand functions which in turn depends on the class of utility functions they are derived from .We compute the fat shattering dimension for two classes of demands .The first is the class of all demand functions , we show that this class has infinite shattering dimension we give two proofs and is therefore not PAC learnable .The second class we consider is the class of demand functions derived from utilities with bounded support and income-Lipschitz .We show that the class has a finite fat shattering dimension that depends on the support and the income-Lipschitz constant .THEOREM 5 .Let C be a set of demand functions from Rd + to Rd + thenProof 1 : For \u03b5 > 0 let pi = 2 \u2212 ip for i = 1 , ... , n be a set of price vectors inducing parallel budget sets Bi and let x1 , ... , xn be the intersection of these hyperplanes with an orthogonal line passing through the center .Let H0 and H1 be hyperplanes that are not parallel to p and let x ~ i \u2208 Bi \u2229 xi + H0 + and x ~ ~ i \u2208 Bi \u2229 xi + H1 \u2212 for i = 1 ... n see figure 1 .For any labeling b = b1 , ... , bn \u2208 0 , 1 n let y = y b = y1 , ... , yn be a set of demands such that yi = x ~ i if bi = 0 and yi = x ~ ~ i if bi = 1 we omit an additional index b in y for notational convenience .To show that p1 , ... , pn is shattered it suffices to find for every b a demand function fb supported by concave utility such that fb pi = ybi .To show that such a function exists it suffices to show that Afriat 's conditions are satisfied .Since yi are in the budgetset yi \u00b7 2 \u2212 ip = 1 , therefore pi \u00b7 yj \u2212 yi = 2j \u2212 i \u2212 1 .This shows that pi \u00b7 yj \u2212 yi \u2264 0 iff j < i hence there can be no negative cycles and the condition is met .\u2737 Proof 2 : The utility functions satisfying Afriat 's condition in the first proof could be trivial assigning the same utility to x ~ i as to x ~ ~ i .In fact , pick a utility function whose level sets are parallel to the budget constraint .Therefore the shattering of the prices p1 , ... , pn is the result of indifference rather than genuine preference .To avoid this problem we reprove the theorem by constructing utility functions u such that u x ~ i = ~ u x ~ ~ i for all i and therefore a distinct utility function is associated with each labeling .For i = 1 , ... n let pi1 , ... , pid be price vectors satisfying the following conditions :Finally let yi1 , ... , yid be points on the facets of \u039bi that intersect yi , such that | | pjr | | 1 \u00b7 | | yi \u2212 yis | | \u221e = o 1 for all j , s and r .We call the set of points yi , yi1 , ... , yid the level i demand and pi , pi1 , ... , pid level i prices .Applying H \u00a8 olders inequality we getThis shows that pir \u00b7 yjs \u2212 yir = pi \u00b7 ys \u2212 yi + o 1 = 2j \u2212 i \u2212 1 + o 1 therefore pir \u00b7 yjs \u2212 yir \u2264 0 iff j < i or i = j .This implies that if there is a negative cycle then all the points in the cycle must belong to the same level .The points of any one level lie on the facets of a polytope \u039bi and the prices pis are supporting hyperplanes of the polytope .Thus , the polytope defines a utility function for which these demands are utility maximizing .The other direction of Afriat 's theorem therefore implies there can be no negative cycles within points on the same level .It follows that there are no negative cycles for the union of observations from all levels hence the sequence of observations y1 , p1 , y11 , p11 , y12 , p12 , ... , ynd , pnd is consistent with monotone concave utility function maximization and again by Afriat 's theorem there exists u supporting a demand function fb \u2737 The proof above relies on the fact that an agent have high utility and marginal utility for very large bundles .In many cases it is reasonable to assume that the marginal for very large bundles is very small , or even that the utility or the marginal utility have compact support .Unfortunately , rescaling the previous example shows that even a compact set may contain a large shattered set .We notice however , that in this case we obtain a utility function that yield demand functions that are very sensitive to small price changes .We show that the class of utility functions that have marginal utilities with compact support and for which the relevant demand functions are income-Lipschitzian has finite fat shattering dimension .THEOREM 6 .Let C be a set of L-income-Lipschitz demand functions from \u2206 d to Rd + for some global constant L \u2208 R. ThenProof : Let p1 , ... , pn \u2208 \u2206 d be a shattered set with witnesses x1 , ... , xn \u2208 Rd + .W.l.g. xi + H0 + \u2229 xj + H0 \u2212 = \u2205 implying xi + H1 \u2212 \u2229 xj + H1 + = \u2205 , for a labeling b = b1 , ... , bn \u2208 0 , 1 n such that bi = 0 and bj = 1 , | | fb pi \u2212 fb pj | | \u221e > \u03b3 hence | | pi \u2212 pj | | \u221e > \u03b3L .A standard packing argument implies n \u2264 L\u03b3 d \u2737", "author_keywords_stem": ["reveal prefer", "machin learn", "fat shatter"]}