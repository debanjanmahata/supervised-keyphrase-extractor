{"abstract": "Online reviews have become increasingly popular as a way to judge the quality of various products and services .Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a service difficult .In this paper , we investigate underlying factors that influence user behavior when reporting feedback .We look at two sources of information besides numerical ratings : linguistic evidence from the textual comment accompanying a review , and patterns in the time sequence of reports .We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature .Second , we show that a user 's rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews .Both give us a less noisy way to produce rating estimates and reveal the reasons behind user bias .Our hypotheses were validated by statistical evidence from hotel reviews on the TripAdvisor website .", "id": "J-10", "reader_keywords_stem": ["onlin review", "reput mechan", "featur-by-featur estim of qualiti", "absenc of clear incent", "clear incent absenc", "util of the product", "product util", "brag-and-moan model", "rate", "great probabl bi-modal", "u-shape distribut", "semant orient of product evalu", "correl", "larg span of time"], "combined_keywords_stem": ["onlin review", "reput mechan", "featur-by-featur estim of qualiti", "absenc of clear incent", "clear incent absenc", "util of the product", "product util", "brag-and-moan model", "rate", "great probabl bi-modal", "u-shape distribut", "semant orient of product evalu", "correl", "larg span of time"], "introduction": "The spread of the internet has made it possible for online feedback forums or reputation mechanisms to become an important channel for Word-of-mouth regarding products , services or other types of commercial interactions .Numerous empirical studies 10 , 15 , 13 , 5 show that buyers seriously consider online feedback when making purchasing decisions , and are willing to pay reputation premiums for products or services that have a good reputation .Recent analysis , however , raises important questions regarding the ability of existing forums to reflect the real quality of a product .In the absence of clear incentives , users with a moderate outlook will not bother to voice their opinions , which leads to an unrepresentative sample of reviews .For example , 12 , 1 show that Amazons ratings of books or CDs follow with great probability bi-modal , U-shaped distributions where most of the ratings are either very good , or very bad .Controlled experiments , on the other hand , reveal opinions on the same items that are normally distributed .Under these circumstances , using the arithmetic mean to predict quality as most forums actually do gives the typical user an estimator with high variance that is often false .Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users .Hu et al. 12 propose the `` Brag-and-Moan Model '' where users rate only if their utility of the product drawn from a normal distribution falls outside a median interval .The authors conclude that the model explains the empirical distribution of reports , and offers insights into smarter ways of estimating the true quality of the product .In the present paper we extend this line of research , and attempt to explain further facts about the behavior of users when reporting online feedback .Using actual hotel reviews from the TripAdvisor2 website , we consider two additional sources of information besides the basic numerical ratings submitted by users .The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings .We use text-mining techniques similar to 7 and 3 , however , we are only interested in identifying what aspects of the service the user is discussing , without computing the semantic orientation of the text .We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature .Intuitively , lengthy comments reveal the importance of the feature to the user .Since people tend to be more knowledgeable in the aspects they consider important , users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature .Second we investigate the relationship between a reviewfor a popular Boston hotel .Name of hotel and advertisements were deliberatively erased .and the reviews that preceded it .A perusal of online reviews shows that ratings are often part of discussion threads , where one post is not necessarily independent of other posts .One may see , for example , users who make an effort to contradict , or vehemently agree with , the remarks of previous users .By analyzing the time sequence of reports , we conclude that past reviews influence the future reports , as they create some prior expectation regarding the quality of service .The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service 17 , 18 , 16 , 21 which will later reflect in the user 's rating .We propose a model that captures the dependence of ratings on prior expectations , and validate it using the empirical data we collected .Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews .Our first result can be used to determine a featureby-feature estimate of quality , where for each feature , a different subset of reviews i.e. , those with lengthy comments of that feature is considered .The second leads to an algorithm that outputs a more precise estimate of the real quality .", "title": "Understanding User Behavior in Online Feedback Reporting", "method": "We use in this paper real hotel reviews collected from the popular travel site TripAdvisor .TripAdvisor indexes hotels from cities across the world , along with reviews written by travelers .Users can search the site by giving the hotel 's name and location optional .The reviews for a given hotel are displayed as a list ordered from the most recent to the oldest , with 5 reviews per page .The reviews contain :Below the name of the hotel , TripAdvisor displays the address of the hotel , general information number of rooms , number of stars , short description , etc , the average overall rating , the TripAdvisor ranking , and an average rating for each feature .Figure 1 shows the page for a popular Boston hotel whose name along with advertisements was explicitly erased .We selected three cities for this study : Boston , Sydney and Las Vegas .For each city we considered all hotels that had at least 10 reviews , and recorded all reviews .Table 1 presents the number of hotels considered in each city , the total number of reviews recorded for each city , and the distribution of hotels with respect to the star-rating as available on the TripAdvisor site .Note that not all hotels have a star-rating .For each review we recorded the overall rating , the textual review title and body of the review and the numerical rating on 7 features : Rooms R , Service S , Cleanliness C , Value V , Food F , Location L and Noise N .TripAdvisor does not require users to submit anything other than the overall rating , hence a typical review rates few additional features , regardless of the discussion in the textual comment .Only the features Rooms R , Service S , Cleanliness C and Value V are rated by a significant number of users .However , we also selected the features Food F , Location L and Noise N because they are referred to in a significant number of textual comments .For each feature we record the numerical rating given by the user , or 0 when the rating is missing .The typical length of the textual comment amounts to approximately 200 words .All data was collected by crawling the TripAdvisor site in September 2006 .We will formally refer to a review by a tuple r , T where :rf \u2208 0 , 1 , ... 5 for the features f \u2208 F = O , R , S , C , V , F , L , N ; note that the overall rating , rO , is abusively recorded as the rating for the feature Overall O ;Reviews are indexed according to the variable i , such that ri , Ti is the ith review in our database .Since we do n't record the username of the reviewer , we will also say that the ith review in our data set was submitted by user i .When we need to consider only the reviews of a given hotel , h , we will use ri h , Ti h to denote the ith review about the hotel h.The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers .The text may , for example , reveal concrete examples of aspects that the user liked or disliked , thus justifying some of the high , respectively low ratings for certain features .The text may also offer guidelines for understanding the preferences of the reviewer , and the weights of different features when computing an overall rating .The problem , however , is that free textual comments are difficult to read .Users are required to scroll through many reviews and read mostly repetitive information .Significant improvements would be obtained if the reviews were automatically interpreted and aggregated .Unfortunately , this seems a difficult task for computers since human users often use witty language , abbreviations , cultural specific phrases , and the figurative style .Nevertheless , several important results use the textual comments of online reviews in an automated way .Using well established natural language techniques , reviews or parts of reviews can be classified as having a positive or negative semantic orientation .Pang et al. 2 classify movie reviews into positive/negative by training three different classifiers Naive Bayes , Maximum Entropy and SVM using classification features based on unigrams , bigrams or part-of-speech tags .Dave et al. 4 analyze reviews from CNet and Amazon , and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams .This result is challenged by Cui et al. 3 who look at large collections of reviews crawled from the web .They show that the size of the data set is important , and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams .Hu and Liu 11 also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers .They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features .Popescu and Etzioni 20 analyze a similar setting , but use search engine hit-counts to identify product attributes ; the semantic orientation is assigned through the relaxation labeling technique .Ghose et al. 7 , 8 analyze seller reviews from the Amazon secondary market to identify the different dimensions e.g. , delivery , packaging , customer support , etc. of reputation .They parse the text , and tag the part-of-speech for each word .Frequent nouns , noun phrases and verbal phrases are identified as dimensions of reputation , while the corresponding modifiers i.e. , adjectives and adverbs are used to derive numerical scores for each dimension .The enhanced reputation measure correlates better with the pricing information observed in the market .Pavlou and Dimoka 19 analyze eBay reviews and find that textual comments have an important impact on reputation premiums .Our approach is similar to the previously mentioned works , in the sense that we identify the aspects i.e. , hotel features discussed by the users in the textual reviews .However , we do not compute the semantic orientation of the text , nor attempt to infer missing ratings .We define the weight , wif , of feature f E F in the text Ti associated with the review ri , Ti , as the fraction of Ti dedicated to discussing aspects both positive and negative related to feature f .We propose an elementary method to approximate the values of these weights .For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f .The initial words were selected from reading some of the reviews , and seeing what words coincide with discussion of which features .The list was then extended by adding all thesaurus entries that were related to the initial words .Finally , we brainstormed for missing words that would normally be associated with each of the features .Let Lf nTi be the list of terms common to both Lf and Ti .Each term of Lf is counted the number of times it appears in T i , with two exceptions :ture is particularly strong .These were ' root ' words for each feature e.g. , 's taff ' is a root word for the feature Service , and were weighted either 2 or 3 .Each feature was assigned up to 3 such root words , so almost all words are counted only once .The list of words for the feature Rooms is given for reference in Appendix A .The weight wif is computed as :f \u2208 , ILf n TiI where JLf nTiJ is the number of terms common to Lf and T i .The weight for the feature Overall was set to min | T i | 5000 , 1 where JTiJ is the number of character in T i .The following is a TripAdvisor review for a Boston hotel the name of the hotel is omitted : '' I 'll start by saying thatThis numerical ratings associated to this review are rO = 3 , rR = 3 , rS = 3 , rC = 4 , rV = 2 for features Overall O , Rooms R , Service S , Cleanliness C and Value V respectively .The ratings for the features Food F , Location L and Noise N are absent i.e. , rF = rL = rN = 0 .The weights wf are computed from the following lists of common terms :The root words 'S taff ' and ' Center ' were tripled and doubled respectively .The overall weight of the textual review is wO = 0.197 .These values account reasonably well for the weights of different features in the discussion of the reviewer .One point to note is that some terms in the lists Lf possess an inherent semantic orientation .For example the word ' grime ' belonging to the list LC would be used most often to assert the presence , and not the absence of grime .This is unavoidable , but care was taken to ensure words from both sides of the spectrum were used .For this reason , some lists such as LR contain only nouns of objects that one would typically describe in a room see Appendix A .The goal of this section is to analyse the influence of the weights wif on the numerical ratings rif .Intuitively , users who spent a lot of their time discussing a feature f i.e. , wif is high had something to say about their experience with regard to this feature .Obviously , feature f is important for user i .Since people tend to be more knowledgeable in the aspects they consider important , our hypothesis is that the ratings rif corresponding to high weights wif constitute a subset of `` expert '' ratings for feature f. Figure 2 plots the distribution of the rates ri h C with respect to the weights wi h C for the cleanliness of a Las Vegas hotel , h. Here , the high ratings are restricted to the reviews that discuss little the cleanliness .Whenever cleanliness appears in the discussion , the ratings are low .Many hotels exhibit similar rating patterns for various features .Ratings corresponding to low weights span the whole spectrum from 1 to 5 , while the ratings corresponding to high weights are more grouped together either around good or bad ratings .We therefore make the following hypothesis : HYPOTHESIS 1 .The ratings rif corresponding to the reviews where wif is high , are more similar to each other than to the overall collection of ratings .To test the hypothesis , we take the entire set of reviews , and feature by feature , we compute the standard deviation of the ratings with high weights , and the standard deviation of the entire set of ratings .High weights were defined as those belonging to the upper 20 % of the weight range for the corresponding feature .If Hypothesis 1 were true , the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights .We use a standard T-test to measure the significance of the results .City by city and feature by feature , Table 2 presents the average standard deviation of all ratings , and the average standard deviation of ratings with high weights .Indeed , the ratings with high weights have lower standard deviation , and the results are significant at the standard 0.05 significance threshold although for certain cities taken independently there does n't seem to be a significant difference , the results are significant for the entire data set .Please note that only the features O , R , S , C and V were considered , since for the others F , L , and N we did n't have enough ratings .Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users , it also suggests some ways of computing better quality estimates .We can , for example , construct a feature-by-feature quality estimate with much lower variance : for each feature we take the subset of reviews that amply discuss that feature , and output as a quality estimate the average rating for this subset .Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set .Given that , indeed , high weights are indicators of `` expert '' opinions , the estimates obtained in this way are more accurate than the current ones .Nevertheless , the validation of this underlying assumption requires further controlled experiments .Two important assumptions are generally made about reviews submitted to online forums .The first is that ratings truthfully reflect the quality observed by the users ; the second is that reviews are independent from one another .While anecdotal evidence 9 , 22 challenges the first assumption3 , in this section , we address the second .A perusal of online reviews shows that reviews are often part of discussion threads , where users make an effort to contradict , or vehemently agree with the remarks of previous users .Consider , for example , the following review : '' I do n't understand the negative reviews ... the hotel was a little dark , but that was the style .It was very artsy .Yes it was close to the freeway , but in my opinion the sound of an occasional loud car is better than hearing the '' ding ding '' of slot machines all night !The staff on-hand is FABULOUS .The waitresses are great and *** does not deserve the bad review she got , she was 100 % attentive to us !, the bartenders are friendly and professional at the same time ... '' Here , the user was disturbed by previous negative reports , addressed these concerns , and set about trying to correct them .Not surprisingly , his ratings were considerably higher than the average ratings up to this point .It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel , or before writing a review .Past reviews create some prior expectation regarding the quality of service , and this expectation has an influence on the submitted review .We believe this observation holds for most online forums .The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation , a fact confirmed by an important line of econometric and marketing research 17 , 18 , 16 , 21 .The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums 6 .We define the prior expectation of user i regarding the feature f , as the average of the previously available ratings on the feature f4 :As a first hypothesis , we assert that the rating rif is a function of the prior expectation ef i :We define high and low expectations as those that are above , respectively below a certain cutoff value 0 .The set of reviews preceded by high , respectively low expectationsare defined as follows :These sets are specific for each hotel , feature pair , and in our experiments we took 0 = 4 .This rather high value is close to the average rating across all features across all hotels , and is justified by the fact that our data set contains mostly high quality hotels .For each city , we take all hotels and compute the average ratings in the sets Rhighf and Rlow f see Table 3 .The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations .As further evidence , we consider all hotels for which the function eV i the expectation for the feature Value has a high value greater than 4 for some i , and a low value less than 4 for some other i. Intuitively , these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews : i.e. , the cumulative average of ratings was at some point high and afterwards became low , or vice-versa .Such variations are observed for about half of all hotels in each city .Figure 3 plots the median across considered hotels rating , rV , when ef i is not more than x but greater than x \u2212 0.5 .There are two ways to interpret the function ef i : \u2022 The expected value for feature f obtained by user i before his experience with the service , acquired by reading reports submitted by past users .In this case , an overly high value for ef i would drive the user to submit a negative report or vice versa , stemming from the difference between the actual value of the service , and the inflated expectation of this value acquired before his experience .\u2022 The expected value of feature f for all subsequent visitors of the site , if user i were not to submit a report .In this case , the motivation for a negative report following an overly high value of ef is different : user i seeks to correct the expectation of future visitors to the site .Unlike the interpretation above , this does not require the user to derive an a priori expectation for the value of f. Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i .There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports : a low value for ef i can influence user i to submit a low rating for feature f because , for example , he fears that submitting a high rating will make him out to be a person with low standards5 .This , at first , appears to contradict Hypothesis 2 .However , this conformity rating can not continue indefinitely : once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception .Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef i .In particular , we examine the following hypothesis : HYPOTHESIS 3 .When a large proportion of the text of a review discusses a certain feature , the difference between the rating for that feature and the average rating up to that point tends to be large .The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature , his opinion differs from the collective opinion of previous postings .This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion , with particular strength if this opinion differs from what he perceives to be the general opinion .To test Hypothesis 3 we measure the average absolute difference between the expectation ef i and the rating rif when the weight wif is high , respectively low .Weights are classified high or low by comparing them with certain cutoff values : wif is low if smaller than 0.1 , while wif is high if greater than 0f .Different cutoff values were used for different features : 0R = 0.4 , 0S = 0.4 , 0C = 0.2 , and 0V = 0.7 .Cleanliness has a lower cutoff since it is a feature rarely discussed ; Value has a high cutoff for the opposite reason .Results are presented in Table 4 .This demonstrates that when weights are unusually high , users tend to express an opinion that does not conform to the net average of previous ratings .As we might expect , for a feature that rarely was a high weight in the discussion , e.g. , cleanliness the difference is particularly large .Even though the difference in the feature Value is quite large for Sydney , the P-value is high .This is because only few reviews discussed value heavily .The reason could be cultural or because there was less of a reason to discuss this feature .Previous models suggest that users who are not highly opinionated will not choose to voice their opinions 12 .In this section , we extend this model to account for the influence of expectations .The motivation for submitting feedback is not only due to extreme opinions , but also to the difference between the current reputation i.e. , the prior expectation of the user and the actual experience .Such a rating model produces ratings that most of the time deviate from the current average rating .The ratings that confirm the prior expectation will rarely be submitted .We test on our data set the proportion of ratings that attempt to `` correct '' the current estimate .We define a deviant rating as one that deviates from the current expectation by at least some threshold 0 , i.e. , | rif \u2212 ef i | \u2265 0 .For each of the three considered cities , the following tables , show the proportion of deviant ratings for 0 = 0.5 and 0 = 1 .The above results suggest that a large proportion of users close to one half , even for the high threshold value 0 = 1 deviate from the prior average .This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature .Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal , U-shaped distributions .To account for the observations described in the previous sections , we propose a model for the behavior of the users when submitting online reviews .For a given hotel , we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the `` objective '' quality offered by the hotel on the feature f .The rating submitted by user i on feature f is :where :The second term of Eq .2 encodes the bias of the rating .The higher the distance between the true observation vif and the function ef , the higher the bias .We use the data set of TripAdvisor reviews to validate the behavior model presented above .We split for convenience the rating values in three ranges : bad B = 1 , 2 , indifferent I = 3 , 4 , and good G = 5 , and perform the following two tests :ings , we try to classify them as either good or bad .For every hotel we take the sequence of reports , and for each report regardless of it value we classify it as being good or bad However , to perform these tests , we need to estimate the objective value , vf , that is the average of the true quality observations , vif .The algorithm we are using is based on the intuition that the amount of conformity rating is minimized .In other words , the value vf should be such that as often as possible , bad ratings follow expectations above vf and good ratings follow expectations below vf .Formally , we define the sets :that correspond to irregularities where even though the expectation at point i is lower than the delivered value , the rating is poor , and vice versa .We define vf as the value that minimize these union of the two sets :In Eq .2 we replace vif by the value vf computed in Eq .3 , and use the following distance function : \u2713 d vf , ef i | wi f = | vf \u2212 ef i | | vf 2 \u2212 ef i 2 | \u00b7 1 + 2wif ; vf \u2212 ef i The constant c \u2208 I was set to min max ef i , 3 , 4 .The values for \u03b4f were fixed at 0.7 , 0.7 , 0.8 , 0.7 , 0.6 for the features Overall , Rooms , Service , Cleanliness , Value respectively .The weights are computed as described in Section 3 .As a first experiment , we take the sets of `` extremal '' ratings rif | rif \u2208 / I for each hotel and feature .For every such rating , rif , we try to estimate it by computing \u02c6rif using Eq .2 .We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features :Table 7 presents the ratio between the root mean square error RMSE when using \u02c6rif and \u00af rf to estimate the actual ratings .In all cases the estimate produced by our model is better than the simple average .As a second experiment , we try to distinguish the sets Bf = i | rif \u2208 B and Gf = i | rif \u2208 G of bad , respectively good ratings on the feature f. For example , we compute the set Bf using the following classifier called \u03c3 : rif E Bf \u03c3f i = 1 q \u02c6rif < 4 ; Tables 8 , 9 and 10 present the Precision p , Recall r andjority classifier , \u03c4 , \u03c4f i = 1 \u21d4 | Bf | \u2265 | Gf | : We see that recall is always higher for \u03c3 and precision is usually slightly worse .For the s metric \u03c3 tends to add a1-20 % improvement over \u03c4 , much higher in some cases for hotels in Sydney .This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare .Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city , but it is still outperformed by around 80 % .The goal of this paper is to explore the factors that drive a user to submit a particular rating , rather than the incentives that encouraged him to submit a report in the first place .For that we use two additional sources of information besides the vector of numerical ratings : first we look at the textual comments that accompany the reviews , and second we consider the reports that have been previously submitted by other users .Using simple natural language processing algorithms , we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review , and the noise present in the numerical rating .Specifically , it seems that users who discuss amply a certain feature are likely to agree on a common rating .This observation allows the construction of feature-by-feature estimators of quality that have a lower variance , and are hopefully less noisy .Nevertheless , further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality .Second , we emphasize the dependence of ratings on previous reports .Previous reports create an expectation of quality which affects the subjective perception of the user .We validate two facts about the hotel reviews we collected from TripAdvisor : First , the ratings following low expectations where the expectation is computed as the average of the previous reports are likely to be higher than the ratingsfollowing high expectations .Intuitively , the perception of quality and consequently the rating depends on how well the actual experience of the user meets her expectation .Second , we include evidence from the textual comments , and find that when users devote a large fraction of the text to discussing a certain feature , they are likely to motivate a divergent rating i.e. , a rating that does not conform to the prior expectation .Intuitively , this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion .We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users .The final rating depends , as expected , on the true observation , and on the gap between the observation and the expectation .The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature .The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted .One assumption that we make is about the existence of an objective quality value vf for the feature f .This is rarely true , especially over large spans of time .Other explanations might account for the correlation of ratings with past reports .For example , if ef i reflects the true value of f at a point in time , the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly .However , the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one .Our approach to approximate this elusive ' objective value ' is by no means perfect , but conforms neatly to the idea behind the model .A natural direction for future work is to examine concrete applications of our results .Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior .Exactly how different factors affect the decisions of the users is not clear .The answer might depend on the particular application , context and culture .All words serve as prefixes : room , space , interior , decor , ambiance , atmosphere , comfort , bath , toilet , bed , building , wall , window , private , temperature , sheet , linen , pillow , hot , water , cold , water , shower , lobby , furniture , carpet , air , condition , mattress , layout , design , mirror , ceiling , lighting , lamp , sofa , chair , dresser , wardrobe , closet", "author_keywords_stem": ["onlin review", "reput mechan"]}