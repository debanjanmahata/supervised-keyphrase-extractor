{"abstract": "I n t hi s paper w e s t udy t er m based f eedback f or i nf or mat i on r etrieval in the language modeling approach .With term feedback a user directly judges the relevance of individual terms without interaction with feedback documents , taking full control of the query expansion process .We propose a cluster-based method for selecting terms to present to the user for judgment , as well as effective algorithms for constructing refined query language models from user term feedback .Our algorithms are shown to bring significant improvement in retrieval accuracy over a non-feedback baseline , and achieve comparable performance to relevance feedback .They are helpful even when there are no relevant documents in the top .", "id": "H-25", "conclusions": "In this paper we studied the use of term feedback for interactive information retrieval in the language modeling approach .We proposed a cluster-based method for selecting presentation terms as well as algorithms to estimate refined query models from user term feedback .We saw significant improvement in retrieval accuracy brought by term feedback , in spite of the fact that a user often makes mistakes in relevance judgment that hurts its performance .We found the best-performing algorithm to be TCFB , which benefits from the combination of directly observed term evidence with TFB and indirectly learned cluster relevance with CFB .When we reduced the number of presentation terms , term feedback is still able to keep much of its performance gain over the baseline .Finally , we compared term feedback to document-level relevance feedback , and found that TCFB3C 's performance is on a par with the latter with 5 feedback documents .We regarded term feedback as a viable alternative to traditional relevance feedback , especially when there are no relevant documents in the top .We propose to extend our work in several ways .First , we want to study whether the use of various contexts can help the user to better identify term relevance , while not sacrificing the simplicity and compactness of term feedback .Second , currently all terms are presented to the user in a single batch .We could instead consider iterative term feedback , by presenting a small number of terms first , and show more terms after receiving user feedback or stop when the refined query is good enough .The presented terms should be selected dynamically to maximize learning benefits at any moment .Third , we have plans to incorporate term feedback into our UCAIR toolbar 20 , an Internet Explorer plugin , to make it work for web search .We are also interested in studying how to combine term feedback with relevance feedback or implicit feedback .We could , for example , allow the user to dynamically modify terms in a language model learned from feedback documents .", "reader_keywords_stem": ["term-base feedback", "inform retriev", "languag model", "queri expans process", "queri model", "interact adhoc search", "retriev perform", "probabl", "kl-diverg", "present term"], "combined_keywords_stem": ["term-base feedback", "inform retriev", "languag model", "queri expans process", "queri model", "interact adhoc search", "retriev perform", "probabl", "kl-diverg", "present term", "queri expans", "interact retriev"], "introduction": "In the language modeling approach to information retrieval , feedback is often modeled as estimating an improved query model or relevance model based on a set of feedback documents 25 , 13 .This is in line with the traditional way of doing relevance feedback presenting a user with documents/passages for relevance judgment and then extracting terms from the judged documents or passages to expand the initial query .It is an indirect way of seeking user 's assistance for query model construction , in the sense that the refined query model based on terms is learned through feedback documents/passages , which are high-level structures of terms .It has the disadvantage that irrelevant terms , which occur along with relevant ones in the judged content , may be erroneously used for query expansion , causing undesired effects .For example , for theTREC query `` Hubble telescope achievements '' , when a relevant document talks more about the telescope 's repair than its discoveries , irrelevant terms such as `` spacewalk '' can be added into the modified query .We can consider a more direct way to involve a user in query model improvement , without an intermediary step of document feedback that can introduce noise .The idea is to present a reasonable number of individual terms to the user and ask him/her to judge the relevance of each term or directly specify their probabilities in the query model .This strategy has been discussed in 15 , but to our knowledge , it has not been seriously studied in existing language modeling literature .Compared to traditional relevance feedback , this term-based approach to interactive query model refinement has several advantages .First , the user has better control of the final query model through direct manipulation of terms : he/she can dictate which terms are relevant , irrelevant , and possibly , to what degree .This avoids the risk of bringing unwanted terms into the query model , although sometimes the user introduces low-quality terms .Second , because a term takes less time to judge than a document 's full text or summary , and as few as around 20 presented terms can bring significant improvement in retrieval performance as we will show later , term feedback makes it faster to gather user feedback .This is especially helpful for interactive adhoc search .Third , sometimes there are no relevant documents in the top N of the initially retrieved results if the topic is hard .This is often true when N is constrained to be small , which arises from the fact that the user is unwilling to judge too many documents .In this case , relevance feedback is useless , as no relevant document can be leveraged on , but term feedback is still often helpful , by allowing relevant terms to be picked from irrelevant documents .During our participation in the TREC 2005 HARD Track and continued study afterward , we explored how to exploit term feedback from the user to construct improved query models for information retrieval in the language modeling approach .We identified two key subtasks of term-based feedback , i.e. , pre-feedback presentation term selection and post-feedback query model construction , with effective algorithms developed for both .We imposed a secondary cluster structure on terms and found that a cluster view sheds additional insight into the user 's information need , and provides a good way of utilizing term feedback .Through experiments we found that term feedback improves significantly over the nonfeedback baseline , even though the user often makes mistakes in relevance judgment .Among our algorithms , the one with best retrieval performance is TCFB , the combination of TFB , the direct term feedback algorithm , and CFB , the cluster-based feedback algorithm .We also varied the number of feedback terms and observed reasonable improvement even at low numbers .Finally , by comparing term feedback with document-level feedback , we foundit to be a viable alternative to the latter with competitive retrieval performance .The rest of the paper is organized as follows .Section 2 discusses some related work .Section 4 outlines our general approach to term feedback .We present our method for presentation term selection in Section 3 and algorithms for query model construction in Section 5 .The experiment results are given in Section 6 .Section 7 concludes this paper .", "title": "Term Feedback for Information Retrieval with Language Models", "method": "We follow the language modeling approach , and base our method on the KL-divergence retrieval model proposed in 25 .With this model , the retrieval task involves estimating a query language model 0 , from a given query , a document language model 0d from each document , and calculating their KL-divergence D 0 , | | 0d , which is then used to score the documents .25 treats relevance feedback as a query model re-estimation problem , i.e. , computing an updated query model 0 , given the original query text and the extra evidence carried by the judged relevant documents .We adopt this view , and cast our task as updating the query model from user term feedback .There are two key subtasks here : First , how to choose the best terms to present to the user for judgment , in order to gather maximal evidence about the user 's information need .Second , how to compute an updated query model based on this term feedback evidence , so that it captures the user 's information need and translates into good retrieval performance .Proper selection of terms to be presented to the user for judgment is crucial to the success of term feedback .If the terms are poorly chosen and there are few relevant ones , the user will have a hard time looking for useful terms to help clarify his/her information need .If the relevant terms are plentiful , but all concentrate on a single aspect of the query topic , then we will only be able to get feedback on that aspect and missing others , resulting in a breadth loss in retrieved results .Therefore , it is important to carefully select presentation terms to maximize expected gain from user feedback , i.e. , those that can potentially reveal most evidence of the user 's information need .This is similar to active feedback 21 , which suggests that a retrieval system should actively probe the user 's information need , and in the case of relevance feedback , the feedback documents should be chosen to maximize learning benefits e.g. diversely so as to increase coverage .In our approach , the top N documents from an initial retrieval using the original query form the source of feedback terms : all terms that appear in them are considered candidates to present to the user .These documents serve as pseudo-feedback , since they provide a much richer context than the original query usually very short , while the user is not asked to judge their relevance .Due to the latter reason , it is possible to make N quite large e.g. , in our experiments we set N = 60 to increase its coverage of different aspects in the topic .The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents .This method , however , has two drawbacks .First , a lot of common noisy terms will be selected due to their high frequencies in the document collection , unless a stop-word list is used for filtering .Second , the presentation list will tend to be filled by terms from major aspects of the topic ; those from a minor aspect are likely to be missed due to their relatively low frequencies .We solve the above problems by two corresponding measures .First , we introduce a background model \u03b8B that is estimated from collection statistics and explains the common terms , so that they are much less likely to appear in the presentation list .Second , the terms are selected from multiple clusters in the pseudo-feedback documents , to ensure sufficient representation of different aspects of the topic .We rely on the mixture multinomial model , which is used for theme discovery in 26 .Specifically , we assume the N documents contain K clusters Ci | i = 1 , 2 , \u00b7 \u00b7 \u00b7 K , each characterized by a multinomial word distribution also known as unigram language model \u03b8i and corresponding to an aspect of the topic .The documents are regarded as sampled from a mixture of K + 1 components , including the K clusters and the background model :where w is a word , \u03bbB is the mixture weight for the background model \u03b8B , and \u03c0d , i is the document-specific mixture weight for the i-th cluster model \u03b8i .We then estimate the cluster models by maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model :where D = di | i = 1 , 2 , \u00b7 \u00b7 \u00b7 N is the set of the N documents , V is the vocabulary , c w ; d is w 's frequency in d and \u039b = \u03b8i | i = 1 , 2 , \u00b7 \u00b7 \u00b7 K \u222a \u03c0dij | i = 1 , 2 , \u00b7 \u00b7 \u00b7 N , j = 1 , 2 , \u00b7 \u00b7 \u00b7 K is the set of model parameters to estimate .The cluster models can be efficiently estimated using the Expectation-Maximization EM algorithm .For its details , we refer the reader to 26 .Table 1 shows the cluster models for TREC query `` Transportation tunnel disasters '' K = 3 .Note that only the middle cluster is relevant .Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 From each of the K estimated clusters , we choose the L = M/K terms with highest probabilities to form a total of M presentation terms .If a term happens to be in top L in multiple clusters , we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation .We also filter out terms in the original query text because they tend to always be relevant when the query is short .The selected terms are then presented to the user for judgment .A sample completed feedback form is shown in Figure 1 .In this study we only deal with binary judgment : a presented term is by default unchecked , and a user may check it to indicate relevance .We also do not explicitly exploit negative feedback i.e. , penalizing irrelevant terms , because with binary feedback an unchecked term is not necessarily irrelevant maybe the user is unsure about its relevance .We could ask the user for finer judgment e.g. , choosing from highly relevant , somewhat relevant , do not know , somewhat irrelevant and highly irrelevant , but binary feedback is more compact , taking less space to display and less user effort to make judgment .In this section , we present several algorithms for exploiting term feedback .The algorithms take as input the original query q , the clusters \u03b8i as generated by the theme discovery algorithm , the set of feedback terms T and their relevance judgment R , and outputs an updated query language model \u03b8 ~ q that makes best use of the feedback evidence to capture the user 's information need .First we describe our notations : | q | where c w ; q is the count of w in q , and | q | = EwEq c w ; q is the query length .where Ew ET \u03b4w is the total number of terms that are judged relevant .We call this method TFB direct Term FeedBack .If we let \u03bc = 1 , this approach is equivalent to appending the relevant terms after the original query , which is what standard query expansion without term reweighting does .If we set \u03bc > 1 , we are putting more emphasis on the query terms than the checked ones .Note that the result model will be more biased toward \u03b8q if the original query is long or the user feedback is weak , which makes sense , as we can trust more on the original query in either case .This is a straight-forward form of term feedback that does not involve any secondary structure .We give a weight of 1 to terms judged relevant by the user , a weight of \u03bc to query terms , zero weight to other terms , and then apply normalization :Here we exploit the cluster structure that played an important role when we selected the presentation terms .The clusters represent different aspects of the query topic , each of which may or may not be relevant .If we are able to identify the relevant clusters , we can combine them to generate a query model that is good at discovering documents belonging to these clusters instead of the irrelevant ones .We could ask the user to directly judge the relevance of a cluster after viewing representative terms in that cluster , but this would sometimes be a difficult task for the user , who has to guess the semantics of a cluster via its set of terms , which may not be well connected to one another due to a lack of context .Therefore , we propose to learn cluster feedback indirectly , inferring the relevance of a cluster through the relevance of its feedback terms .Because each cluster has an equal number of terms presented to the user , the simplest measure of a cluster 's relevance is the number of terms that are judged relevant in it .Intuitively , the more terms are marked relevant in a cluster , the closer the cluster is to the query topic , and the more the cluster should participate in query modification .If we combine the cluster models using weights determined this way and then interpolate with the original query model , we get the following formula for query updating , which we call CFB Cluster FeedBack :where EL j = 1 \u03b4ti , j is the number of relevant terms in cluster Ci , and EK EL j = 1 \u03b4tk , j is the total number of relevant terms .k = 1 We note that when there is only one cluster K = 1 , the above formula degenerates towhich is merely pseudo-feedback of the form proposed in 25 .TFB and CFB both have their drawbacks .TFB assigns non-zero probabilities to the presented terms that are marked relevant , but completely ignores a lot more others , which may be left unchecked due to the user 's ignorance , or simply not included in the presentation list , but we should be able to infer their relevance from the checked ones .For example , in Figure 1 , since as many as 5 terms in the middle cluster the third and fourth columns are checked , we should have high confidence in the relevance of other terms in that cluster .CFB remedies TFB 's problem by treating the terms in a cluster collectively , so that unchecked/unpresented terms receive weights when presented terms in their clusters are judged as relevant , but it does not distinguish which terms in a cluster are presented or judged .Intuitively , the judged relevant terms should receive larger weights because they are explicitly indicated as relevant by the user .Therefore , we try to combine the two methods , hoping to get the best out of both .We do this by interpolating the TFB model with the CFB model , and call it TCFB :", "evaluation": "In this section , we describe our experiment results .We first describe our experiment setup and present an overview of various methods ' performance .Then we discuss the effects of varying the parameter setting in the algorithms , as well as the number of presentation terms .Next we analyze user term feedback behavior and its relation to retrieval performance .Finally we compare term feedback to relevance feedback and show that it has its particular advantage .We took the opportunity of TREC 2005 HARD Track 2 for the evaluation of our algorithms .The tracks used the AQUAINT collection , a 3GB corpus of English newswire text .The topics included 50 ones previously known to be hard , i.e. with low retrieval performance .It is for these hard topics that user feedback is most helpful , as it can provide information to disambiguate the queries ; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough .Participants of the track were able to submit custom-designed clarification forms CF to solicit feedback from human assessors provided byNIST .We designed three sets of clarification forms for term feedback , differing in the choice of K , the number of clusters , and L , the number of presented terms from each cluster .They are : 1 x 48 , a big cluster with 48 terms , 3 x 16 , 3 clusters with 16 terms each , and 6 x 8 , 6 clusters with 8 terms each .The total number of presented terms M is fixed at 48 , so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering .For each topic , an assessor would complete the forms ordered by 6 x 8 , 1 x 48 and 3 x 16 , spending up to three minutes on each form .The sample clarification form shown in Figure 1 is of type 3 x 16 .It is a simple and compact interface in which the user can check relevant terms .The form is self-explanatory ; there is no need for extra user training on how to use it .Our initinal queries are constructed only using the topic title descriptions , which are on average 2.7 words in length .As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents .We stem the terms , choose Dirichlet smoothing with a prior of 2000 , and truncate query language models to 50 terms these settings are used throughout the experiments .For all other parameters we use Lemur 's default settings .The baseline turns out to perform above average among the track participants .After an initial run using this baseline retrieval method , we take the top 60 documents for each topic and apply the theme discovery algorithm to output the clusters 1 , 3 , or 6 of them , based on which we generate clarification forms .After user feedback is received , we run the term feedback algorithms TFB , CFB or TCFB to estimate updated query models , which are then used for a second iteration of retrieval .We evaluate the different retrieval methods ' performance on their rankings of the top 1000 documents .The evaluation metrics we adopt include mean average non-interpolated precision MAP , precision at top 30 Pr@30 and total relevant retrieved RR .Table 2 shows the performance of various methods and configurations of K x L .The suffixes 1C , 3C , 6C after TFB , CFB , TCFB stand for the number of clusters K .For example , TCFB3C means the TCFB method on the 3 x 16 clarification forms .From Table 2 we can make the following observations :our speculation that TCFB overcomes the drawbacks of TFB paying attention only to checked terms and CFB not distinguishing checked and unchecked terms in a cluster .Except for TCFB6C v.s. CFB6C , the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test .This is not true in the case of TFB v.s. CFB , each of which is better than the other in nearly half of the topics .In some situations we may have to reduce the number of presentation terms due to limits in display space or user feedback efforts .It is interesting to know whether our algorithms ' performance deteriorates when the user is presented with fewer terms .Because the presentation terms within each cluster are generated in decreasing order of their frequencies , the presentation list forms a subset of the original one if its size is reduced2 .Therefore , we can easily simulate what happens when the number of presentation terms decreases 2There are complexities arising from terms appearing in top L of multiple clusters , but these are exceptionsfrom M to M ' : we will keep all judgments of the top L' = M ' / K terms in each cluster and discard those of others .Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48 .We find that the performance of TFB is more susceptible to presentation term reduction than that of CFB or TCFB .For example , at 12 terms the MAP of TFB3C is 90.6 % of that at 48 terms , while the numbers for CFB3C and TCFB3C are 98.0 % and 96.1 % respectively .We conjecture the reason to be that while TFB 's performance heavily depends on how many good terms are chosen for query expansion , CFB only needs a rough estimate of cluster weights to work .Also , the 3 x 16 clarification forms seem to be more robust than the 6 x 8 ones : at 12 terms the MAP of TFB6C is 87.1 % of that at 48 terms , lower than 90.6 % for TFB3C .Similarly , for CFB it is 95.0 % against 98.0 % .This is natual , as for a large cluster number of 6 , it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful .Overall , we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small .For example , at only 12 terms CFB3C the clarification form is of size 3 x 4 can still improve 36.5 % over the baseline , dropping slightly from 39.3 % at 48 terms .In this part we study several aspects of user 's term feedback behavior , and whether they are connected to retrieval performance .completion time seconds Figure 2 shows the distribution of time needed to complete a clarification form3 .We see that the user is usually able to finish term feedback within a reasonably short amount of time : for more than half of the topics the clarification form is completed in just 1 minute , and only a small fraction of topics less than 10 % for 1 x 48 and 3 x 16 take more than 2 minutes .This suggests that term feedback is suitable for interactive ad-hoc retrieval , where a user usually does not want to spend too much time on providing feedback .We find that a user often makes mistakes when judging term relevance .Sometimes a relevant term may be left out because its connection to the query topic is not obvious to the user .Other times a dubious term may be included but turns out to be irrelevant .Take the topic in Figure 1 for example .There was a fire disaster in MontBlanc Tunnel between France and Italy in 1999 , but the user failed to select such keywords as `` mont '' , `` blanc '' , `` french '' and `` italian '' due to his/her ignorance of the event .Indeed , without proper context it would be hard to make perfect judgment .What is then , the extent to which the user is good at term feedback ?Does it have serious impact on retrieval performance ?To answer these questions , we need a measure of individual terms ' true relevance .We adopt the Simplified KL Divergence metric used in 24 to decide query expansion terms as our term relevance measure :where p wIR is the probability that a relevant document contains term w , and p wI-R is the probability that an irrelevant document contains w , both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment .If \u03c3KLD w > 0 , w is more likely to appear in relevant documents than irrelevant ones .We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold \u03c30 .We can then define precision and recall of user term judgment accordingly : precision is the fraction of terms checked by the user that are relevant ; recall is the fraction of presented relevant terms that are checked by the user .Table 4 shows the number of checked terms , relevant terms and relevant checked terms when \u03c30 is set to 1.0 , as well as the precision/recall of user term judgment .Note that when the clarification forms contain more clusters , fewer terms are checked : 14.8 for 1 x 48 , 13.3 for 3 x 16 and 11.2 for 6 x 8 .Similar pattern holds for relevant terms and relevant checked terms .There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms : when there are more clusters , each of them gets fewer terms to present , which can hurt a major relevant cluster that contains many relevant terms .Therefore , it is not always helpful to have more clusters , e.g. , TFB6C is actually worse than TFB1C .The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms , which echoes the discovery in 18 .In the case of 3 x 16 clarification forms , the average number of terms checked as relevant by the user is 13.3 per topic , and the average number of relevant terms whose \u03c3KLD value exceed 1.0 is 12.6 .The user is able to recognize only 6.9 of these terms on average .Indeed , the precision and recall of user feedback terms as defined previously are far from perfect .On the other hand , If the user had correctly checked all such relevant terms , the performance of our algorithms would have increased a lot , as shown in Table 5 .We see that TFB gets big improvement when there is an oracle who checks all relevant terms , while CFB meets a bottleneck around MAP of 0.325 , since all it does is adjust cluster weights , and when the learned weights are close to being accurate , it can not benefit more from term feedback .Also note that TCFB fails to outperform TFB , probably because TFB is sufficiently accurate .Now we compare term feedback with document-level relevance feedback , in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance .The feedback process is simulated using document relevance judgment from NIST .We use the mixture model based feedback method proposed in 25 , with mixture noise set to 0.95 and feedback coefficient set to 0.9 .Comparative evaluation of relevance feedback against other methods is complicated by the fact that some documents have already been viewed during feedback , so it makes no sense to include them in the retrieval results of the second run .However , this does not hold for term feedback .Thus , to make it fair w.r.t. user 's information gain , if the feedback documents are relevant , they should be kept in the top of the ranking ; if they are irrelevant , they should be left out .Therefore , we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded , and then prepend the relevant feedback documents at the front .Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C .We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents .Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents 4947 when going down the ranked list .We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback .This is done by truncating the relevance feedback modified query model to a size equal to the number of checked terms for the same topic .We can then compare the terms in the truncated model with the checked terms .Figure 3 shows the distribution of the terms ' \u03c3KLD scores .We find that term feedback tends to produce expansion terms of higher quality those with \u03c3KLD > 1 compared to relevance feedback with 10 feedback documents .This does not contradict the fact that the latter yields higher retrieval performance .Actually , when we use the truncated query model instead of the intact one refined from relevance feedback , the MAP is only 0.304 .The truthis , although there are many unwanted terms in the expanded query model from feedback documents , there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents , and the positive effects often outweights the negative ones .We are interested to know under what circumstances term feedback has advantage over relevance feedback .One such situation is when none of the top N feedback documents is relevant , rendering relevance feedback useless .This is not infrequent , as one might have thought : out of the 50 topics , there are 13 such cases when N = 5 , 10 when N = 10 , and still 3 when N = 20 .When this happens , one can only back off to the original retrieval method ; the power of relevance feedback is lost .Surprisingly , in 11 out of 13 such cases where relevance feedback seems impossible , the user is able to check at least 2 relevant terms from the 3 \u00d7 16 clarification form we consider term t to be relevant if \u03c3KLD t > 1.0 .Furthermore , in 10 out of them TCFB3C outperforms the pseudo-feedback baseline , increasing MAP from 0.076 to 0.146 on average these are particularly hard topics .We think that there are two possible explanations for this phenomenon of term feedback being active even when relevance feedback does not work : First , even if none of the top N suppose it is a small number documents are relevant , we may still find relevant documents in top 60 , which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search , from which we can draw feedback terms .This is true for topic 367 `` piracy '' , where the top 10 feedback documents are all about software piracy , yet there are documents between 10-60 that are about piracy on the seas which is about the real information need , contributing terms such as '' pirate '' , '' ship '' for selection in the clarification form .Second , for some topics , a document needs to meet some special condition in order to be relevant .The top N documents may be related to the topic , but nonetheless irrelevant .In this case , we may still extract useful terms from these documents , even if they do not qualify as relevant ones .For example , in topic 639 `` consumer online shopping '' , a document needs to mention what contributes to shopping growth to really match the specified information need , hence none of the top 10 feedback documents are regarded as relevant .But nevertheless , the feedback terms such as '' retail '' , `` commerce '' are good for query expansion .", "author_keywords_stem": ["queri expans", "interact retriev"], "related work": "Relevance feedback 17 , 19 has long been recognized as an effective method for improving retrieval performance .Normally , the top N documents retrieved using the original query are presented to the user for judgment , after which terms are extracted from the judged relevant documents , weighted by their potential of attracting more relevant documents , and added into the query model .The expanded query usually represents the user 's information need better than the original one , which is often just a short keyword query .A second iteration of retrieval using this modified query usually produces significant increase in retrieval accuracy .In cases where true relevance judgment is unavailable and all top N documents are assumed to be relevant , it is called blind or pseudo feedback 5 , 16 and usually still brings performance improvement .Because document is a large text unit , when it is used for relevance feedback many irrelevant terms can be introduced into the feedback process .To overcome this , passage feedback is proposed and shown to improve feedback performance 1 , 23 .A more direct solution is to ask the user for their relevance judgment of feedback terms .For example , in some relevance feedback systems such as 12 , there is an interaction step that allows the user to add or remove expansion terms after they are automatically extracted from relevant documents .This is categorized as interactive query expansion , where the original query is augmented with user-provided terms , which can come from direct user input free-form text or keywords 22 , 7 , 10 or user selection of system-suggested terms using thesauri 6 , 22 or extracted from feedback documents 6 , 22 , 12 , 4 , 7 .In many cases term relevance feedback has been found to effectively improve retrieval performance 6 , 22 , 12 , 4 , 10 .For example , the study in 12 shows that the user prefers to have explicit knowledge and direct control of which terms are used for query expansion , and the penetrable interface that provides this freedom is shown to perform better than other interfaces .However , in some other cases there is no significant benefit 3 , 14 , even if the user likes interacting with expansion terms .In a simulated study carried out in 18 , the author compares the retrieval performance of interactive query expansion and automatic query expansion with a simulated study , and suggests that the potential benefits of the former can be hard to achieve .The user is found to be not good at identifying useful terms for query expansion , when a simple term presentation interface is unable to provide sufficient semantic context of the feedback terms .Our work differs from the previous ones in two important aspects .First , when we choose terms to present to the user for relevance judgment , we not only consider single-term value e.g. , the relative frequency of a term in the top documents , which can be measured by metrics such as Robertson Selection Value and Simplified Kullback-Leibler Distance as listed in 24 , but also examine the cluster structure of the terms , so as to produce a balanced coverage of the different topic aspects .Second , with the language modelling framework , we allow an elaborate construction of the updated query model , by setting different probabilities for different terms based on whether it is a query term , its significance in the top documents , and its cluster membership .Although techniques for adjusting query term weights exist for vector space models 17 and probablistic relevance models 9 , most of the aforementioned works do not use them , choosing to just append feedback terms to the original query thus using equal weights for them , which can lead to poorer retrieval performance .The combination of the two aspects allows our method to perform much better than the baseline .The usual way for feedback term presentation is just to display the terms in a list .There have been some works on alternative user interfaces .8 arranges terms in a hierarchy , and 11 compares three different interfaces , including terms + checkboxes , terms + context sentences + checkboxes , sentences + input text box .In both studies , however , there is no significant performance difference .In our work we adopt the simplest approach of terms + checkboxes .We focus on term presentation and query model construction from feedback terms , and believe using contexts to improve feedback term quality should be orthogonal to our method ."}