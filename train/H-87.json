{"reader_keywords": ["adaptive filtering", "information retrieval", "topic detection", "topic tracking", "relevance feedback", "statistical learning", "robustness", "systematic method for parameter tuning across multiple corpora", "external corpus", "validation set", "logistic regression", "granularity difference", "cross-benchmark evaluation", "utility function", "cost function", "penalty ratio", "optimization criterion", "probabilistic threshold calibration", "rocchio-style method", "cross-corpus parameter optimization", "posterior probability", "sigmoid function", "bias", "gaussian", "regularization", "lr"], "reader_keywords_stem": ["adapt filter", "inform retriev", "topic detect", "topic track", "relev feedback", "statist learn", "robust", "systemat method for paramet tune across multipl corpora", "extern corpu", "valid set", "logist regress", "granular differ", "cross-benchmark evalu", "util function", "cost function", "penalti ratio", "optim criterion", "probabilist threshold calibr", "rocchio-style method", "cross-corpu paramet optim", "posterior probabl", "sigmoid function", "bia", "gaussian", "regular", "lr"], "introduction": "Adaptive filtering AF has been a challenging research topic in information retrieval .The task is for the system to make an online topic membership decision yes or no for every document , as soon as it arrives , with respect to each pre-definedThe above conditions attempt to mimic realistic situations where an AF system would be used .That is , the user would be willing to provide a few positive examples for each topic of interest at the start , and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback .Furthermore , topics of interest might change over time , with new topics appearing and growing , and old topics shrinking and diminishing .These conditions make adaptive filtering a difficult task in statistical learning online classification , for the following reasons :None of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once .The first two problems have been studied in the adaptive filtering literature , including topic profile adaptation using incremental Rocchio , Gaussian-Exponential density models , logistic regression in a Bayesian framework , etc. , and threshold optimization strategies using probabilisticcalibration or local fitting techniques 1 2 9 10 11 12 13 .Although these works provide valuable insights for understanding the problems and possible solutions , it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated .Addressing the third issue is the main focus in this paper .We argue that robustness is an important measure for evaluating and comparing AF methods .By `` robust '' we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora .Most AF methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts .Available training examples , on the other hand , are often insufficient for tuning the parameters .In TDT5 , for example , there is only one labeled training example per topic at the start ; parameter optimization on such training data is doomed to be ineffective .This leaves only one option assuming tuning on the test set is not an alternative , that is , choosing an external corpus as the validation set .Notice that the validation-set topics often do not overlap with the test-set topics , thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other .Now the important question is : which methods if any are robust under the condition of using cross-corpus validation to tune parameters ?Current literature does not offer an answer because no thorough investigation on the robustness of AF methods has been reported .In this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in AF : incremental Rocchio and regularized logistic regression LR .Rocchio-style classifiers have been popular in AF , with good performance in benchmark evaluations TREC and TDT if appropriate parameters were used and if combined with an effective threshold calibration strategy 2 4 7 8 9 11 13 .Logistic regression is a classical method in statistical learning , and one of the best in batch-mode text categorization 15 14 .It was recently evaluated in adaptive filtering and was found to have relatively strong performance Section 5.1 .Furthermore , a recent paper 13 reported that the joint use of Rocchio and LR in a Bayesian framework outperformed the results of using each method alone on the TREC11 corpus .Stimulated by those findings , we decided to include Rocchio and LR in our crossbenchmark evaluation for robustness testing .Specifically , we focus on how much the performance of these methods depends on parameter tuning , what the most influential parameters are in these methods , how difficult or how easy to optimize these influential parameters using cross-corpus validation , how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora , and how efficient these methods are in running AF on large benchmark corpora .The organization of the paper is as follows : Section 2 introduces the four benchmark corpora TREC10 and TREC11 , TDT3 and TDT5 used in this study .Section 3 analyzes the differences among the TREC and TDT metrics utilities and tracking cost and the potential implications of those differences .Section 4 outlines the Rocchio and LR approaches to AF , respectively .Section 5 reports the experiments and results .Section 6 concludes the main findings in this study .", "title": "Robustness of Adaptive Filtering Methods In a Cross-benchmark Evaluation", "author_keywords_stem": ["adaptive filter", "topic track", "cross-benchmark evaluation", "logistic regression", "rocchio"], "abstract": "This paper reports a cross-benchmark evaluation of regularized logistic regression LR and incremental Rocchio for adaptive filtering .Using four corpora from the Topic Detection and Tracking TDT forum and the Text Retrieval Conferences TREC we evaluated these methods with non-stationary topics at various granularity levels , and measured performance with different utility settings .We found that LR performs strongly and robustly in optimizing T11SU a TREC utility function while Rocchio is better for optimizing Ctrk the TDT tracking cost , a high-recall oriented objective function .Using systematic cross-corpus parameter optimization with both methods , we obtained the best results ever reported on TDT5 , TREC10 and TREC11 .Relevance feedback on a small portion 0.05 ~ 0.2 % of the TDT5 test documents yielded significant performance improvements , measuring up to a 54 % reduction in Ctrk and a 20.9 % increase in T11SU with \u03b2 = 0.1 , compared to the results of the top-performing system in TDT2004 without relevance feedback information .", "id": "H-87", "combined_keywords_stem": ["adapt filter", "inform retriev", "topic detect", "topic track", "relev feedback", "statist learn", "robust", "systemat method for paramet tune across multipl corpora", "extern corpu", "valid set", "logist regress", "granular differ", "cross-benchmark evalu", "util function", "cost function", "penalti ratio", "optim criterion", "probabilist threshold calibr", "rocchio-style method", "cross-corpu paramet optim", "posterior probabl", "sigmoid function", "bia", "gaussian", "regular", "lr", "adapt filter", "topic track", "rocchio"], "evaluation": "We report our empirical findings in four parts : the TDT2004 official evaluation results , the cross-corpus parameter optimization results , and the results corresponding to the amounts of relevance feedback .The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004 .Multiple research teams participated and multiple runs from each team were allowed .Ctrk and TDT5SU were used as the metrics .Figure 2 and Figure 3 show the results ; the best run from each team was selected with respect to Ctrk or TDT5SU , respectively .Our Rocchio with adaptive profiles but fixed universal threshold for all topics had the best result in Ctrk , and our logistic regression had the best result in TDT5SU .All the parameters of our runs were tuned on the TDT3 corpus .Results for other sites are also listed anonymously for comparison .Adaptive filtering without using true relevance feedback was also a part of the evaluations .In this case , systems had only one labeled training example per topic during the entire training and testing processes , although unlabeled test documents could be used as soon as predictions on them were made .Such a setting has been conventional for the Topic Tracking task in TDT until 2004 .Figure 4 shows the summarized official submissions from each team .Our PRF Rocchio with a fixed threshold for all the topics had the best performance .2 We use quartiles rather than standard deviations since the former is more resistant to outliers .How much the strong performance of our systems depends on parameter tuning is an important question .Both Rocchio and LR have parameters that must be prespecified before the AF process .The shared parameters include the sample weights \u03b1 , \u03b2 and \u03b3 , the sample size of the negative training documents i.e. , D \u2212 T , the term-weighting scheme , and the maximal number of non-zero elements in each document vector .The method-specific parameters include the decision threshold in Rocchio , and \u00b5r , \u03bb and MI the maximum number of iterations in training in LR .Given that we only have one labeled example per topic in the TDT5 training sets , it is impossible to effectively optimize these parameters on the training data , and we had to choose an external corpus for validation .Among the choices of TREC10 , TREC11 and TDT3 , we chose TDT3 c.f. Section 2 because it is most similar to TDT5 in terms of the nature of the topics Section 2 .We optimized the parameters of our systems on TDT3 , and fixed those parameters in the runs on TDT5 for our submissions to TDT2004 .We also tested our methods on TREC10 and TREC11 for further analysis .Since exhaustive testing of all possible parameter settings is computationally intractable , we followed a step-wise forward chaining procedure instead : we pre-specified an order of the parameters in a method Rocchio or LR , and then tuned one parameter at the time while fixing the settings of the remaining parameters .We repeated this procedure for several passes as time allowed .Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3 , TDT5 , TREC10 and TREC11 when the decision threshold varied .These curves peak at different locations : the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal .If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5 , or if the TDT3 corpus were not available , we would have difficulty in obtaining strong performance for Rocchio in TDT2004 .The difficulty comes from the ad hoc non-probabilistic scores generated by the Rocchio method : the distribution of the scores depends on the corpus , making cross-corpus threshold optimization a tricky problem .Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of Pr y = 1 | xupon which the optimal threshold can be directly computed if probability estimation is accurate .Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU , 10:1 in TDT5SU and 583:1 in Ctrk Section 3.3 , the corresponding optimal thresholds t are 0.33 , 0.091 and 0.0017 respectively .Although the theoretical threshold could be inaccurate , it still suggests the range of near-optimal settings .With these threshold settings in our experiments for LR , we focused on the cross-corpus validation of the Bayesian prior parameters , that is , \u03bcr and X. Table 2 summarizes the results3 .We measured the performance of the runs on TREC10 and TREC11 using T11SU , and the performance of the runs on TDT3 and TDT5 using TDT5SU .For comparison we also include the best results of Rocchio-based methods on these corpora , which are our own results of Rocchio on TDT3 and TDT5 , and the best results reported by NIST for TREC10 and TREC11 .From this set of results , we see that LR significantly outperformed Rocchio on all the corpora , even in the runs of standard LR without any tuning , i.e. X = 0 .This empirical finding is consistent with a previous report 13 for LR on TREC11 although our results of LR 0.585 ~ 0.608 in T11SU are stronger than the results 0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior in that report .More importantly , our cross-benchmark evaluation gives strong evidence for the robustness of LR .The robustness , we believe , comes from the probabilistic nature of the system-generated scores .That is , compared to the ad hoc scores in Rocchio , the normalized posterior probabilities make the threshold optimization in LR a much easier problem .Moreover , logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers ' parameters do not .Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior ; instead , the performance decreased in some cases .This observation does not support the previous report by 13 , but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process , and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR .We also believe that variance reduction in the testing phase should be controlled by the choice of X but not \u03bcr , for which we conducted the experiments as shown in Figure 6 .3 The LR results 0.77 ~ 0.78 on TDT5 in this table are better than our TDT2004 official result 0.73 because parameter optimization has been improved afterwards .4 The TREC10-best result 0.496 by Oracle is only available in T10U which is not directly comparable to the scores in T11SU , just indicative .* : \u03bcr was set to the Rocchio prototypeThe performance of LR is summarized with respect to X tuning on the corpora of TREC10 , TREC11 and TDT3 .The performance on each corpus was measured using the corresponding metrics , that is , T11SU for the runs on TREC10 and TREC11 , and TDT5SU and Ctrk for the runs on TDT3 , .In the case of maximizing the utilities , the `` safe '' interval for X is between 0 and 0.01 , meaning that the performance of regularized LR is stable , the same as or improved slightly over the performance of standard LR .In the case of minimizing Ctrk , the safe range for X is between 0 and 0.1 , and setting \u03bb between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky , and hence more regularization is needed .In either case , tuning X is relatively safe , and easy to do successfully by cross-corpus tuning .Another influential choice in our experiment settings is term weighting : we examined the choices of binary , TF and TF-IDF the `` ltc '' version schemes .We found TF-IDF most effective for both Rocchio and LR , and used this setting in all our experiments .How much relevance feedback RF would be needed during the AF process is a meaningful question in real-world applications .To answer it , we evaluated Rocchio and LR on TDT with the following settings :Table 3 summarizes the results in Ctrk : Adaptive Rocchio with relevance feedback on 0.6 % of the test documents reduced the tracking cost by 54 % over the result of the PRF Rocchio , the best system in the TDT2004 evaluation for topic tracking without relevance feedback information .Incremental LR , on the other hand , was weaker but still impressive .Recall that Ctrk is an extremely high-recall oriented metric , causing frequent updating of profiles and hence an efficiency problem in LR .For this reason we set a higher threshold 0.004 instead of the theoretically optimal threshold 0.0017 in LR to avoid an untolerable computation cost .The computation time in machinehours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk .Table 4 summarizes the results in TDT5SU ; adaptive LR was the winner in this case , with relevance feedback on 0.05 % of the test documents improving the utility by 20.9 % over the results of PRF Rocchio .Evidently , both Rocchio and LR are highly effective in adaptive filtering , in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning , which is the main goal of AF .After we decided the parameter settings using validation , we perform the adaptive filtering in the following steps for each topic : 1 Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples ; 2 For each document in the test corpus : we first make a prediction about relevance , and then get relevance feedback for those predicted positive documents .3 Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback .", "combined_keywords": ["adaptive filtering", "information retrieval", "topic detection", "topic tracking", "relevance feedback", "statistical learning", "robustness", "systematic method for parameter tuning across multiple corpora", "external corpus", "validation set", "logistic regression", "granularity difference", "cross-benchmark evaluation", "utility function", "cost function", "penalty ratio", "optimization criterion", "probabilistic threshold calibration", "rocchio-style method", "cross-corpus parameter optimization", "posterior probability", "sigmoid function", "bias", "gaussian", "regularization", "lr", "adaptive filter", "topic track", "rocchio"], "author_keywords": ["adaptive filter", "topic track", "cross-benchmark evaluation", "logistic regression", "rocchio"], "method": "We used four benchmark corpora in our study .Table 1 shows the statistics about these data sets .TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001 , consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels subject categories 7 .The first two weeks August 20th to 31st , 1996 of documents is the training set , and the remaining 11 & 1/2 months from September 1st , 1996 to August 19th , 1997 is the test set .TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002 , consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets .The TREC11 topics 50 are quite different from those in TREC10 ; they are queries for retrieval with relevance judgments by NIST assessors 8 .TDT3 was the evaluation benchmark in the TDT2001 dry run1 .The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin AP , NYT , CNN , ABC , NBC , MSNBC , Xinhua , Zaobao , Voice of America and PRI the World in the period of October to December 1998 .Machine-translated versions of the non-English stories Xinhua , Zaobao and VOA Mandarin are provided as well .The splitting point for training-test sets is different for each topic in TDT .TDT5 was the evaluation benchmark in TDT2004 4 .The tracking part of the corpus consists of 407,459 news stories in the period of April to September , 2003 from 15 news agents or broadcast sources in English , Arabic and Mandarin , with machine-translated versions of the non-English stories .We only used the English versions of those documents in our experiments for this paper .The TDT `` topics '' differ from TREC topics both conceptually and statistically .Instead of generic , ever-lasting subject categories as those in TREC , TDT topics are defined at a finer level of granularity , for events that happen at certain times and locations , and that are `` born '' and `` die '' , typically associated with a bursty distribution over chronologically ordered news stories .The average size of TDT topics events is two orders of magnitude smaller than that of the TREC10 topics .Figure 1 compares the document densities of a TREC topic `` Civil Wars '' and two TDT topics `` Gunshot '' and `` APEC Summit Meeting '' , respectively over a 3-month time period , where the area under each curve is normalized to one .The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting .For example , algorithms favoring large and stable topics may not work well for short-lasting and nonstationary topics , and vice versa .Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics .To make our results comparable to the literature , we decided to use both TREC-conventional and TDT-conventional metrics in our evaluation .Let A , B , C and D be , respectively , the numbers of true positives , false alarms , misses and true negatives for a specific topic , and N = A + B + C + D be the total number of test documents .The TREC-conventional metrics are defined as :where parameters \u03b2 and \u03b7 were set to 0.5 and -0.5 respectively in TREC10 2001 and TREC11 2002 .For evaluating the performance of a system , the performance scores are computedFrom an optimization point of view , TDT5SU and T11SU are both utility functions while Ctrk is a cost function .Our objective for individual topics first and then averaged over topics macroaveraging .The TDT-conventional metric for topic tracking is defined as :where P T is the percentage of documents on topic T , Pmiss is the miss rate by the system on that topic , Pfa is the false alarm rate , and w1 and w2 are the costs pre-specified constants for a miss and a false alarm , respectively .The TDT benchmark evaluations since 1997 have used the settings of w1 = 1 , w2 = 0 .1 and P T = 0.02 for all topics .For evaluating the performance of a system , Ctrk is computed for each topic first and then the resulting scores are averaged for a single measure the topic-weighted Ctrk .To make the intuition behind this measure transparent , we substitute the terms in the definition of Ctrk as follows :Clearly , Ctrk is the average cost per error on topic T , with w1 and w2 controlling the penalty ratio for misses vs. false alarms .In addition to Ctrk , TDT2004 also employed T 11 SU\u03b2 = 0 .1 as a utility metric .To distinguish this from the T 11SU\u03b2 = 0 .5 in TREC11 , we call former TDT5SU in the rest of this paper .is to maximize the former or to minimize the latter on test documents .The differences and correlations among these objective functions can be analyzed through the shared counts of A , B , C and D in their definitions .For example , both TDT5SUand T11SU are positively correlated to the values of A and D , and negatively correlated to the values of B and C ; the only difference between them is in their penalty ratios for misses vs. false alarms , i.e. , 10:1 in TDT5SU and 2:1 in T11SU .The Ctrk function , on the other hand , is positively correlated to the values of C and B , and negatively correlated to the values of A and D ; hence , it is negatively correlated to T11SU and TDT5SU .More importantly , there is a subtle and major difference between Ctrk and the utility functions : T11SU and TDT5SU .That is , Ctrk has a very different penalty ratio for misses vs. false alarms : it favors recall-oriented systems to an extreme .At first glance , one would think that the penalty ratio in Ctrk is 10:1 since w1 = 1 and w2 = 0 .1 .However , this is not true if P T = 0 .02 is an inaccurate estimate of the on-topic documents on average for the test corpus .Using TDT3 as an example , the true percentage is :where N is the average size of the test sets in TDT3 , and n + is the average number of positive examples per topic in the test sets .Using P\u02c6 T = 0.02 as an inaccurate estimate of 0.002 enlarges the intended penalty ratio of 10:1 to 100:1 , roughly speaking .To wit :estimation of P T compared to the truth .Comparing the above result to formula 2 , we can see the actual penalty ratio for misses vs. false alarms was 100:1 in the evaluations on TDT3 using Ctrk .Similarly , we can compute the enlargement factor for TDT5 using the statistics in Table 1 as follows : which means the actual penalty ratio for misses vs. false alarms in the evaluation on TDT5 using Ctrk was approximately 583:1 .The implications of the above analysis are rather significant :Although these problems with Ctrk were not originally anticipated , it offered an opportunity to examine the ability of systems in trading off precision for extreme recall .This was a challenging part of the TDT2004 evaluation for AF .Comparing the metrics in TDT and TREC from a utility or cost optimization point of view is important for understanding the evaluation results of adaptive filtering methods .This is the first time this issue is explicitly analyzed , to our knowledge .We employed a common version of Rocchio-style classifiers which computes a prototype vector per topic T as follows :The first term on the RHS is the weighted vector representation of topic description whose elements are terms weights .The second term is the weighted centroid of the set D + T of positive training examples , each of which is a vector of withindocument term weights .The third term is the weighted centroid of the set D \u2212 T of negative training examples which are the nearest neighbors of the positive centroid .The three terms are given pre-specified weights of \u03b1 , \u03b2 and \u03b3 , controlling the relative influence of these components in the prototype .The prototype of a topic is updated each time the system makes a `` yes '' decision on a new document for that topic .If relevance feedback is available as is the case in TREC adaptive filtering , the new document is added to the pool of either D + T or D \u2212 T , and the prototype is recomputed accordingly ; if relevance feedback is not available as is the case in TDT event tracking , the system 's prediction `` yes '' is treated as the truth , and the new document is added to D + T for updating the prototype .Both cases are part of our experiments in this paper and part of the TDT 2004 evaluations for AF .To distinguish the two , we call the first case simply `` Rocchio '' and the second case `` PRF Rocchio '' where PRF stands for pseudorelevance feedback .The predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector , and then comparing the resulting scores against a threshold :Threshold calibration in incremental Rocchio is a challenging research topic .Multiple approaches have been developed .The simplest is to use a universal threshold for all topics , tuned on a validation set and fixed during the testing phase .More elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities r i.e. , P T | d for utility optimization 9 13 , and margin-based local regression for risk reduction 11 .It is beyond the scope of this paper to compare all the different ways to adapt Rocchio-style methods for AF .Instead , our focus here is to investigate the robustness of Rocchio-style methods in terms of how much their performance depends on elaborate system tuning , and how difficult or how easy it is to get good performance through cross-corpus parameter optimization .Hence , we decided to use a relatively simple version of Rocchio as the baseline , i.e. , with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase .This simple version of Rocchio has been commonly used in the past TDT benchmark evaluations for topic tracking , and had strong performance in the TDT2004 evaluations for adaptive filtering with and without relevance feedback Section 5.1 .Results of more complex variants of Rocchio are also discussed when relevant .Logistic regression LR estimates the posterior probability of a topic given a document using a sigmoid functionwhere xr is the document vector whose elements are term r weights , w is the vector of regression coefficients , and y \u2208 +1 , \u2212 1 is the output variable corresponding to `` yes '' or `` no '' with respect to a particular topic .Given a training set of labeled documents D x1 , y1 , , xrn , yn r = L , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients `` the model parameters '' :This is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in O INF time for training per topic , where I is the average number of iterations needed for convergence , and N and F are the number of training documents and number of features respectively 14 .Once the regression coefficients are optimized on the training data , the filtering prediction on each incoming document is made as :Note that wr is constantly updated whenever a new relevance judgment is available in the testing phase of AF , while the optimal threshold \u03b8opt is constant , depending only on the predefined utility or cost function for evaluation .If T11SU is the metric , for example , with the penalty ratio of 2:1 for misses and false alarms Section 3.1 , the optimal threshold for LR is 1 / 2 + 1 = 0 .3 3 for all topics .We modified the standard above version of LR to allow more flexible optimization criteria as follows :where s yi is taken to be \u03b1 , \u03b2 and \u03b3 for query , positive and negative documents respectively , which are similar to those in Rocchio , giving different weights to the three kinds of training examples : topic descriptions `` queries '' , on-topic documents and off-topic documents .The second term in the objective function is for regularization , equivalent to adding a Gaussian prior to the regression coefficients with mean \u00b5 r and covariance variance matrix 1 / 2\u03bb \u22c5 \u0399 , where \u0399 is the identity matrix .Tuning \u03bb \u2265 0 is theoretically justified for reducing model complexity `` the effective degree of freedom '' and avoiding over-fitting on training data 5 .How to find an effective \u00b5r is an open issue for research , depending on the user 's belief about the parameter space and the optimal range .The solution of the modified objective function is called the Maximum A Posteriori MAP estimate , which reduces to the maximum likelihood solution for standard LR if \u03bb = 0 .", "conclusions": "We presented a cross-benchmark evaluation of incremental Rocchio and incremental LR in adaptive filtering , focusing on their robustness in terms of performance consistency with respect to cross-corpus parameter optimization .Our main conclusions from this study are the following :validation corpus is available , and a preferred choice when optimizing Ctrk is the objective , favoring recall over precision to an extreme .For future research we want to study explicit modeling of the temporal trends in topic distributions and content drifting ."}