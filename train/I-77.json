{"reader_keywords": ["successive negotiation encounter", "long term relationship", "utterance", "utilitarian interpretation", "ontology", "set predicate", "multiagent system", "logic agent architecture", "negotiation strategy", "view of acceptability", "acceptability view", "acceptance criterion", "component dialogue", "confidence measure"], "reader_keywords_stem": ["success negoti encount", "long term relationship", "utter", "utilitarian interpret", "ontolog", "set predic", "multiag system", "logic agent architectur", "negoti strategi", "view of accept", "accept view", "accept criterion", "compon dialogu", "confid measur"], "introduction": "In this paper we propose a new negotiation model to deal with long term relationships that are founded on successive negotiation encounters .The model is grounded on results from business and psychological studies 1 , 16 , 9 , and acknowledges that negotiation is an information exchange process as well as a utility exchange process 15 , 14 .We believe that if agents are to succeed in real application domains they have to reconcile both views : informational and gametheoretical .Our aim is to model trading scenarios where agents represent their human principals , and thus we want their behaviour to be comprehensible by humans and to respect usual human negotiation procedures , whilst being consistent with , and somehow extending , game theoretical and information theoretical results .In this sense , agents are not just utility maximisers , but aim at building long lasting relationships with progressing levels of intimacy that determine what balance in information and resource sharing is acceptable to them .These two concepts , intimacy and balance are key in the model , and enable us to understand competitive and co-operative game theory as two particular theories of agent relationships i.e. at different intimacy levels .These two theories are too specific and distinct to describe how a business relationship might grow because interactions have some aspects of these two extremes on a continuum in which , for example , agents reveal increasing amounts of private information as their intimacy grows .We do n't follow the ' Co-Opetition ' aproach 4 where co-operation and competition depend on the issue under negotiation , but instead we belief that the willingness to co-operate/compete affect all aspects in the negotiation process .Negotiation strategies can naturally be seen as procedures that select tactics used to attain a successful deal and to reach a target intimacy level .It is common in human settings to use tactics that compensate for unbalances in one dimension of a negotiation with unbalances in another dimension .In this sense , humans aim at a general sense of fairness in an interaction .In Section 2 we outline the aspects of human negotiation modelling that we cover in this work .Then , in Section 3 we introduce the negotiation language .Section 4 explains in outline the architecture and the concepts of intimacy and balance , and how they influence the negotiation .Section 5 contains a description of the different metrics used in the agent model including intimacy .Finally , Section 6 outlines how strategies and tactics use the LOGIC framework , intimacy and balance .", "title": "The LOGIC Negotiation Model", "author_keywords_stem": ["multiagent system", "negotiation"], "abstract": "Successful negotiators prepare by determining their position along five dimensions : Legitimacy , Options , Goals , Independence , and Commitment , LOGIC .We introduce a negotiation model based on these dimensions and on two primitive concepts : intimacy degree of closeness and balance degree of fairness .The intimacy is a pair of matrices that evaluate both an agent 's contribution to the relationship and its opponent 's contribution each from an information view and from a utilitarian view across the five LOGIC dimensions .The balance is the difference between these matrices .A relationship strategy maintains a target intimacy for each relationship that an agent would like the relationship to move towards in future .The negotiation strategy maintains a set of Options that are in-line with the current intimacy level , and then tactics wrap the Options in argumentation with the aim of attaining a successful deal and manipulating the successive negotiation balances towards the target intimacy .", "id": "I-77", "combined_keywords_stem": ["success negoti encount", "long term relationship", "utter", "utilitarian interpret", "ontolog", "set predic", "multiag system", "logic agent architectur", "negoti strategi", "view of accept", "accept view", "accept criterion", "compon dialogu", "confid measur", "negoti"], "combined_keywords": ["successive negotiation encounter", "long term relationship", "utterance", "utilitarian interpretation", "ontology", "set predicate", "multiagent system", "logic agent architecture", "negotiation strategy", "view of acceptability", "acceptability view", "acceptance criterion", "component dialogue", "confidence measure", "negotiation"], "author_keywords": ["multiagent system", "negotiation"], "method": "Before a negotiation starts human negotiators prepare the dialogic exchanges that can be made along the five LOGIC dimensions 7 :Negotiation dialogues , in this context , exchange dialogical moves , i.e. messages , with the intention of getting information about the opponent or giving away information about us along these five dimensions : request for information , propose options , inform about interests , issue promises , appeal to standards ... A key part of any negotiation process is to build a model of our opponent s along these dimensions .All utterances agents make during a negotiation give away information about their current LOGIC model , that is , about their legitimacy , options , goals , independence , and commitments .Also , several utterances can have a utilitarian interpretation in the sense that an agent can associate a preferential gain to them .For instance , an offer may inform our negotiation opponent about our willingness to sign a contract in the terms expressed in the offer , and at the same time the opponent can compute what is its associated expected utilitarian gain .These two views : informationbased and utility-based , are central in the model proposed in this paper .There is evidence from psychological studies that humans seek a balance in their negotiation relationships .The classical view 1 is that people perceive resource allocations as being distributively fair i.e. well balanced if they are proportional to inputs or contributions i.e. equitable .However , more recent studies 16 , 17 show that humans follow a richer set of norms of distributive justice depending on their intimacy level : equity , equality , and need .Equity being the allocation proportional to the effort e.g. the profit of a company goes to the stock holders proportional to their investment , equality being the allocation in equal amounts e.g. two friends eat the same amount of a cake cooked by one of them , and need being the allocation proportional to the need for the resource e.g. in case of food scarcity , a mother gives all food to her baby .For instance , if we are in a purely economic setting low intimacy we might request equity for the Options dimension but could accept equality in the Goals dimension .The perception of a relation being in balance i.e. fair depends strongly on the nature of the social relationships between individuals i.e. the intimacy level .In purely economical relationships e.g. , business , equity is perceived as more fair ; in relations where joint action or fostering of social relationships are the goal e.g. friends , equality is perceived as more fair ; and in situations where personal development or personal welfare are the goal e.g. family , allocations are usually based on need .We believe that the perception of balance in dialogues in negotiation or otherwise is grounded on social relationships , and that every dimension of an interaction between humans can be correlated to the social closeness , or intimacy , between the parties involved .According to the previous studies , the more intimacy across the five LOGIC dimensions the more the need norm is used , and the less intimacy the more the equity norm is used .This might be part of our social evolution .There is ample evidence that when human societies evolved from a hunter-gatherer structure1 to a shelterbased one2 the probability of survival increased when food was scarce .In this context , we can clearly see that , for instance , families exchange not only goods but also information and knowledge based on need , and that few families would consider their relationships as being unbalanced , and thus unfair , when there is a strong asymmetry in the exchanges a mother explaining everything to her children , or buying toys , does not expect reciprocity .In the case of partners there is some evidence 3 that the allocations of goods and burdens i.e. positive and negative utilities are perceived as fair , or in balance , based on equity for burdens and equality for goods .See Table 1 for some examples of desired balances along the LOGIC dimensions .The perceived balance in a negotiation dialogue allows negotiators to infer information about their opponent , about its LOGIC stance , and to compare their relationships with all negotiators .For instance , if we perceive that every time we request information it is provided , and that no significant questions are returned , or no complaints about not receiving information are given , then that probably means that our opponent perceives our social relationship to be very close .Alternatively , we can detect what issues are causing a burden to our opponent by observing an imbalance in the information or utilitarian senses on that issue .In order to define a language to structure agent dialogues we need an ontology that includes a minimum repertoire of elements : a set of concepts e.g. quantity , quality , material organised in a is-a hierarchy e.g. platypus is a mammal , Australian-dollar is a currency , and a set of relations over these concepts e.g. price beer , AUD .3 We model ontologies following an algebraic approach 8 as : An ontology is a tuple O = C , R , < , \u03c3 where :2In these societies there are family units , around a shelter , that represent the basic food sharing structure .Usually , food is accumulated at the shelter for future use .Then the food intake depends more on the need of the members .3Usually , a set of axioms defined over the concepts and relations is also required .We will omit this here .Legitimacy equity equity equity equality need Options equity equity equity mixeda need Goals equity need equity need need Independence equity equity equality need need Commitment equity equity equity mixed need aequity on burden , equality on goodwhere \u2264 is the traditional is-a hierarchy .To simplify computations in the computing of probability distributions we assume that there is a number of disjoint is-a trees covering different ontological spaces e.g. a tree for types of fabric , a tree for shapes of clothing , and so on .R contains relations between the concepts in the hierarchy , this is needed to define ` objects ' e.g. deals that are defined as a tuple of issues .The semantic distance between concepts within an ontology depends on how far away they are in the structure defined by the \u2264 relation .Semantic distance plays a fundamental role in strategies for information-based agency .How signed contracts , Commit \u00b7 , about objects in a particular semantic region , and their execution , Done \u00b7 , affect our decision making process about signing future contracts in nearby semantic regions is crucial to modelling the common sense that human beings apply in managing trading relationships .A measure 10 bases the semantic similarity between two concepts on the path length induced by \u2264 more distance in the \u2264 graph means less semantic similarity , and the depth of the subsumer concept common ancestor in the shortest path between the two concepts the deeper in the hierarchy , the closer the meaning of the concepts .Semantic similarity is then defined as :where l is the length i.e. number of hops of the shortest path between the concepts , h is the depth of the deepest concept subsuming both concepts , and \u03ba1 and \u03ba2 are parameters scaling the contributions of the shortest path length and the depth respectively .The shape of the language that \u03b1 uses to represent the information received and the content of its dialogues depends on two fundamental notions .First , when agents interact within an overarching institution they explicitly or implicitly accept the norms that will constrain their behaviour , and accept the established sanctions and penalties whenever norms are violated .Second , the dialogues in which \u03b1 engages are built around two fundamental actions : i passing information , and ii exchanging proposals and contracts .A contract \u03b4 = a , b between agents \u03b1 and \u03b2 is a pair where a and b represent the actions that agents \u03b1 and \u03b2 are responsible for respectively .Contracts signed by agents and information passed by agents , are similar to norms in the sense that they oblige agents to behave in a particular way , so as to satisfy the conditions of the contract , or to make the world consistent with the information passed .Contracts and Information can thus be thought of as normative statements that restrict an agent 's behaviour .Norms , contracts , and information have an obvious temporal dimension .Thus , an agent has to abide by a norm while it is inside an institution , a contract has a validity period , and a piece of information is true only during an interval in time .The set of norms affecting the behaviour of an agent defines the context that the agent has to take into account .\u03b1 's communication language has two fundamental primitives : Commit \u03b1 , \u03b2 , \u03d5 to represent , in \u03d5 , the world that \u03b1 aims at bringing about and that \u03b2 has the right to verify , complain about or claim compensation for any deviations from , and Done \u03bc to represent the event that a certain action \u03bc4 has taken place .In this way , norms , contracts , and information chunks will be represented as instances of Commit \u00b7 where \u03b1 and \u03b2 can be individual agents or institutions .C is :where \u03d5v is a formula with free variable v , illoc is any appropriate set of illocutionary particles , ` ; ' means sequencing , and context represents either previous agreements , previous illocutions , the ontological working context , that is a projection of the ontological trees that represent the focus of the conversation , or code that aligns the ontological differences between the speakers needed to interpret an action a. Representing an ontology as a set predicates in Prolog is simple .The set term contains instances of the ontology concepts and relations .5 For example , we can represent the following offer : `` If you spend a total of more than $ 100 in my shop during October then I will give you a 10 % discount on all goods in November '' , as :\u03be is an institution agent that reports the payment .4Without loss of generality we will assume that all actions are dialogical .5We assume the convention that C c means that c is an instance of concept C and r c1 , ... , cn implicitly determines that ci is an instance of the concept in the i-th position of the relation r.A multiagent system \u03b1 , ,31 , ... , ,3 n , \u03be , B1 , ... , Bt , contains an agent \u03b1 that interacts with other argumentation agents , ,3 i , information providing agents , Bj , and an institutional agent , \u03be , that represents the institution where we assume the interactions happen 2 .The institutional agent reports promptly and honestly on what actually occurs after an agent signs a contract , or makes some other form of commitment .In Section 4.1 this enables us to measure the difference between an utterance and a subsequent observation .The communication language C introduced in Section 3.2 enables us both to structure the dialogues and to structure the processing of the information gathered by agents .Agents have a probabilistic first-order internal language L used to represent a world model , Mt. A generic information-based architecture is described in detail in 15 .The LOGIC agent architecture is shown in Figure 1 .Agent \u03b1 acts in response to a need that is expressed in terms of the ontology .A need may be exogenous such as a need to trade profitably and may be triggered by another agent offering to trade , or endogenous such as \u03b1 deciding that it owns more wine than it requires .Needs trigger \u03b1 's goal/plan proactive reasoning , while other messages are dealt with by \u03b1 's reactive reasoning .6 Each plan prepares for the negotiation by assembling the contents of a ` LOGIC briefcase ' that the agent ` carries ' into the negotiation7 .The relationship strategy determines which agent to negotiate with for a given need ; it uses risk management analysis to preserve a strategic set of trading relationships for each mission-critical need this is not detailed here .For each trading relationship this strategy generates a relationship target that is expressed in the LOGIC framework as a desired level of intimacy to be achieved in the long term .Each negotiation consists of a dialogue , \u03a8t , between two agents with agent \u03b1 contributing utterance \u00b5 and the part6Each of \u03b1 's plans and reactions contain constructors for an initial world model Mt. Mt is then maintained from percepts received using update functions that transform percepts into constraints on Mt for details , see 14 , 15 .7Empirical evidence shows that in human negotiation , better outcomes are achieved by skewing the opening Options in favour of the proposer .We are unaware of any empirical investigation of this hypothesis for autonomous agents in real trading scenarios .ner ,3 contributing \u00b5 ' using the language described in Section 3.2 .Each dialogue , \u03a8t , is evaluated using the LOGIC framework in terms of the value of \u03a8t to both \u03b1 and ,3 see Section 5.2 .The negotiation strategy then determines the current set of Options 5i , and then the tactics , guided by the negotiation target , decide which , if any , of these Options to put forward and wraps them in argumentation dialogue see Section 6 .We now describe two of the distributions in Mt that support offer exchange .Pt acc \u03b1 , ,3 , x , 5 estimates the probability that \u03b1 should accept proposal 5 in satisfaction of her need x , where 5 = a , b is a pair of commitments , a for \u03b1 and b for ,3 .\u03b1 will accept 5 if : Pt acc \u03b1 , ,3 , x , 5 > c , for level of certainty c .This estimate is compounded from subjective and objective views of acceptability .The subjective estimate takes account of : the extent to which the enactment of 5 will satisfy \u03b1 's need x , how much 5 is ` worth ' to \u03b1 , and the extent to which \u03b1 believes that she will be in a position to execute her commitment a 14 , 15 .S\u03b1 ,3 , a is a random variable denoting \u03b1 's estimate of ,3 's subjective valuation of a over some finite , numerical evaluation space .The objective estimate captures whether 5 is acceptable on the open market , and variable U\u03b1 b denotes \u03b1 's open-market valuation of the enactment of commitment b , again taken over some finite numerical valuation space .We also consider needs , the variable T\u03b1 ,3 , a denotes \u03b1 's estimate of the strength of ,3 's motivating need for the enactment of commitment a over a valuation space .Then for 5 = a , b : Pt acc \u03b1 , ,3 , x , 5 =where g \u2208 0 , 1 is \u03b1 's greed , h \u2208 0 , 1 is \u03b1 's degree of altruism , and s \u2248 1 is derived from the stance8 described in Section 6 .The parameters g and h are independent .We can imagine a relationship that begins with g = 1 and h = 0 .Then as the agents share increasing amounts of their information about their open market valuations g gradually reduces to 0 , and then as they share increasing amounts of information about their needs h increases to 1 .The basis for the acceptance criterion has thus developed from equity to equality , and then to need .Pt acc ,3 , \u03b1 , 5 estimates the probability that ,3 would accept 5 , by observing ,3 's responses .For example , if ,3 sends the message Offer 51 then \u03b1 derives the constraint : Pt acc ,3 , \u03b1 , 51 = 1 on the distribution Pt ,3 , \u03b1 , 5 , and if this is a counter offer to a former offer of \u03b1 's , 50 , then : Pt acc ,3 , \u03b1 , 50 = 0 .In the not-atypical special case of multi-issue bargaining where the agents ' preferences over the individual issues only are known and are complementary to each other 's , maximum entropy reasoning can be applied to estimate the probability that any multi-issue 5 will be acceptable to ,3 by enumerating the possible worlds that represent ,3 's `` limit of acceptability '' 6 .\u03b1 's world model consists of probability distributions that represent its uncertainty in the world state .\u03b1 is interested 8If \u03b1 chooses to inflate her opening Options then this is achieved in Section 6 by increasing the value of s .If s '' 1 then a deal may not be possible .This illustrates the wellknown inefficiency of bilateral bargaining established analytically by Myerson and Satterthwaite in 1983 .in the degree to which an utterance accurately describes what will subsequently be observed .All observations about the world are received as utterances from an all-truthful institution agent \u03be .For example , if \u03b2 communicates the goal `` I am hungry '' and the subsequent negotiation terminates with \u03b2 purchasing a book from \u03b1 by \u03be advising \u03b1 that a certain amount of money has been credited to \u03b1 's account then \u03b1 may conclude that the goal that \u03b2 chose to satisfy was something other than hunger .So , \u03b1 's world model contains probability distributions that represent its uncertain expectations of what will be observed on the basis of utterances received .We represent the relationship between utterance , \u03d5 , and subsequent observation , \u03d5 ~ , by Pt \u03d5 ~ I\u03d5 E Mt , where \u03d5 ~ and \u03d5 may be ontological categories in the interest of computational feasibility .For example , if \u03d5 is `` I will deliver a bucket of fish to you tomorrow '' then the distribution P \u03d5 ~ I\u03d5 need not be over all possible things that \u03b2 might do , but could be over ontological categories that summarise \u03b2 's possible actions .In the absence of in-coming utterances , the conditional probabilities , Pt \u03d5 ~ I\u03d5 , should tend to ignorance as represented by a decay limit distribution D \u03d5 ~ I\u03d5 .\u03b1 may have background knowledge concerning D \u03d5 ~ I\u03d5 as t + oc , otherwise \u03b1 may assume that it has maximum entropy whilst being consistent with the data .In general , given a distribution , Pt Xi , and a decay limit distribution D Xi , Pt Xi decays by :where \u0394i is the decay function for the Xi satisfying the property that limt \u2192 \u221e Pt Xi = D Xi .For example , \u0394i could be linear : Pt +1 Xi = 1 \u03bdi x D Xi + \u03bdi x Pt Xi , where \u03bdi < 1 is the decay rate for the i ' th distribution .Either the decay function or the decay limit distribution could also be a function of time : \u0394ti and Dt Xi .Suppose that \u03b1 receives an utterance \u03bc = illoc \u03b1 , \u03b2 , \u03d5 , t from agent \u03b2 at time t. Suppose that \u03b1 attaches an epistemic belief Rt \u03b1 , \u03b2 , \u03bc to \u03bc this probability takes account of \u03b1 's level of personal caution .We model the update of Pt \u03d5 ~ I\u03d5 in two cases , one for observations given \u03d5 , second for observations given \u03c6 in the semantic neighbourhood ofy is the Kullback-Leibler distance between two probability distributions x ~ and y ~ 11 .Finally incorporating Eqn .2 we obtain the method for updating a distribution Pt \u03d5 ~ I\u03d5 on receipt of a message \u03bc :This procedure deals with integrity decay , and with two probabilities : first , the probability z in the utterance \u03bc , and second the belief Rt \u03b1 , \u03b2 , \u03bc that \u03b1 attached to \u03bc .The sim method : Given as above \u03bc = illoc \u03b1 , \u03b2 , \u03d5 , t and the observation \u03d5k we define the vector t ~ bywith 1\u03c61 , \u03c62 , ... , \u03c6p1 the set of all possible observations in the context of \u03c6 and i = 1 , ... , p. t ~ is not a probability distribution .The multiplying factor Sim \u03d5 ~ , \u03c6 limits the variation of probability to those formulae whose ontological context is not too far away from the observation .The posterior Pt +1 \u03c6 ~ I\u03c6 is obtained with Equation 3 with ~ r \u03bc defined to be the normalisation of ~ t .The valuation method : For a given \u03c6k , wexp \u03c6k = Pm j = 1 Pt \u03c6jI\u03c6k \u2022 w \u03c6j is \u03b1 's expectation of the value of what will be observed given that \u03b2 has stated that \u03c6k will be observed , for some measure w .Now suppose that , as before , \u03b1 observes \u03d5k after agent \u03b2 has stated \u03d5 .\u03b1 revises the prior estimate of the expected valuation wexp \u03c6k in the light of the observation \u03d5k to :First , if \u03d5k is observed then \u03b1 may set Pt +1 \u03d5kI\u03d5 to some value d where 1\u03d51 , \u03d52 , ... , \u03d5m1 is the set of all possible observations .We estimate the complete posterior distribution Pt +1 \u03d5 ~ I\u03d5 by applying the principle of minimum relative entropy9 as follows .Let ~ p \u03bc be the distribution : 9Given a probability distribution ~ q , the minimum relative entropy distribution p ~ = p1 , ... , pI subject to a set of J linear constraints g ~ = gj ~ p = ~ aj \u2022 p ~ cj = 01 , j = 1 , ... , J that must include the constraint Pi pi 1 = 0 is : p ~ =q .This may be calculated by introducing Lagrange multipliers ~ \u03bb : L ~ p , ~ \u03bb = Pj pj log p q + \u03bb ~ \u2022 ~ g. Minimising L , \u2202 L \u2202 \u03bb = gj ~ p = 01 , j = 1 , ... , J is the set of given constraints ~ g , and a solution to \u2202 L \u2202 pi = 0 , i = 1 , ... , I leads eventually to ~ p. Entropy-based inference is a form of Bayesian inference that is convenient when the data is sparse 5 and encapsulates common-sense reasoning 12 .for some function g ~ the idea being , for example , that if the execution , \u03d5k , of the commitment , \u03d5 , to supply cheese was devalued then \u03b1 's expectation of the value of a commitment , \u03c6 , to supply wine should decrease .We estimate the posterior by applying the principle of minimum relative entropy as for Equation 3 , where the distribution ~ p \u03bc = ~ p \u03c6 | \u03c6 satisfies the constraint :A dialogue , \u03a8t , between agents \u03b1 and \u03b2 is a sequence of inter-related utterances in context .A relationship , \u03a8 \u2217 t , is a sequence of dialogues .We first measure the confidence that an agent has for another by observing , for each utterance , the difference between what is said the utterance and whatsubsequently occurs the observation .Second we evaluate each dialogue as it progresses in terms of the LOGIC framework this evaluation employs the confidence measures .Finally we define the intimacy of a relationship as an aggregation of the value of its component dialogues .Confidence measures generalise what are commonly called trust , reliability and reputation measures into a single computational framework that spans the LOGIC categories .In Section 5.2 confidence measures are applied to valuing fulfilment of promises in the Legitimacy category we formerly called this `` honour '' 14 , to the execution of commitments we formerly called this `` trust '' 13 , and to valuing dialogues in the Goals category we formerly called this `` reliability '' 14 .Ideal observations .Consider a distribution of observations that represent \u03b1 's `` ideal '' in the sense that it is the best that \u03b1 could reasonably expect to observe .This distribution will be a function of \u03b1 's context with \u03b2 denoted by e , and is PtI \u03d5 ~ I\u03d5 , e .Here we measure the relative entropy between this ideal distribution , PtI \u03d5 ~ I\u03d5 , e , and the distribution of expected observations , Pt \u03d5 ~ I\u03d5 .That is :where the `` 1 '' is an arbitrarily chosen constant being the maximum value that this measure may have .This equation measures confidence for a single statement \u03d5 .It makes sense to aggregate these values over a class of statements , say over those \u03d5 that are in the ontological context o , that is \u03d5 < o :where Pt\u03b2 \u03d5 is a probability distribution over the space of statements that the next statement \u03b2 will make to \u03b1 is \u03d5 .Similarly , for an overall estimate of \u03b2 's confidence in \u03b1 :Preferred observations .The previous measure requires that an ideal distribution , PtI \u03d5 ~ I\u03d5 , e , has to be specified for each \u03d5 .Here we measure the extent to which the observation \u03d5 ~ is preferable to the original statement \u03d5 .Given a predicate Prefer c1 , c2 , e meaning that \u03b1 prefers c1 to c2 in environment e .Then if \u03d5 < o :Certainty in observation .Here we measure the consistency in expected acceptable observations , or `` the lack of expected uncertainty in those possible observations that are better than the original statement '' .If \u03d5 < o let : 4D + \u03d5 , o , \u03ba = J\u03d5 ~ I Pt Prefer \u03d5 ~ , \u03d5 , o > \u03ba for some constant \u03ba , and :As above we aggregate this measure for observations in a particular context o , and measure confidence as before .Computational Note .The various measures given above involve extensive calculations .For example , Eqn .4 contains P\u03d5 ' that sums over all possible observations \u03d5 ~ .We obtain a more computationally friendly measure by appealing to the structure of the ontology described in Section 3.2 , and the right-hand side of Eqn .4 may be approximated to :where Pt\u03b7 , I \u03d5 ~ I\u03d5 , e is the normalisation of PtI \u03d5 ~ I\u03d5 , e for Sim \u03d5 ~ , \u03d5 > \u03b7 , and similarly for Pt\u03b7 \u03d5 ~ I\u03d5 .The extent of this calculation is controlled by the parameter \u03b7 .An even tighter restriction may be obtained with : Sim \u03d5 ~ , \u03d5 > \u03b7 and \u03d5 ~ < \u03c8 for some \u03c8 .Suppose that a negotiation commences at time s , and by time t a string of utterances , 4Dt = \u03bc1 , ... , \u03bcn has been exchanged between agent \u03b1 and agent \u03b2 .This negotiation dialogue is evaluated by \u03b1 in the context of \u03b1 's world model at time s , Ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources 1\u03b8i .Let \u03a8t = 4Dt , Ms , e , then \u03b1 estimates the value of this dialogue to itself in the context of Ms and e as a 2 x 5 array V\u03b1 \u03a8twhere the I \u2022 and U \u2022 functions are information-based and utility-based measures respectively as we now describe .\u03b1 estimates the value of this dialogue to \u03b2 as V\u03b2 \u03a8t by assuming that \u03b2 's reasoning apparatus mirrors its own .In general terms , the information-based valuations measure the reduction in uncertainty , or information gain , that the dialogue gives to each agent , they are expressed in terms of decrease in entropy that can always be calculated .The utility-based valuations measure utility gain are expressed in terms of `` some suitable '' utility evaluation function U \u2022 that can be difficult to define .This is one reason why the utilitarian approach has no natural extension to the management of argumentation that is achieved here by our informationbased approach .For example , if \u03b1 receives the utterance `` Today is Tuesday '' then this may be translated into a constraint on a single distribution , and the resulting decrease in entropy is the information gain .Attaching a utilitarian measure to this utterance may not be so simple .We use the term `` 2 x 5 array '' loosely to describe V\u03b1 in that the elements of the array are lists of measures that will be determined by the agent 's requirements .Table 2 shows a sample measure for each of the ten categories , in it the dialogue commences at time s and terminates at time t .In that Table , U \u2022 is a suitable utility evaluation function , needs \u03b2 , \u03c7 means `` agent \u03b2 needs the need \u03c7 '' , cho \u03b2 , \u03c7 , \u03b3 means `` agent \u03b2 satisfies need \u03c7 by choosing to negotiatewith agent \u03b3 '' , N is the set of needs chosen from the ontology at some suitable level of abstraction , Tt is the set of offers on the table at time t , com \u03b2 , \u03b3 , b means `` agent \u03b2 has an outstanding commitment with agent \u03b3 to execute the commitment b '' where b is defined in the ontology at some suitable level of abstraction , B is the number of such commitments , and there are n + 1 agents in the system .The balance in a negotiation dialogue , \u03a8t , is defined as : B\u03b1\u03b2 \u03a8t = V\u03b1 \u03a8t ~ V\u03b2 \u03a8t for an element-by-element difference operator ~ that respects the structure of V \u03a8t .The intimacy between agents \u03b1 and \u03b2 , I * t\u03b1\u03b2 , is the pattern of the two 2 \u00d7 5 arrays V * t \u03b1 and V * t \u03b2 that are computed by an update function as each negotiation round terminates ,where \u03bd is the learning rate , and x = \u03b1 , \u03b2 .Additionally , V * t x continually decays by : Vx * t +1 = \u03c4 \u00d7 V * t x + 1 \u2212 \u03c4 \u00d7 Dx , where x = \u03b1 , \u03b2 ; \u03c4 is the decay rate , and Dx is a 2 \u00d7 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction .Dx is the reputation of agent x .The relationship balance between agents \u03b1 and \u03b2 is : B * t\u03b2 .In particular , the intimacy determines values for the parameters g and h in Equation 1 .As a simple example , if both IO\u03b1 \u03a8 * t and IO\u03b2 \u03a8 * t increase then g decreases , and as the remaining eight information-based LOGIC components increase , h increases .The notion of balance may be applied to pairs of utterances by treating them as degenerate dialogues .In simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining , and extends to a tit-for-tat argumentation strategy by applying the same principle across the LOGIC framework .", "conclusions": "Each negotiation has to achieve two goals .First it may be intended to achieve some contractual outcome .Second it will aim to contribute to the growth , or decline , of the relationship intimacy .We now describe in greater detail the contents of the `` Negotiation '' box in Figure 1 .The negotiation literature consistently advises that an agent 's behaviour should not be predictable even in close , intimate relationships .The required variation of behaviour is normally described as varying the negotiation stance that informally varies from `` friendly guy '' to `` tough guy '' .The stance is shown in Figure 1 , it injects bounded random noise into the process , where the bound tightens as intimacy increases .The stance , St\u03b1\u03b2 , is a 2 \u00d7 5 matrix of randomly chosen multipliers , each \u2248 1 , that perturbs \u03b1 's actions .The value in the x , y position in the matrix , where x = I , U and y = L , O , G , I , C , is chosen at random from 1 \u03b1\u03b2 , x , y is the l I \u2217 t \u03b1\u03b2 , x , y , l I * t \u03b1\u03b2 , x , y where l I * t bound , and I * t \u03b1\u03b2 is the intimacy .The negotiation strategy is concerned with maintaining a working set of Options .If the set of options is empty then \u03b1 will quit the negotiation .\u03b1 perturbs the acceptance machinery see Section 4 by deriving s from the St\u03b1\u03b2 matrix such as the value at the I , O position .In line with the comment in Footnote 7 , in the early stages of the negotiation \u03b1 may decide to inflate her opening Options .This is achieved by increasing the value of s in Equation 1 .The following strategy uses the machinery described in Section 4 .Fix h , g , s and c , set the Options to the empty set , let Dts = \u03b4 | Pt acc \u03b1 , \u03b2 , \u03c7 , \u03b4 > c , then :\u03b4 = arg maxx Pt acc \u03b2 , \u03b1 , x | x \u2208 Dts to Options , remove y \u2208 Dts | Sim y , \u03b4 < k for some k from Dts By using Pt acc \u03b2 , \u03b1 , \u03b4 this strategy reacts to \u03b2 's history of Propose and Reject utterances .Negotiation tactics are concerned with selecting some Options and wrapping them in argumentation .Prior interactions with agent \u03b2 will have produced an intimacy pattern expressed in the form of V * t \u03b1 , V * t .Suppose that the rela\u03b2 .Following from Equation 5 , \u03b1 will want to achieve a negotiation target , N\u03b2 \u03a8t such that : \u03bd \u00b7 N\u03b2 \u03a8t + 1 \u2212 \u03bd \u00b7 V\u03b2 * t is `` a bit on the T\u03b2 * t side of '' V * tfor small \u03ba \u2208 0 , \u03bd that represents \u03b1 's desired rate of development for her relationship with \u03b2 .N\u03b2 \u03a8t is a 2 \u00d7 5 matrix containing variations in the LOGIC dimensions that \u03b1 would like to reveal to \u03b2 during \u03a8t e.g. I 'll pass a bit more information on options than usual , I 'll be stronger in concessions on options , etc. .It is reasonable to expect \u03b2 to progress towards her target at the same rate and N\u03b1 \u03a8t is calculated by replacing \u03b2 by \u03b1 in Equation 6 .N\u03b1 \u03a8t is what \u03b1 hopes to receive from \u03b2 during \u03a8t .This gives a negotiation balance target of : N\u03b1 \u03a8t ~ N\u03b2 \u03a8t that can be used as the foundation for reactive tactics by striving to maintain this balance across the LOGIC dimensions .A cautious tactic could use the balance to bound the response \u03bc to each utterance \u03bc ' from \u03b2 by the constraint : V\u03b1 \u03bc ' ~ V\u03b2 \u03bc \u2248 St\u03b1\u03b2 \u2297 N\u03b1 \u03a8t ~ N\u03b2 \u03a8t , where \u2297 is element-by-element matrix multiplication , and St\u03b1\u03b2 is the stance .A less neurotic tactic could attempt to achieve the target negotiation balance over the anticipated complete dialogue .If a balance bound requires negative information revelation in one LOGIC category then \u03b1 will contribute nothing to it , and will leave this to the natural decay to the reputation D as described above ."}