{"reader_keywords": ["spoken term detection", "vocabulary", "speech datum retrieval", "phonetic transcript", "vocabulary independent system", "speech recognizer", "indexing timestamp", "word index", "phonetic index", "index merging", "oov search", "automatic speech recognition"], "reader_keywords_stem": ["spoken term detect", "vocabulari", "speech data retriev", "phonet transcript", "vocabulari independ system", "speech recogn", "index timestamp", "word index", "phonet index", "index merg", "oov search", "automat speech recognit"], "introduction": "The rapidly increasing amount of spoken data calls for solutions to index and search this data .The classical approach consists of converting the speech to word transcripts using a large vocabulary continuous speech recognition LVCSR tool .In the past decade , most of the research efforts on spoken data retrieval have focused on extending classical IR techniques to word transcripts .Some of these works have been done in the framework of the NIST TREC Spoken Document Retrieval tracks and are described by Garofolo et al. 12 .These tracks focused on retrieval from a corpus of broadcast news stories spoken by professionals .One of the conclusions of those tracks was that the effectiveness of retrieval mostly depends on the accuracy of the transcripts .While the accuracy of automatic speech recognition ASR systems depends on the scenario and environment , state-of-the-art systems achieved better than 90 % accuracy in transcription of such data .In 2000 , Garofolo et al. concluded that `` Spoken document retrieval is a solved problem '' 12 .However , a significant drawback of such approaches is that search on queries containing out-of-vocabulary OOV terms will not return any results .OOV terms are missing words from the ASR system vocabulary and are replaced in the output transcript by alternatives that are probable , given the recognition acoustic model and the language model .It has been experimentally observed that over 10 % of user queries can contain OOV terms 16 , as queries often relate to named entities that typically have a poor coverage in the ASR vocabulary .The effects of OOV query terms in spoken data retrieval are discussed by Woodland et al. 28 .In many applications the OOV rate may get worse over time unless the recognizer 's vocabulary is periodically updated .Another approach consists of converting the speech to phonetic transcripts and representing the query as a sequence of phones .The retrieval is based on searching the sequence of phones representing the query in the phonetic transcripts .The main drawback of this approach is the inherent high error rate of the transcripts .Therefore , such approach can not be an alternative to word transcripts , especially for in-vocabulary IV query terms that are part of the vocabulary of the ASR system .A solution would be to combine the two different approaches presented above : we index both word transcripts and phonetic transcripts ; during query processing , the information is retrieved from the word index for IV terms and from the phonetic index for OOV terms .We would like to be able to process also hybrid queries , i.e , queries that include both IV and OOV terms .Consequently , we need to merge pieces of information retrieved from word index and phonetic index .Proximity information on the occurrencesof the query terms is required for phrase search and for proximity-based ranking .In classical IR , the index stores for each occurrence of a term , its offset .Therefore , we can not merge posting lists retrieved by phonetic index with those retrieved by word index since the offset of the occurrences retrieved from the two different indices are not comparable .The only element of comparison between phonetic and word transcripts are the timestamps .No previous work combining word and phonetic approach has been done on phrase search .We present a novel scheme for information retrieval that consists of storing , during the indexing process , for each unit of indexing phone or word its timestamp .We search queries by merging the information retrieved from the two different indices , word index and phonetic index , according to the timestamps of the query terms .We analyze the retrieval effectiveness of this approach on the NIST Spoken Term Detection 2006 evaluation data 1 .The paper is organized as follows .We describe the audio processing in Section 2 .The indexing and retrieval methods are presented in section 3 .Experimental setup and results are given in Section 4 .In Section 5 , we give an overview of related work .Finally , we conclude in Section 6 .", "title": "Vocabulary Independent Spoken Term Detection", "author_keywords_stem": ["speech retrieval", "speak term detection", "out-of-vocabulary"], "abstract": "We are interested in retrieving information from speech data like broadcast news , telephone conversations and roundtable meetings .Today , most systems use large vocabulary continuous speech recognition tools to produce word transcripts ; the transcripts are indexed and query terms are retrieved from the index .However , query terms that are not part of the recognizer 's vocabulary can not be retrieved , and the recall of the search is affected .In addition to the output word transcript , advanced systems provide also phonetic transcripts , against which query terms can be matched phonetically .Such phonetic transcripts suffer from lower accuracy and can not be an alternative to word transcripts .We present a vocabulary independent system that can handle arbitrary queries , exploiting the information provided by having both word transcripts and phonetic transcripts .A speech recognizer generates word confusion networks and phonetic lattices .The transcripts are indexed for query processing and ranking purpose .The value of the proposed method is demonstrated by the relative high performance of our system , which received the highest overall ranking for US English speech data in the recent NIST Spoken Term Detection evaluation 1 .", "id": "H-52", "combined_keywords_stem": ["spoken term detect", "vocabulari", "speech data retriev", "phonet transcript", "vocabulari independ system", "speech recogn", "index timestamp", "word index", "phonet index", "index merg", "oov search", "automat speech recognit", "speech retriev", "speak term detect", "out-of-vocabulari"], "combined_keywords": ["spoken term detection", "vocabulary", "speech datum retrieval", "phonetic transcript", "vocabulary independent system", "speech recognizer", "indexing timestamp", "word index", "phonetic index", "index merging", "oov search", "automatic speech recognition", "speech retrieval", "speak term detection", "out-of-vocabulary"], "author_keywords": ["speech retrieval", "speak term detection", "out-of-vocabulary"], "method": "We use an ASR system for transcribing speech data .It works in speaker-independent mode .For best recognition results , a speaker-independent acoustic model and a language model are trained in advance on data with similar characteristics .Typically , ASR generates lattices that can be considered as directed acyclic graphs .Each vertex in a lattice is associated with a timestamp and each edge u , v is labeled with a word or phone hypothesis and its prior probability , which is the probability of the signal delimited by the timestamps of the vertices u and v , given the hypothesis .The 1-best path transcript is obtained from the lattice using dynamic programming techniques .Mangu et al. 18 and Hakkani-Tur et al. 13 propose a compact representation of a word lattice called word confusion network WCN .Each edge u , v is labeled with a word hypothesis and its posterior probability , i.e. , the probability of the word given the signal .One of the main advantages of WCN is that it also provides an alignment for all of the words in the lattice .As explained in 13 , the three main steps for building a WCN from a word lattice are as follows :The 1-best path of a WCN is obtained from the path containing the best hypotheses .As stated in 18 , although WCNs are more compact than word lattices , in general the 1-best path obtained from WCN has a better word accuracy than the 1-best path obtained from the corresponding word lattice .Typical structures of a lattice and a WCN are given inThe main problem with retrieving information from spoken data is the low accuracy of the transcription particularly on terms of interest such as named entities and content words .Generally , the accuracy of a word transcript is characterized by its word error rate WER .There are three kinds of errors that can occur in a transcript : substitution of a term that is part of the speech by another term , deletion of a spoken term that is part of the speech and insertion of a term that is not part of the speech .Substitutions and deletions reflect the fact that an occurrence of a term in the speech signal is not recognized .These misses reduce the recall of the search .Substitutions and insertions reflect the fact that a term which is not part of the speech signal appears in the transcript .These misses reduce the precision of the search .Search recall can be enhanced by expanding the transcript with extra words .These words can be taken from the other alternatives provided by the WCN ; these alternatives may have been spoken but were not the top choice of the ASR .Such an expansion tends to correct the substitutions and the deletions and consequently , might improve recall but will probably reduce precision .Using an appropriate ranking model , we can avoid the decrease in precision .Mamou et al. have presented in 17 the enhancement in the recall and the MAP by searching on WCN instead of considering only the 1-best path word transcript in the context of spoken document retrieval .We have adapted this model of IV search to term detection .In word transcripts , OOV terms are deleted or substituted .Therefore , the usage of phonetic transcripts is more desirable .However , due to their low accuracy , we have preferred to use only the 1-best path extracted from the phonetic lattices .We will show that the usage of phonetic transcripts tends to improve the recall without affecting the precision too much , using an appropriate ranking .As stated in the STD 2006 evaluation plan 2 , the task consists in finding all the exact matches of a specific queryin a given corpus of speech data .A query is a phrase containing several words .The queries are text and not speech .Note that this task is different from the more classical task of spoken document retrieval .Manual transcripts of the speech are not provided but are used by the evaluators to find true occurrences .By definition , true occurrences of a query are found automatically by searching the manual transcripts using the following rule : the gap between adjacent words in a query must be less than 0.5 seconds in the corresponding speech .For evaluating the results , each system output occurrence is judged as correct or not according to whether it is `` close '' in time to a true occurrence of the query retrieved from manual transcripts ; it is judged as correct if the midpoint of the system output occurrence is less than or equal to 0.5 seconds from the time span of a true occurrence of the query .We have used the same indexing process for WCN and phonetic transcripts .Each occurrence of a unit of indexing word or phone u in a transcript D is indexed with the following information :Note that since the task is to find exact matches of the phrase queries , we have not filtered stopwords and the corpus is not stemmed before indexing .In the following , we present our approach for accomplishing the STD task using the indices described above .The terms are extracted from the query .The vocabulary of the ASR system building word transcripts is given .Terms that are part of this vocabulary are IV terms ; the other terms are OOV .For an IV query term , the posting list is extracted from the word index .For an OOV query term , the term is converted to a sequence of phones using a joint maximum entropy N-gram model 10 .For example , the term prosody is converted to the sequence of phones p , r , aa , z , ih , d , iy .The posting list of each phone is extracted from the phonetic index .The next step consists of merging the different posting lists according to the timestamp of the occurrences in order to create results matching the query .First , we check that the words and phones appear in the right order according to their begin times .Second , we check that the gap in time between adjacent words and phones is `` reasonable '' .Conforming to the requirements of the STD evaluation , the distance in time between two adjacent query terms must be less than 0.5 seconds .For OOV search , we check that the distance in time between two adjacent phones of a query term is less that 0.2 seconds ; this value has been determined empirically .In such a way , we can reduce the effect of insertion errors since we allow insertions between the adjacent words and phones .Our query processing does not allow substitutions and deletions .Example : Let us consider the phrase query prosody research .The term prosody is OOV and the term research is IV .The term prosody is converted to the sequence of phones p , r , aa , z , ih , d , iy .The posting list of each phone is extracted from the phonetic index .We merge the posting lists of the phones such that the sequence of phones appears in the right order and the gap in time between the pairs of phones p , r , r , aa , aa , z , z , ih , ih , d , d , iy is less than 0.2 seconds .We obtain occurrences of the term prosody .The posting list of research is extracted from the word index and we merge it with the occurrences found for prosody such that they appear in the right order and the distance in time between prosody and research is less than 0.5 seconds .Note that our indexing model allows to search for different types of queries :query processing , the posting lists of the IV terms retrieved from the word index are merged with the posting lists of the OOV terms retrieved from the phonetic index .The merging is possible since we have stored the timestamps for each unit of indexing word and phone in both indices .The STD evaluation has focused on the fourth query type .It is the hardest task since we need to combine posting lists retrieved from phonetic and word indices .Since IV terms and OOV terms are retrieved from two different indices , we propose two different functions for scoring an occurrence of a term ; afterward , an aggregate score is assigned to the query based on the scores of the query terms .Because the task is term detection , we do not use a document frequency criterion for ranking the occurrences .Let us consider a query Q = k0 , ... , kn , associated with a boosting vector B = B1 , ... , Bj .This vector associates a boosting factor to each rank of the different hypotheses ; the boosting factors are normalized between 0 and 1 .If the rank r is larger than j , we assume Br = 0 .For IV term ranking , we extend the work of Mamou et al. 17 on spoken document retrieval to term detection .We use the information provided by the word index .We define the score score k , t , D of a keyword k occurring at a time t in the transcript D , by the following formula : score k , t , D = Brank k | t , D \u00d7 Pr k | t , D Note that 0 < score k , t , D < 1 .For OOV term ranking , we use the information provided by the phonetic index .We give a higher rank to occurrences of OOV terms that contain phones close in time to each other .We define a scoring function that is related to the average gap in time between the different phones .Let us consider a keyword k converted to the sequence of phones pk0 , ... , pkl .We define the normalized score score k , tk0 , D of a keyword k = pk0 , ... , pkl , where each pki occurs at time tki with a duration of dki in the transcript D , by the following formula : l Note that according to what we have ex-plained in Section 3.3 , we have ` d1 < i < l , 0 < tki tki \u2212 1 + dki \u2212 1 < 0.2 sec , 0 < 5 x tki tki \u2212 1 + dki \u2212 1 < 1 , and consequently , 0 < score k , tk0 , D < 1 .The duration of the keyword occurrence is tkl tk0 + dkl .Example : let us consider the sequence p , r , aa , z , ih , d , iy and two different occurrences of the sequence .For each phone , we give the begin time and the duration in second .Occurrence 1 : p , 0.25 , 0.01 , r , 0.36 , 0.01 , aa , 0.37 , 0.01 , z , 0.38 , 0.01 , ih , 0.39 , 0.01 , d , 0.4 , 0.01 , iy , 0.52 , 0.01 .Occurrence 2 : p , 0.45 , 0.01 , r , 0.46 , 0.01 , aa , 0.47 , 0.01 , z , 0.48 , 0.01 , ih , 0.49 , 0.01 , d , 0.5 , 0.01 , iy , 0.51 , 0.01 .According to our formula , the score of the first occurrence is 0.83 and the score of the second occurrence is 1 .In the first occurrence , there are probably some insertion or silence between the phone p and r , and between the phone d and iy .The silence can be due to the fact that the phones belongs to two different words ans therefore , it is not an occurrence of the term prosody .The score of an occurrence of a query Q at time t0 in the document D is determined by the multiplication of the score of each keyword ki , where each ki occurs at time ti with a duration di in the transcript D :Note that according to what we have ex-plained in Section 3.3 , we have ` d1 < i < n , 0 < ti ti \u2212 1 + di \u2212 1 < 0.5 sec .Our goal is to estimate for each found occurrence how likely the query appears .It is different from classical IR that aims to rank the results and not to score them .Since the probability to have a false alarm is inversely proportional to the length of the phrase query , we have boosted the score of queries by a \u03b3n exponent , that is related to the number of keywords in the phrase .We have determined empirically the value of \u03b3n = 1/n .The begin time of the query occurrence is determined by the begin time t0 of the first query term and the duration of the query occurrence by tn t0 + dn .Our corpus consists of the evaluation set provided by NIST for the STD 2006 evaluation 1 .It includes three different source types in US English : three hours of broadcast news BNEWS , three hours of conversational telephony speech CTS and two hours of conference room meetings CONFMTG .As shown in Section 4.2 , these different collections have different accuracies .CTS and CONFMTG are spontaneous speech .For the experiments , we have processed the query set provided by NIST that includes 1100 queries .Each query is a phrase containing between one to five terms , common and rare terms , terms that are in the manual transcripts and those that are not .Testing and determination of empirical values have been achieved on another set of speech data and queries , the development set , also provided by NIST .We have used the IBM research prototype ASR system , described in 26 , for transcribing speech data .We have produced WCNs for the three different source types .1-best phonetic transcripts were generated only for BNEWS and CTS , since CONFMTG phonetic transcripts have too low accuracy .We have adapted Juru 7 , a full-text search library written in Java , to index the transcripts and to store the timestamps of the words and phones ; search results have been retrieved as described in Section 3 .For each found occurrence of the given query , our system outputs : the location of the term in the audio recording begin time and duration , the score indicating how likely is the occurrence of query , as defined in Section 3.4 and a hard binary decision as to whether the detection is correct .We measure precision and recall by comparing the results obtained over the automatic transcripts only the results having true hard decision to the results obtained over the reference manual transcripts .Our aim is to evaluate the ability of the suggested retrieval approach to handle transcribed speech data .Thus , the closer the automatic results to the manual results is , the better the search effectiveness over the automatic transcripts will be .The results returned from the manual transcription for a given query are considered relevant and are expected to be retrieved with highest scores .This approach for measuring search effectiveness using manual data as a reference is very common in speech retrieval research 25 , 22 , 8 , 9 , 17 .Beside the recall and the precision , we use the evaluation measures defined by NIST for the 2006 STD evaluation 2 : the Actual Term-Weighted Value ATWV and the Maximum Term-Weighted Value MTWV .The term-weighted value TWV is computed by first computing the miss and false alarm probabilities for each query separately , then using these and an arbitrarily chosen prior probability to compute query-specific values , and finally averaging these query-specific values over all queries q to produce an overall system value :where \u03b2 = VC Pr \u2212 1 q 1 .\u03b8 is the detection threshold .For the evaluation , the cost/value ratio , C/V , has been determined to 0.1 and the prior probability of a query Prq to 10 \u2212 4 .Therefore , \u03b2 = 999.9 .Miss and false alarm probabilities for a given query q are functions of \u03b8 :where :ATWV is the '' actual term-weighted value '' ; it is the detection value attained by the system as a result of the system output and the binary decision output for each putative occurrence .It ranges from \u2212 \u221e to +1 .MTWV is the '' maximum term-weighted value '' over the range of all possible values of 0 .It ranges from 0 to +1 .We have also provided the detection error tradeoff DET curve 19 of miss probability Pmiss vs. false alarm probability PF A .We have used the STDEval tool to extract the relevant results from the manual transcripts and to compute ATWV , MTWV and the DET curve .We have determined empirically the following values for the boosting vector defined in Section 3.4 : Bi = 1i .We use the word error rate WER in order to characterize the accuracy of the transcripts .WER is defined as follows :where N is the total number of words in the corpus , and S , I , and D are the total number of substitution , insertion , and deletion errors , respectively .The substitution error rate SUBR is defined byDeletion error rate DELR and insertion error rate INSR are defined in a similar manner .Table 1 gives the WER and the distribution of the error types over 1-best path transcripts extracted from WCNs .The WER of the 1-best path phonetic transcripts is approximately two times worse than the WER of word transcripts .That is the reason why we have not retrieved from phonetic transcripts on CONFMTG speech data .We have determined empirically a detection threshold 0 per source type and the hard decision of the occurrences having a score less than 0 is set to false ; false occurrences returned by the system are not considered as retrieved and therefore , are not used for computing ATWV , precision and recall .The value of the threshold 0 per source type is reported in Table 2 .It is correlated to the accuracy of the transcripts .Basically , setting a threshold aims to eliminate from the retrieved occurrences , false alarms without adding misses .The higher the WER is , the higher the 0 threshold should be .0.4 0.61 0.91We report in Table 3 the processing resource profile .Concerning the index size , note that our index is compressed using IR index compression techniques .The indexing time includes both audio processing generation of word and phonetic transcripts and building of the searchable indices .We compare our approach WCN phonetic presented in Section 4.1 with another approach 1-best-WCN phonetic .The only difference between these two approaches is that , in 1-best-WCN phonetic , we index only the 1-best path extracted from the WCN instead of indexing all the WCN .WCN phonetic was our primary system for the evaluation and 1-best-WCN phonetic was one of our contrastive systems .Average precision and recall , MTWV and ATWV on the 1100 queries are given in Table 4 .We provide also the DET curve for WCN phonetic approach in Figure 2 .The point that maximizes the TWV , the MTWV , is specified on each curve .Note that retrieval performance has been evaluated separately for each source type since the accuracy of the speech differs per source type as shown in Section 4.2 .As expected , we can see that MTWV and ATWV decrease in higher WER .The retrieval performance is improved whenusing WCNs relatively to 1-best path .It is due to the fact that miss probability is improved by indexing all the hypotheses provided by the WCNs .This observation confirms the results shown by Mamou et al. 17 in the context of spoken document retrieval .The ATWV that we have obtained is close to the MTWV ; we have combined our ranking model with appropriate threshold \u03b8 to eliminate results with lower score .Therefore , the effect of false alarms added by WCNs is reduced .WCN phonetic approach was used in the recent NIST STD evaluation and received the highest overall ranking among eleven participants .For comparison , the system that ranked at the third place , obtained an ATWV of 0.8238 for BNEWS , 0.6652 for CTS and 0.1103 for CONFMTG .We have analysed the retrieval performance according to the average duration of the occurrences in the manual transcripts .The query set was divided into three different quantiles according to the duration ; we have reported in Table 5 ATWV and MTWV according to the duration .We can see that we performed better on longer queries .One of the reasons is the fact that the ASR system is more accurate on long words .Hence , it was justified to boost the score of the results with the exponent \u03b3n , as explained in Section 3.4.3 , according to the length of the query .We have randomly chosen three sets of queries from the query sets provided by NIST : 50 queries containing only IV terms ; 50 queries containing only OOV terms ; and 50 hybrid queries containing both IV and OOV terms .The following experiment has been achieved on the BNEWS collection and IV and OOV terms has been determined according to the vocabulary of BNEWS ASR system .We would like to compare three different approaches of retrieval : using only word index ; using only phonetic index ; combining word and phonetic indices .Table 6 summarizes the retrieval performance according to each approach and to each type of queries .Using a word-based approach for dealing with OOV and hybrid queries affects drastically the performance of the retrieval ; precision and recall are null .Using a phone-based approach for dealing with IV queries affects also the performance of the retrieval relatively to the word-based approach .As expected , the approach combining word and phonetic indices presented in Section 3 leads to the same retrieval performance as the word approach for IV queries and to the same retrieval performance as the phonetic approach for OOV queries .This approach always outperforms the others and it justifies the fact that we need to combine word and phonetic search .", "conclusions": "This work studies how vocabulary independent spoken term detection can be performed efficiently over different data sources .Previously , phonetic-based and word-based approaches have been used for IR on speech data .The former suffers from low accuracy and the latter from limited vocabulary of the recognition system .In this paper , we have presented a vocabulary independent model of indexing and search that combines both the approaches .The system can deal with all kinds of queries although the phrases that need to combine for the retrieval , information extracted from two different indices , a word index and a phonetic index .The scoring of OOV terms is based on the proximity in time between the different phones .The scoring of IV terms is based on information provided by the WCNs .We have shown an improvement in the retrieval performance when using all the WCN and not only the 1-best path and when using phonetic index for search of OOV query terms .This approach always outperforms the other approaches using only word index or phonetic index .As a future work , we will compare our model for OOV search on phonetic transcripts with a retrieval model based on the edit distance .", "related work": "In the past decade , the research efforts on spoken data retrieval have focused on extending classical IR techniques to spoken documents .Some of these works have been done in the context of the TREC Spoken Document Retrieval evaluations and are described by Garofolo et al. 12 .An LVCSR system is used to transcribe the speech into 1-best path word transcripts .The transcripts are indexed as clean text : for each occurrence , its document , its word offset and additional information are stored in the index .A generic IR system over the text is used for word spotting and search as described by Brown et al. 6 and James 14 .This strategy works well for transcripts like broadcast news collections that have a low WER in the range of 15 % -30 % and are redundant by nature the same piece of information is spoken several times in different manners .Moreover , the algorithms have been mostly tested over long queries stated in plain English and retrieval for such queries is more robust against speech recognition errors .An alternative approach consists of using word lattices in order to improve the effectiveness of SDR .Singhal et al. 24 , 25 propose to add some terms to the transcript in order to alleviate the retrieval failures due to ASR errors .From an IR perspective , a classical way to bring new terms is document expansion using a similar corpus .Their approach consists in using word lattices in order to determine which words returned by a document expansion algorithm should be added to the original transcript .The necessity to use a document expansion algorithm was justified by the fact that the word lattices they worked with , lack information about word probabilities .Chelba and Acero in 8 , 9 propose a more compact word lattice , the position specific posterior lattice PSPL .This data structure is similar to WCN and leads to a more compact index .The offset of the terms in the speech documents is also stored in the index .However , the evaluation framework is carried out on lectures that are relatively planned , in contrast to conversational speech .Their ranking model is based on the term confidence level but does not take into consideration the rank of the term among the other hypotheses .Mamou et al. 17 propose a model for spoken document retrieval using WCNs in order to improve the recall and the MAP of the search .However , in the above works , the problem of queries containing OOV terms is not addressed .Popular approaches to deal with OOV queries are based on sub-words transcripts , where the sub-words are typically phones , syllables or word fragments sequences of phones 11 , 20 , 23 .The classical approach consists of using phonetic transcripts .The transcripts are indexed in the same manner as words in using classical text retrieval techniques ; during query processing , the query is represented as a sequence of phones .The retrieval is based on searching the string of phones representing the query in the phonetic transcript .To account for the high recognition error rates , some other systems use richer transcripts like phonetic lattices .They are attractive as they accommodate high error rate conditions as well as allow for OOV queries to be used 15 , 3 , 20 , 23 , 21 , 27 .However , phonetic lattices contain many edges that overlap in time with the same phonetic label , and are difficult to index .Moreover , beside the improvement in the recall of the search , the precision is affected since phonetic lattices are often inaccurate .Consequently , phonetic approaches should be used only for OOV search ; for searching queries containing also IV terms , this technique affects the performance of the retrieval in comparison to the word based approach .Saraclar and Sproat in 22 show improvement in word spotting accuracy for both IV and OOV queries , using phonetic and word lattices , where a confidence measure of a word or a phone can be derived .They propose three different retrieval strategies : search both the word and the phonetic indices and unify the two different sets of results ; search the word index for IV queries , search the phonetic index for OOV queries ; search the word index and if no result is returned , search the phonetic index .However , no strategy is proposed to deal with phrase queries containing both IV and OOV terms .Amir et al. in 5 , 4 propose to merge a word approach with a phonetic approach in the context of video retrieval .However , the phonetic transcript is obtained from a text to phonetic conversion of the 1-best path of the word transcript and is not based on a phonetic decoding of the speech data .An important issue to be considered when looking at the state-of-the-art in retrieval of spoken data , is the lack of a common test set and appropriate query terms .This paper uses such a task and the STD evaluation is a good summary of the performance of different approaches on the same test conditions ."}