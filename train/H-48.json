{"reader_keywords": ["query expansion", "query-document term mismatch", "information retrieval", "information search", "document expansion", "document processing", "relevant document"], "reader_keywords_stem": ["queri expans", "queri-document term mismatch", "inform retriev", "inform search", "document expans", "document process", "relev document"], "introduction": "In our domain , ' and unlike web search , it is very important for attorneys to find all documents e.g. , cases that are relevant to an issue .Missing relevant documents may have non-trivial consequences on the outcome of a court proceeding .Attorneys are especially concerned about missing relevant documents when researching a legal topic that is new to them , as they may not be aware of all language variations in such topics .Therefore , it is important to develop information retrieval systems that are robust with respect to language variations or term mismatch between queries and relevant documents .During our work on developing such systems , we concluded that current evaluation methods are not sufficient for this purpose .Whooping cough , pertussis , heart attack , myocardial infarction , car wash , automobile cleaning , attorney , legal counsel , lawyer are all examples of things that share the same meaning .Often , the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs .This query-document term mismatch arises from two sources : 1 the synonymy found in natural language , both at the term and the phrasal level , and 2 the degree to which the user is an expert at searching and/or has expert knowledge in the domain of the collection being searched .IR evaluations are comparative in nature cf. TREC .Generally , IR evaluations show how System A did in relation to System B on the same test collection based on various precision and recall-based metrics .Similarly , IR systems with QE capabilities are typically evaluated by executing each search twice , once with and once without query expansion , and then comparing the two result sets .While this approach shows which system may have performed better overall with respect to a particular test collection , it does not directly or systematically measure the effectiveness of IR systems in overcoming query-document term mismatch .If the goal of QE is to increase search performance by mitigating the effects of query-document term mismatch , then the degree to which a system does so should be measurable in evaluation .An effective evaluation method should measure the performance of IR systems under varying degrees of query-document term mismatch , not just in terms of overall performance on a collection relative to another system . 'Thomson Corporation builds information based solutions to the professional markets including legal , financial , health care , scientific , and tax and accounting .In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a user 's query , but that do not necessarily contain the query terms themselves , we systematically introduce term mismatch into the test collection by removing query terms from known relevant documents .Because we are purposely inducing term mismatch between the queries and known relevant documents in our test collections , the proposed evaluation framework is able to measure the effectiveness of QE in a way that testing on the whole collection is not .If a QE search method finds a document that is known to be relevant but that is nonetheless missing query terms , it shows that QE technique is indeed robust with respect to query-document term mismatch .", "title": "A New Approach for Evaluating Query Expansion : Query-Document Term Mismatch", "author_keywords_stem": ["information retrieval", "evaluation", "query expansion"], "abstract": "The effectiveness of information retrieval IR systems is influenced by the degree of term overlap between user queries and relevant documents .Query-document term mismatch , whether partial or total , is a fact that must be dealt with by IR systems .Query Expansion QE is one method for dealing with term mismatch .IR systems implementing query expansion are typically evaluated by executing each query twice , with and without query expansion , and then comparing the two result sets .While this measures an overall change in performance , it does not directly measure the effectiveness of IR systems in overcoming the inherent issue of term mismatch between the query and relevant documents , nor does it provide any insight into how such systems would behave in the presence of query-document term mismatch .In this paper , we propose a new approach for evaluating query expansion techniques .The proposed approach is attractive because it provides an estimate of system performance under varying degrees of query-document term mismatch , it makes use of readily available test collections , and it does not require any additional relevance judgments or any form of manual processing .", "id": "H-48", "combined_keywords_stem": ["queri expans", "queri-document term mismatch", "inform retriev", "inform search", "document expans", "document process", "relev document", "evalu"], "evaluation": "We used the proposed evaluation framework to evaluate four IR systems on two test collections .Of the four systems used in the evaluation , two implement query expansion techniques : Okapi with pseudo-feedback for QE , and a proprietary concept search engine we 'll call it TCS , for Thomson Concept Search .TCS is a language modeling based retrieval engine that utilizes a subject-appropriate external corpus i.e. , legal or news as a knowledge source .This external knowledge source is a corpus separate from , but thematically related to , the document collection to be searched .Translation probabilities for QE 2 are calculated from these large external corpora .Okapi without feedback and a language model query likelihood QL model implemented using Indri are included as keyword-only baselines .Okapi without feedback is intended as an analogous baseline for Okapi with feedback , and the QL model is intended as an appropriate baseline for TCS , as they both implement language-modeling based retrieval algorithms .We choose these as baselines because they are dependent only on the words appearing in the queries and have no QE capabilities .As a result , we expect that when query terms are removed from relevant documents , the performance of these systems should degrade more dramatically than their counterparts that implement QE .The Okapi and QL model results were obtained using the Lemur Toolkit .2 Okapi was run with the parameters k1 = 1.2 , b = 0.75 , and k3 = 7 .When run with feedback , the feedback parameters used in Okapi were set at 10 documents and 25 terms .The QL model used Jelinek-Mercer smoothing , with \u03bb = 0.6 .We evaluated the performance of the four IR systems outlined above on two different test collections .The two test collections used were the TREC AP89 collection TIPSTER disk 1 and the FSupp Collection .The FSupp Collection is a proprietary collection of 11,953 case law documents for which we have 44 queries ranging from four to twenty-two words after stop word removal with full relevance judgments .3 The average length of documents in the FSupp Collection is 3444 words .The TREC AP89 test collection contains 84,678 documents , averaging 252 words in length .In our evaluation , we used both the title and the description fields of topics 151200 as queries , so we have two sets of results for the AP89 Collection .After stop word removal , the title queries range from two to eleven words and the description queries range from four to twenty-six terms .In our experiments , we chose to sequentially and additively remove query terms from highest-to-lowest inverse document frequency IDF with respect to the entire document collection .Terms with high IDF values tend to influence document ranking more than those with lower IDF values .Additionally , high IDF terms tend to be domainspecific terms that are less likely to be known to non-expert user , hence we start by removing these first .For the FSupp Collection , queries were evaluated incrementally with one , two , three , five , and seven terms removed from their corresponding relevant documents .The longer description queries from TREC topics 151-200 were likewise evaluated on the AP89 Collection with one , two , three , five , and seven query terms removed from their relevant documents .For the shorter TREC title queries , we removed one , two , three , and five terms from the relevant documents .In this implementation of the evaluation framework , we chose three metrics by which to compare IR system performance : mean average precision MAP , precision at 10 documents P10 , and recall at 1000 documents .Although these are the metrics we chose to demonstrate this framework , any appropriate IR metrics could be used within the framework .Figures 1 , 2 , and 3 show the performance in terms of MAP , P10 and Recall , respectively for the four search engines on the FSupp Collection .As expected , the performance of the keyword-only IR systems , QL and Okapi , drops quickly as query terms are removed from the relevant documents in the collection .The performance of Okapi with feedback Okapi FB is somewhat surprising in that on the original collection i.e. , prior to query term removal , its performance is worse than that of Okapi without feedback on all three measures .TCS outperforms the QL keyword baseline on every measure except for MAP on the original collection i.e. , prior to removing any query terms .Because TCS employs implicit query expansion using an external domain specific knowledge base , it is less sensitive to term removal i.e. , mismatch than the Okapi FB , which relies on terms from the top-ranked documents retrieved by an initial keywordonly search .Because overall search engine performance is frequently measured in terms of MAP , and because other evaluations of QE often only consider performance on the entire collection i.e. , they do not consider term mismatch , the QE implemented in TCS would be considered in another evaluation to hurt performance on the FSupp Collection .However , when we look at the comparison of TCS to QL when query terms are removed from the relevant documents , we can see that the QE in TCS is indeed contributing positively to the search .Figures 4 , 5 , and 6 show the performance of the four IR systems on the AP89 Collection , using the TREC topic descriptions as queries .The most interesting difference between the performance on the FSupp Collection and the AP89 collection is the reversal of Okapi FB and TCS .On FSupp , TCS outperformed the other engines consistently see Figures 1 , 2 , and 3 ; on the AP89 Collection , Okapi FB is clearly the best performer see Figures 4 , 5 , and 6 .This is all the more interesting , based on the fact that QE in Okapi FB takes place after the first search iteration , whichon the AP89 Collection , using TREC description queries , and as a function of the number of query terms removed .we would expect to be handicapped when query terms are removed .Looking at P10 in Figure 5 , we can see that TCS and Okapi FB score similarly on P10 , starting at the point where one query term is removed from relevant documents .At two query terms removed , TCS starts outperforming Okapi FB .If modeling this in terms of expert versus non-expert users , we could conclude that TCS might be a better search engine for non-experts to use on the AP89 Collection , while Okapi FB would be best for an expert searcher .It is interesting to note that on each metric for the AP89 description queries , TCS performs more poorly than all the other systems on the original collection , but quickly surpasses the baseline systems and approaches Okapi FB 's performance as terms are removed .This is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch .Figures 7 , 8 , and 9 show the performance of the four IR systems on the AP89 Collection , using the TREC topic titles as queries .As with the AP89 description queries , Okapi FB is again the best performer of the four systems in the evaluation .As before , the performance of the Okapi and QL systems , the non-QE baseline systems , sharply degrades as query terms are removed .On the shorter queries , TCS seems to have a harder time catching up to the performance of Okapi FB as terms are removed .Perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed consistently and as expected on both collections with respect to query term removal from relevant documents , the performances of the engines implementing QE techniques differed dramatically between collections .", "combined_keywords": ["query expansion", "query-document term mismatch", "information retrieval", "information search", "document expansion", "document processing", "relevant document", "evaluation"], "author_keywords": ["information retrieval", "evaluation", "query expansion"], "method": "In order to accurately measure IR system performance in the presence of query-term mismatch , we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner .Our approach is to introduce querydocument term mismatch into a corpus in a controlled manner and then measure the performance of IR systems as the degree of term mismatch changes .We systematically remove query terms from known relevant documents , creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query .Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term mismatch between relevant documents and queries in a controlled manner .This removal process affects only the relevant documents in the search collection .The queries themselves remain unaltered .Query terms are removed from documents one by one , so the differences in IR system performance can be measured with respect to missing terms .In the most extreme case i.e. , when the length of the query is less than or equalto the number of query terms removed from the relevant documents , there will be no term overlap between a query and its relevant documents .Notice that , for a given query , only relevant documents are modified .Non-relevant documents are left unchanged , even in the case that they contain query terms .Although , on the surface , we are changing the distribution of terms between the relevant and non-relevant documents sets by removing query terms from the relevant documents , doing so does not change the conceptual relevancy of these documents .Systematically removing query terms from known relevant documents introduces a controlled amount of query-document term mismatch by which we can evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents , despite a lack of actual term overlap .Removing a query term from relevant documents simply masks the presence of that query term in those documents .It does not in any way change the conceptual relevancy of the documents .The evaluation framework presented in this paper consists of three elements : a test collection , C ; a strategy for selecting which query terms to remove from the relevant documents in that collection , S ; and a metric by which to compare performance of the IR systems , m .The test collection , C , consists of a document collection , queries , and relevance judgments .The strategy , S , determines the order and manner in which query terms are removed from the relevant documents in C .This evaluation framework is not metric-specific ; any metric MAP , P@10, recall , etc. can be used to measure IR system performance .Although test collections are difficult to come by , it should be noted that this evaluation framework can be used on any available test collection .In fact , using this framework stretches the value of existing test collections in that one collection becomes several when query terms are removed from relevant documents , thereby increasing the amount of information that can be gained from evaluating on a particular collection .In other evaluations of QE effectiveness , the controlled variable is simply whether or not queries have been expanded or not , compared in terms of some metric .In contrast , the controlled variable in this framework is the query term that has been removed from the documents relevant to that query , as determined by the removal strategy , S. Query terms are removed one by one , in a manner and order determined by S , so that collections differ only with respect to the one term that has been removed or masked in the documents relevant to that query .It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch .The choice of a query term removal strategy is relatively flexible ; the only restriction in choosing a strategy S is that query terms must be removed one at a time .Two decisions must be made when choosing a removal strategy S .The first is the order in which S removes terms from the relevant documents .Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection .Based on the purpose of the evaluation and the retrieval algorithm being used , it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection .Once an order for removal has been decided , a manner for term removal/masking must be decided .It must be determined if S will remove the terms individually i.e. , remove just one different term each time or additively i.e. , remove one term first , then that term in addition to another , and so on .The incremental additive removal of query terms from relevant documents allows the evaluation to show the degree to which IR system performance degrades as more and more query terms are missing , thereby increasing the degree of query-document term mismatch .Removing terms individually allows for a clear comparison of the contribution of QE in the absence of each individual query term .", "conclusions": "The proposed evaluation framework allows us to measure the degree to which different IR systems overcome or do n't overcome term mismatch between queries and relevant documents .Evaluations of IR systems employing QE performed only on the entire collection do not take into account that the purpose of QE is to mitigate the effects of term mismatch in retrieval .By systematically removing query terms from relevant documents , we can measure the degree to which QE contributes to a search by showing the difference between the performances of a QE system and its keywordonly baseline when query terms have been removed from known relevant documents .Further , we can model the behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection .The evaluation framework proposed in this paper is attractive for several reasons .Most importantly , it provides a controlled manner in which to measure the performance of QE with respect to query-document term mismatch .In addition , this framework takes advantage and stretches the amount of information we can get from existing test collections .Further , this evaluation framework is not metricspecific : information in terms of any metric MAP , P@10, etc. can be gained from evaluating an IR system this way .It should also be noted that this framework is generalizable to any IR system , in that it evaluates how well IR systems evaluate users ' information needs as represented by their queries .An IR system that is easy to use should be good at retrieving documents that are relevant to users ' information needs , even if the queries provided by the users do not contain the same keywords as the relevant documents .", "related work": "Accounting for term mismatch between the terms in user queries and the documents relevant to users ' information needs has been a fundamental issue in IR research for almost 40 years 38 , 37 , 47 .Query expansion QE is one technique used in IR to improve search performance by increasing the likelihood of term overlap either explicitly or implicitly between queries and documents that are relevant to users ' information needs .Explicit query expansion occurs at run-time , based on the initial search results , as is the case with relevance feedback and pseudo relevance feedback 34 , 37 .Implicit query expansion can be based on statistical properties of the document collection , or it may rely on external knowledge sources such as a thesaurus or an ontology 32 , 17 , 26 , 50 , 51 , 2 .Regardless of method , QE algorithms that are capable of retrieving relevant documents despite partial or total term mismatch between queries and relevant documents should increase the recall of IR systems by retrieving documents that would have previously been missed as well as their precision by retrieving more relevant documents .In practice , QE tends to improve the average overall retrieval performance , doing so by improving performance on some queries while making it worse on others .QE techniques are judged as effective in the case that they help more than they hurt overall on a particular collection 47 , 45 , 41 , 27 .Often , the expansion terms added to a query in the query expansion phase end up hurting the overall retrieval performance because they introduce semantic noise , causing the meaning of the query to drift .As such , much work has been done with respect to different strategies for choosing semantically relevant QE terms to include in order to avoid query drift 34 , 50 , 51 , 18 , 24 , 29 , 30 , 32 , 3 , 4 , 5 .The evaluation of IR systems has received much attention in the research community , both in terms of developing test collections for the evaluation of different systems 11 , 12 , 13 , 43 and in terms of the utility of evaluation metrics such as recall , precision , mean average precision , precision at rank , Bpref , etc. 7 , 8 , 44 , 14 .In addition , there have been comparative evaluations of different QE techniques on various test collections 47 , 45 , 41 .In addition , the IR research community has given attention to differences between the performance of individual queries .Research efforts have been made to predict which queries will be improved by QE and then selectively applying it only to those queries 1 , 5 , 27 , 29 , 15 , 48 , to achieve optimal overall performance .In addition , related work on predicting query difficulty , or which queries are likely to perform poorly , has been done 1 , 4 , 5 , 9 .There is general interest in the research community to improve the robustness of IR systems by improving retrieval performance on difficult queries , as is evidenced by the Robust track in the TREC competitions and new evaluation measures such as GMAP .GMAP geometric mean average precision gives more weight to the lower end of the average precision as opposed to MAP , thereby emphasizing the degree to which difficult or poorly performing queries contribute to the score 33 .However , no attention is given to evaluating the robustness of IR systems implementing QE with respect to querydocument term mismatch in quantifiable terms .By purposely inducing mismatch between the terms in queries and relevant documents , our evaluation framework allows us a controlled manner in which to degrade the quality of the queries with respect to their relevant documents , and then to measure the both the degree of induced difficulty of the query and the degree to which QE improves the retrieval performance of the degraded query .The work most similar to our own in the literature consists of work in which document collections or queries are altered in a systematic way to measure differences query performance .42 introduces into the document collection pseudowords that are ambiguous with respect to word sense , in order to measure the degree to which word sense disambiguation is useful in IR .6 experiments with altering the document collection by adding semantically related expansion terms to documents at indexing time .In cross-language IR , 28 explores different query expansion techniques while purposely degrading their translation resources , in what amounts to expanding a query with only a controlled percentage of its translation terms .Although similar in introducing a controlled amount of variance into their test collections , these works differ from the work being presented in this paper in that the work being presented here explicitly and systematically measures query effectiveness in the presence of query-document term mismatch ."}