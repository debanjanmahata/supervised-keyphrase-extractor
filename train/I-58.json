{"reader_keywords": ["adversarial multiagent domain", "agent security via approximate policy", "security of agent system", "agent system security", "bayesian game", "bayes-nash equilibrium", "bayesian and stackelberg game", "np-hard", "patrolling domain", "heuristic approach", "decomposition for multiple adversary", "mixed-integer linear program"], "reader_keywords_stem": ["adversari multiag domain", "agent secur via approxim polici", "secur of agent system", "agent system secur", "bayesian game", "bay-nash equilibrium", "bayesian and stackelberg game", "np-hard", "patrol domain", "heurist approach", "decomposit for multipl adversari", "mix-integ linear program"], "introduction": "In many multiagent domains , agents must act in order to provide security against attacks by adversaries .A common issue that agents face in such security domains is uncertainty about the adversaries they may be facing .For example , a security robot may need to make a choice about which areas to patrol , and how often 16 .However , it will not know in advance exactly where a robber will choose to strike .A team of unmanned aerial vehicles UAVs 1 monitoring a region undergoing a humanitarian crisis may also need to choose a patrolling policy .They must make this decision without knowing in advance whether terrorists or other adversaries may be waiting to disrupt the mission at a given location .It may indeed be possible to model the motivations of types of adversaries the agent or agent team is likely to face in order to target these adversaries more closely .However , in both cases , the security robot or UAV team will not know exactly which kinds of adversaries may be active on any given day .A common approach for choosing a policy for agents in such scenarios is to model the scenarios as Bayesian games .A Bayesian game is a game in which agents may belong to one or more types ; the type of an agent determines its possible actions and payoffs .The distribution of adversary types that an agent will face may be known or inferred from historical data .Usually , these games are analyzed according to the solution concept of a Bayes-Nash equilibrium , an extension of the Nash equilibrium for Bayesian games .However , in many settings , a Nash or Bayes-Nash equilibrium is not an appropriate solution concept , since it assumes that the agents ' strategies are chosen simultaneously 5 .In some settings , one player can or must commit to a strategy before the other players choose their strategies .These scenarios are known as Stackelberg games 6 .In a Stackelberg game , a leader commits to a strategy first , and then a follower or group of followers selfishly optimize their own rewards , considering the action chosen by the leader .For example , the security agent leader must first commit to a strategy for patrolling various areas .This strategy could be a mixed strategy in order to be unpredictable to the robbers followers .The robbers , after observing the pattern of patrols over time , can then choose their strategy which location to rob .Often , the leader in a Stackelberg game can attain a higher reward than if the strategies were chosen simultaneously .To see the advantage of being the leader in a Stackelberg game , consider a simple game with the payoff table as shown in Table 1 .The leader is the row player and the follower is the column player .Here , the leader 's payoff is listed first .The only Nash equilibrium for this game is when the leader plays 2 and the follower plays 2 which gives the leader a payoff of 2 .However , if the leader commits to a uniform mixed strategy of playing 1 and 2 with equal 0.5 probability , the follower 's best response is to play 3 to get an expected payoff of 5 10 and 0 with equal probability .The leader 's payoff would then be 4 3 and 5 with equal probability .In this case , the leader now has an incentive to deviate and choose a pure strategy of 2 to get a payoff of 5 .However , this would cause the follower to deviate to strategy 2 as well , resulting in the Nash equilibrium .Thus , by committing to a strategy that is observed by the follower , and by avoiding the temptation to deviate , the leader manages to obtain a reward higher than that of the best Nash equilibrium .The problem of choosing an optimal strategy for the leader to commit to in a Stackelberg game is analyzed in 5 and found to be NP-hard in the case of a Bayesian game with multiple types of followers .Thus , efficient heuristic techniques for choosing highreward strategies in these games is an important open issue .Methods for finding optimal leader strategies for non-Bayesian games 5 can be applied to this problem by converting the Bayesian game into a normal-form game by the Harsanyi transformation 8 .If , on the other hand , we wish to compute the highest-reward Nash equilibrium , new methods using mixed-integer linear programs MILPs 17 may be used , since the highest-reward Bayes-Nash equilibrium is equivalent to the corresponding Nash equilibrium in the transformed game .However , by transforming the game , the compact structure of the Bayesian game is lost .In addition , since the Nash equilibrium assumes a simultaneous choice of strategies , the advantages of being the leader are not considered .This paper introduces an efficient heuristic method for approximating the optimal leader strategy for security domains , known as ASAP Agent Security via Approximate Policies .This method has three key advantages .First , it directly searches for an optimal strategy , rather than a Nash or Bayes-Nash equilibrium , thus allowing it to find high-reward non-equilibrium strategies like the one in the above example .Second , it generates policies with a support which can be expressed as a uniform distribution over a multiset of fixed size as proposed in 12 .This allows for policies that are simple to understand and represent 12 , as well as a tunable parameter the size of the multiset that controls the simplicity of the policy .Third , the method allows for a Bayes-Nash game to be expressed compactly without conversion to a normal-form game , allowing for large speedups over existing Nash methods such as 17 and 11 .The rest of the paper is organized as follows .In Section 2 we fully describe the patrolling domain and its properties .Section 3 introduces the Bayesian game , the Harsanyi transformation , and existing methods for finding an optimal leader 's strategy in a Stackelberg game .Then , in Section 4 the ASAP algorithm is presented for normal-form games , and in Section 5 we show how it can be adapted to the structure of Bayesian games with uncertain adversaries .Experimental results showing higher reward and faster policy computation over existing Nash methods are shown in Section 6 , and we conclude with a discussion of related work in Section 7 .", "title": "An Efficient Heuristic Approach for Security Against Multiple Adversaries", "author_keywords_stem": ["security of agent system", "agent system security", "game theory", "bayesian and stackelberg game"], "abstract": "In adversarial multiagent domains , security , commonly defined as the ability to deal with intentional threats from other agents , is a critical issue .This paper focuses on domains where these threats come from unknown adversaries .These domains can be modeled as Bayesian games ; much work has been done on finding equilibria for such games .However , it is often the case in multiagent security domains that one agent can commit to a mixed strategy which its adversaries observe before choosing their own strategies .In this case , the agent can maximize reward by finding an optimal strategy , without requiring equilibrium .Previous work has shown this problem of optimal strategy selection to be NP-hard .Therefore , we present a heuristic called ASAP , with three key advantages to address the problem .First , ASAP searches for the highest-reward strategy , rather than a Bayes-Nash equilibrium , allowing it to find feasible strategies that exploit the natural first-mover advantage of the game .Second , it provides strategies which are simple to understand , represent , and implement .Third , it operates directly on the compact , Bayesian game representation , without requiring conversion to normal form .We provide an efficient Mixed Integer Linear Program MILP implementation for ASAP , along with experimental results illustrating significant speedups and higher rewards over other approaches .", "id": "I-58", "combined_keywords_stem": ["adversari multiag domain", "agent secur via approxim polici", "secur of agent system", "agent system secur", "bayesian game", "bay-nash equilibrium", "bayesian and stackelberg game", "np-hard", "patrol domain", "heurist approach", "decomposit for multipl adversari", "mix-integ linear program", "game theori"], "evaluation": "The patrolling domain and the payoffs for the associated game are detailed in Sections 2 and 3 .We performed experiments for this game in worlds of three and four houses with patrols consisting of two houses .The description given in Section 2 is used to generate a base case for both the security agent and robber payoff functions .The payoff tables for additional robber types are constructed and added to the game by adding a random distribution of varying size to the payoffs in the base case .All games are normalized so that , for each robber type , the minimum and maximum payoffs to the security agent and robber are 0 and 1 , respectively .Using the data generated , we performed the experiments using four methods for generating the security agent 's strategy :The last three methods were applied using CPLEX 8.1 .Because the last two methods are designed for normal-form games rather than Bayesian games , the games were first converted using the Harsanyi transformation 8 .The uniform randomization method is simply choosing a uniform random policy over all possible patrol routes .We use this method as a simple baseline to measure the performance of our heuristics .We anticipated that the uniform policy would perform reasonably well since maximum-entropy policies have been shown to be effective in multiagent security domains 14 .The highest-reward Bayes-Nash equilibria were used in order to demonstrate the higher reward gained by looking for an optimal policy rather than an equilibria in Stackelberg games such as our security domain .Based on our experiments we present three sets of graphs to demonstrate 1 the runtime of ASAP compared to other common methods for finding a strategy , 2 the reward guaranteed by ASAP compared to other methods , and 3 the effect of varying the parameter k , the size of the multiset , on the performance of ASAP .In the first two sets of graphs , ASAP is run using a multiset of 80 elements ; in the third set this number is varied .The first set of graphs , shown in Figure 1 shows the runtime graphs for three-house left column and four-house right column domains .Each of the three rows of graphs corresponds to a different randomly-generated scenario .The x-axis shows the number of robber types the security agent faces and the y-axis of the graph shows the runtime in seconds .All experiments that were not concluded in 30 minutes 1800 seconds were cut off .The runtime for the uniform policy is always negligible irrespective of the number of adversaries and hence is not shown .The ASAP algorithm clearly outperforms the optimal , multipleLP method as well as the MIP-Nash algorithm for finding the highestreward Bayes-Nash equilibrium with respect to runtime .For adomain of three houses , the optimal method can not reach a solution for more than seven robber types , and for four houses it can not solve for more than six types within the cutoff time in any of the three scenarios .MIP-Nash solves for even fewer robber types within the cutoff time .On the other hand , ASAP runs much faster , and is able to solve for at least 20 adversaries for the three-house scenarios and for at least 12 adversaries in the four-house scenarios within the cutoff time .The runtime of ASAP does not increase strictly with the number of robber types for each scenario , but in general , the addition of more types increases the runtime required .The second set of graphs , Figure 2 , shows the reward to the patrol agent given by each method for three scenarios in the three-house left column and four-house right column domains .This reward is the utility received by the security agent in the patrolling game , and not as a percentage of the optimal reward , since it was not possible to obtain the optimal reward as the number of robber types increased .The uniform policy consistently provides the lowest reward in both domains ; while the optimal method of course produces the optimal reward .The ASAP method remains consistently close to the optimal , even as the number of robber types increases .The highest-reward Bayes-Nash equilibria , provided by the MIPNash method , produced rewards higher than the uniform method , but lower than ASAP .This difference clearly illustrates the gains in the patrolling domain from committing to a strategy as the leader in a Stackelberg game , rather than playing a standard Bayes-Nash strategy .The third set of graphs , shown in Figure 3 shows the effect of the multiset size on runtime in seconds left column and reward right column , again expressed as the reward received by the security agent in the patrolling game , and not a percentage of the optimal", "combined_keywords": ["adversarial multiagent domain", "agent security via approximate policy", "security of agent system", "agent system security", "bayesian game", "bayes-nash equilibrium", "bayesian and stackelberg game", "np-hard", "patrolling domain", "heuristic approach", "decomposition for multiple adversary", "mixed-integer linear program", "game theory"], "author_keywords": ["security of agent system", "agent system security", "game theory", "bayesian and stackelberg game"], "method": "In most security patrolling domains , the security agents like UAVs 1 or security robots 16 can not feasibly patrol all areas all the time .Instead , they must choose a policy by which they patrol various routes at different times , taking into account factors such as the likelihood of crime in different areas , possible targets for crime , and the security agents ' own resources number of security agents , amount of available time , fuel , etc. .It is usually beneficial for this policy to be nondeterministic so that robbers can not safely rob certain locations , knowing that they will be safe from the security agents 14 .To demonstrate the utility of our algorithm , we use a simplified version of such a domain , expressed as a game .The most basic version of our game consists of two players : the security agent the leader and the robber the follower in a world consisting of m houses , 1 ... m .The security agent 's set of pure strategies consists of possible routes of d houses to patrol in an order .The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may patrol , but the robber will know the mixed strategy the security agent has chosen .For example , the robber can observe over time how often the security agent patrols each area .With this knowledge , the robber must choose a single house to rob .We assume that the robber generally takes a long time to rob a house .If the house chosen by the robber is not on the security agent 's route , then the robber successfully robs it .Otherwise , if it is on the security agent 's route , then the earlier the house is on the route , the easier it is for the security agent to catch the robber before he finishes robbing it .We model the payoffs for this game with the following variables :The security agent 's set of possible pure strategies patrol routes is denoted by X and includes all d-tuples i = < w1 , w2 , ... , wd > with w1 ... wd = 1 ... m where no two elements are equal the agent is not allowed to return to the same house .The robber 's set of possible pure strategies houses to rob is denoted by Q and includes all integers j = 1 ... m .The payoffs security agent , robber for pure strategies i , j are :With this structure it is possible to model many different types of robbers who have differing motivations ; for example , one robber may have a lower cost of getting caught than another , or may value the goods in the various houses differently .If the distribution of different robber types is known or inferred from historical data , then the game can be modeled as a Bayesian game 6 .A Bayesian game contains a set of N agents , and each agent n must be one of a given set of types 0n .For our patrolling domain , we have two agents , the security agent and the robber .01 is the set of security agent types and 02 is the set of robber types .Since there is only one type of security agent , 01 contains only one element .During the game , the robber knows its type but the security agent does not know the robber 's type .For each agent the security agent or the robber n , there is a set of strategies \u03c3n and a utility function un : 01 \u00d7 02 \u00d7 \u03c31 \u00d7 \u03c32 \u2192 R .A Bayesian game can be transformed into a normal-form game using the Harsanyi transformation 8 .Once this is done , new , linear-program LP based methods for finding high-reward strategies for normal-form games 5 can be used to find a strategy in the transformed game ; this strategy can then be used for the Bayesian game .While methods exist for finding Bayes-Nash equilibria directly , without the Harsanyi transformation 10 , they find only asingle equilibrium in the general case , which may not be of high reward .Recent work 17 has led to efficient mixed-integer linear program techniques to find the best Nash equilibrium for a given agent .However , these techniques do require a normal-form game , and so to compare the policies given by ASAP against the optimal policy , as well as against the highest-reward Nash equilibrium , we must apply these techniques to the Harsanyi-transformed matrix .The next two subsections elaborate on how this is done .The first step in solving Bayesian games is to apply the Harsanyi transformation 8 that converts the Bayesian game into a normal form game .Given that the Harsanyi transformation is a standard concept in game theory , we explain it briefly through a simple example in our patrolling domain without introducing the mathematical formulations .Let us assume there are two robber types a and b in the Bayesian game .Robber a will be active with probability \u03b1 , and robber b will be active with probability 1 \u03b1 .The rules described in Section 2 allow us to construct simple payoff tables .Assume that there are two houses in the world 1 and 2 and hence there are two patrol routes pure strategies for the agent : 11,21 and 12,11 .The robber can rob either house 1 or house 2 and hence he has two strategies denoted as 1l , 2l for robber type l .Since there are two types assumed denoted as a and b , we construct two payoff tables shown in Table 2 corresponding to the security agent playing a separate game with each of the two robber types with probabilities \u03b1 and 1 \u03b1 .First , consider robber type a. Borrowing the notation from the domain section , we assign the following values to the variables : v1 , x = v1 , q = 3/4 , v2 , x = v2 , q = 1/4 , cx = 1/2 , cq = 1 , p1 = 1 , p2 = 1/2 .Using these values we construct a base payoff table as the payoff for the game against robber type a. For example , if the security agent chooses route 11,21 when robber a is active , and robber a chooses house 1 , the robber receives a reward of -1 for being caught and the agent receives a reward of 0.5 for catching the robber .The payoffs for the game against robber type b are constructed using different values .Using the Harsanyi technique involves introducing a chance node , that determines the robber 's type , thus transforming the security agent 's incomplete information regarding the robber into imperfect information 3 .The Bayesian equilibrium of the game is then precisely the Nash equilibrium of the imperfect information game .The transformed , normal-form game is shown in Table 3 .In the transformed game , the security agent is the column player , and the set of all robber types together is the row player .Suppose that robber type a robs house 1 and robber type b robs house 2 , while the security agent chooses patrol 11,21 .Then , the security agent and the robber receive an expected payoff corresponding to their payoffs from the agent encountering robber a at house 1 with probability \u03b1 and robber b at house 2 with probability 1 \u03b1 .Although a Nash equilibrium is the standard solution concept for games in which agents choose strategies simultaneously , in our security domain , the security agent the leader can gain an advantage by committing to a mixed strategy in advance .Since the followers the robbers will know the leader 's strategy , the optimal response for the followers will be a pure strategy .Given the common assumption , taken in 5 , in the case where followers are indifferent , they will choose the strategy that benefits the leader , there must exist a guaranteed optimal strategy for the leader 5 .From the Bayesian game in Table 2 , we constructed the Harsanyi transformed bimatrix in Table 3 .The strategies for each player security agent or robber in the transformed game correspond to all combinations of possible strategies taken by each of that player 's types .Therefore , we denote X = \u03c3\u03b811 = \u03c31 and Q = \u03c3\u03b82 2 as the index sets of the security agent and robbers ' pure strategies respectively , with R and C as the corresponding payoff matrices .Rij is the reward of the security agent and Cij is the reward of the robbers when the security agent takes pure strategy i and the robbers take pure strategy j .A mixed strategy for the security agent is a probability distribution over its set of pure strategies and will be represented by a vector x = px1 , px2 , ... , pxIXI , where pxi > 0 and E pxi = 1 .Here , pxi is the probability that the security agent will choose its ith pure strategy .The optimal mixed strategy for the security agent can be found in time polynomial in the number of rows in the normal form game using the following linear program formulation from 5 .For every possible pure strategy j by the follower the set of all robber types ,Then , for all feasible follower strategies j , choose the one that maximizes EiEX pxiRij , the reward for the security agent leader .The pxi variables give the optimal strategy for the security agent .Note that while this method is polynomial in the number of rows in the transformed , normal-form game , the number of rows increases exponentially with the number of robber types .Using this method for a Bayesian game thus requires running 1\u03c3211\u03b821 separate linear programs .This is no surprise , since finding the leader 's optimal strategy in a Bayesian Stackelberg game is NP-hard 5 .Given that finding the optimal strategy for the leader is NP-hard , we provide a heuristic approach .In this heuristic we limit the possible mixed strategies of the leader to select actions with probabilities that are integer multiples of 1/k for a predetermined integer k. Previous work 14 has shown that strategies with high entropy are beneficial for security applications when opponents ' utilities are completely unknown .In our domain , if utilities are not considered , this method will result in uniform-distribution strategies .One advantage of such strategies is that they are compact to represent as fractions and simple to understand ; therefore they can be efficiently implemented by real organizations .We aim to maintain the advantage provided by simple strategies for our security application problem , incorporating the effect of the robbers ' rewards on the security agent 's rewards .Thus , the ASAP heuristic will produce strategies which are k-uniform .A mixed strategy is denoted k-uniform if it is a uniform distribution on a multiset S of pure strategies with IS = k .A multiset is a set whose elements may be repeated multiple times ; thus , for example , the mixed strategy corresponding to the multiset 11 , 1 , 21 would take strategy 1with probability 2/3 and strategy 2 with probability 1/3 .ASAP allows the size of the multiset to be chosen in order to balance the complexity of the strategy reached with the goal that the identified strategy will yield a high reward .Another advantage of the ASAP heuristic is that it operates directly on the compact Bayesian representation , without requiring the Harsanyi transformation .This is because the different follower robber types are independent of each other .Hence , evaluating the leader strategy against a Harsanyi-transformed game matrix is equivalent to evaluating against each of the game matrices for the individual follower types .This independence property is exploited in ASAP to yield a decomposition scheme .Note that the LP method introduced by 5 to compute optimal Stackelberg policies is unlikely to be decomposable into a small number of games as it was shown to be NP-hard for Bayes-Nash problems .Finally , note that ASAP requires the solution of only one optimization problem , rather than solving a series of problems as in the LP method of 5 .For a single follower type , the algorithm works the following way .Given a particular k , for each possible mixed strategy x for the leader that corresponds to a multiset of size k , evaluate the leader 's payoff from x when the follower plays a reward-maximizing pure strategy .We then take the mixed strategy with the highest payoff .We need only to consider the reward-maximizing pure strategies of the followers robbers , since for a given fixed strategy x of the security agent , each robber type faces a problem with fixed linear rewards .If a mixed strategy is optimal for the robber , then so are all the pure strategies in the support of that mixed strategy .Note also that because we limit the leader 's strategies to take on discrete values , the assumption from Section 3.2 that the followers will break ties in the leader 's favor is not significant , since ties will be unlikely to arise .This is because , in domains where rewards are drawn from any random distribution , the probability of a follower having more than one pure optimal response to a given leader strategy approaches zero , and the leader will have only a finite number of possible mixed strategies .Our approach to characterize the optimal strategy for the security agent makes use of properties of linear programming .We briefly outline these results here for completeness , for detailed discussion and proofs see one of many references on the topic , such as 2 .Every linear programming problem , such as : max cT x I Ax = b , x > 0 , has an associated dual linear program , in this case : min bT y I AT y > c .These primal/dual pairs of problems satisfy weak duality : For any x and y primal and dual feasible solutions respectively , cT x < bT y. Thus a pair of feasible solutions is optimal if cT x = bT y , and the problems are said to satisfy strong duality .In fact if a linear program is feasible and has a bounded optimal solution , then the dual is also feasible and there is a pair x ' , y ' that satisfies cT x ' = bT y ' .These optimal solutions are characterized with the following optimality conditions as defined in 2 :Note that this last condition implies thatwhich proves optimality for primal dual feasible solutions x and y .In the following subsections , we first define the problem in its most intuititive form as a mixed-integer quadratic program MIQP , and then show how this problem can be converted into a mixedinteger linear program MILP .We begin with the case of a single type of follower .Let the leader be the row player and the follower the column player .We denote by x the vector of strategies of the leader and q the vector of strategies of the follower .We also denote X and Q the index sets of the leader and follower 's pure strategies , respectively .The payoff matrices R and C correspond to : Rij is the reward of the leader and Cij is the reward of the follower when the leader takes pure strategy i and the follower takes pure strategy j. Let k be the size of the multiset .We first fix the policy of the leader to some k-uniform policy x .The value xi is the number of times pure strategy i is used in the k-uniform policy , which is selected with probability xi/k .We formulate the optimization problem the follower solves to find its optimal response to x as the following linear program :The objective function maximizes the follower 's expected reward given x , while the constraints make feasible any mixed strategy q for the follower .The dual to this linear programming problem is the following :From strong duality and complementary slackness we obtain that the follower 's maximum reward value , a , is the value of every pure strategy with qj > 0 , that is in the support of the optimal mixed strategy .Therefore each of these pure strategies is optimal .Optimal solutions to the follower 's problem are characterized by linear programming optimality conditions : primal feasibility constraints in 2 , dual feasibility constraints in 3 , and complementary slacknessThese conditions must be included in the problem solved by the leader in order to consider only best responses by the follower to the k-uniform policy x.The leader seeks the k-uniform solution x that maximizes its own payoff , given that the follower uses an optimal response q x .Therefore the leader solves the following integer problem :Problem 4 maximizes the leader 's reward with the follower 's best response qj for fixed leader 's policy x and hence denoted q x j by selecting a uniform policy from a multiset of constant size k .We complete this problem by including the characterization of q x through linear programming optimality conditions .To simplify writing the complementary slackness conditions , we will constrain q x to be only optimal pure strategies by just considering integer solutions of q x .The leader 's problem becomes :Here , the constant M is some large number .The first and fourth constraints enforce a k-uniform policy for the leader , and the second and fifth constraints enforce a feasible pure strategy for the follower .The third constraint enforces dual feasibility of the follower 's problem leftmost inequality and the complementary slackness constraint for an optimal pure strategy q for the follower rightmost inequality .In fact , since only one pure strategy can be selected by the follower , say qh = 1 , this last constraint enforces thatpure strategies which have qj = 0 .We conclude this subsection noting that Problem 5 is an integer program with a non-convex quadratic objective in general , as the matrix R need not be positive-semi-definite .Efficient solution methods for non-linear , non-convex integer problems remains a challenging research question .In the next section we show a reformulation of this problem as a linear integer programming problem , for which a number of efficient commercial solvers exist .We can linearize the quadratic program of Problem 5 through the change of variables zij = xiqj , obtaining the following problemProof : Consider x , q a feasible solution of 5 .We will show that q , zij = xiqj is a feasible solution of 6 of same objective function value .The equivalence of the objective functions , and constraints 4 , 6 and 7 of 6 are satisfied by construction .The fact that EjEQ zij = xi as EjEQ qj = 1 explains constraints 1 , 2 , and 5 of 6 .Constraint 3 of 6 is satisfied because E iEX zij = kqj .Let us now consider q , z feasible for 6 .We will show that q and jEQ zij are feasible for 5 with the same objective value .In fact all constraints of 5 are readily satisfied by construction .To constraint in 6 implies that E see that the objectives match , notice that if qh = 1 then the thirdThis last equality is because both are 0 when j = ~ h .This shows that the transformation preserves the objective function value , completing the proof .Given this transformation to a mixed-integer linear program MILP , we now show how we can apply our decomposition technique on the MILP to obtain significant speedups for Bayesian games with multiple follower types .The MILP developed in the previous section handles only one follower .Since our security scenario contains multiple follower robber types , we change the response function for the follower from a pure strategy into a weighted combination over various pure follower strategies where the weights are probabilities of occurrence of each of the follower types .To admit multiple adversaries in our framework , we modify the notation defined in the previous section to reason about multiple follower types .We denote by x the vector of strategies of the leader and ql the vector of strategies of follower l , with L denoting the index set of follower types .We also denote by X and Q the index sets of leader and follower l 's pure strategies , respectively .We also index the payoff matrices on each follower l , considering the matrices Rl and Cl .Using this modified notation , we characterize the optimal solution of follower l 's problem given the leaders k-uniform policy x , with the following optimality conditions :Again , considering only optimal pure strategies for follower l 's problem we can linearize the complementarity constraint above .We incorporate these constraints on the leader 's problem that selects the optimal k-uniform policy .Therefore , given a priori probabilities pl , with l \u2208 L of facing each follower , the leader solves the following problem :Problem 7 for a Bayesian game with multiple follower types is indeed equivalent to Problem 5 on the payoff matrix obtained from the Harsanyi transformation of the game .In fact , every pure strategy j in Problem 5 corresponds to a sequence of pure strategies jl , one for each follower l E L .This means that qj = 1 if and only if qlj , = 1 for all l E L .In addition , given the a priori probabilities pl of facing player l , the reward in the Harsanyi transformation payoff table is Rij = E lEL plRlij , .The same relation holds between C and Cl .These relations between a pure strategy in the equivalent normal form game and pure strategies in the individual games with each followers are key in showing these problems are equivalent .We can linearize the quadratic programming problem 7 through the change of variables zlij = xiqlj , obtaining the following probProof : Consider x , ql , al with l E L a feasible solution of 7 .We will show that ql , al , zlij = xiqlj is a feasible solution of 8 of same objective function value .The equivalence of the objective functions , and constraints 4 , 7 and 8 of 8 are satisfied by construction .The fact that EjEQ zlij = xi as EjEQ qlj = 1 explains constraints 1 , 2 , 5 and 6 of 8 .Constraint 3 of 8 is satisfied because E iEX zlij = kqlj .ql , al and xi = E Lets now consider ql , zl , al feasible for 8 .We will show that jEQ z1 ij are feasible for 7 with the same objective value .In fact all constraints of 7 are readily satisfied by construction .To see that the objectives match , notice for each l one qlj must equal 1 and the rest equal 0 .Let us say that qlj , = 1 , then the third constraint in 8 implies that E iEX zlij , = k , which means that zlij = 0 for all i E X and all j = ~ jl .In particular this implies thatthe last equality from constraint 6 of 8 .Therefore xiqlj = zlij , qlj = zlij .This last equality is because both are 0 when j = ~ jl .Effectively , constraint 6 ensures that all the adversaries are calculating their best responses against a particular fixed policy of the agent .This shows that the transformation preserves the objective function value , completing the proof .We can therefore solve this equivalent linear integer program with efficient integer programming packages which can handle problems with thousands of integer variables .We implemented the decomposed MILP and the results are shown in the following section .", "conclusions": "reward .Results here are for the three-house domain .The trend is that as as the multiset size is increased , the runtime and reward level both increase .Not surprisingly , the reward increases monotonically as the multiset size increases , but what is interesting is that there is relatively little benefit to using a large multiset in this domain .In all cases , the reward given by a multiset of 10 elements was within at least 96 % of the reward given by an 80-element multiset .The runtime does not always increase strictly with the multiset size ; indeed in one example scenario 2 with 20 robber types , using a multiset of 10 elements took 1228 seconds , while using 80 elements only took 617 seconds .In general , runtime should increase since a larger multiset means a larger domain for the variables in the MILP , and thus a larger search space .However , an increase in the number of variables can sometimes allow for a policy to be constructed more quickly due to more flexibility in the problem ."}