{"reader_keywords": ["time machine", "text search", "inverted file index", "temporal search", "approximate temporal coalescing", "web archive", "versioned document collection", "collaborative authoring environment", "timestamped information feed", "document-content overlap", "indexing range-based value", "open source search-engine nutch", "static indexpruning technique", "validity time-interval", "sublist materialization"], "reader_keywords_stem": ["time machin", "text search", "invert file index", "tempor search", "approxim tempor coalesc", "web archiv", "version document collect", "collabor author environ", "timestamp inform feed", "document-content overlap", "index rang-base valu", "open sourc search-engin nutch", "static indexprun techniqu", "valid time-interv", "sublist materi"], "introduction": "In this work we address time-travel text search over temporally versioned document collections .Given a keyword query q and a time t our goal is to identify and rank relevant documents as if the collection was in its state as of time t .An increasing number of such versioned document collections is available today including web archives , collaborative authoring environments like Wikis , or timestamped information feeds .Text search on these collections , however , is mostly time-ignorant : while the searched collection changes over time , often only the most recent version of a documents is indexed , or , versions are indexed independently and treated as separate documents .Even worse , for some collections , in particular web archives like the Internet Archive 18 , a comprehensive text-search functionality is often completely missing .Time-travel text search , as we develop it in this paper , is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates .For a documentary about a past political scandal , a journalist needs to research early opinions and statements made by the involved politicians .Sending an appropriate query to a major web search-engine , the majority of returned results contains only recent coverage , since many of the early web pages have disappeared and are only preserved in web archives .If the query could be enriched with a time point , say August 20th 2003 as the day after the scandal got revealed , and be issued against a web archive , only pages that existed specifically at that time could be retrieved thus better satisfying the journalist 's information need .Document collections like the Web or Wikipedia 32 , as we target them here , are already large if only a single snapshot is considered .Looking at their evolutionary history , we are faced with even larger data volumes .As a consequence , na \u00a8 \u0131ve approaches to time-travel text search fail , and viable approaches must scale-up well to such large data volumes .This paper presents an efficient solution to time-travel text search by making the following key contributions :The remainder of this paper is organized as follows .The presented work is put in context with related work in Section 2 .We delineate our model of a temporally versioned document collection in Section 3 .We present our time-travel inverted index in Section 4 .Building on it , temporal coalescing is described in Section 5 .In Section 6 we describe principled techniques to improve index performance , before presenting the results of our experimental evaluation in Section 7 .", "title": "A Time Machine for Text Search", "author_keywords_stem": ["temporal text index", "time-travel text search", "web archive"], "abstract": "Text search over temporally versioned document collections such as web archives has received little attention as a research problem .As a consequence , there is no scalable and principled solution to search such a collection as of a specified time t .In this work , we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for temporal search .We introduce approximate temporal coalescing as a tunable method to reduce the index size without significantly affecting the quality of results .In order to further improve the performance of time-travel queries , we introduce two principled techniques to trade off index size for its performance .These techniques can be formulated as optimization problems that can be solved to near-optimality .Finally , our approach is evaluated in a comprehensive series of experiments on two large-scale real-world datasets .Results unequivocally show that our methods make it possible to build an efficient `` time machine '' scalable to large versioned text collections .", "id": "H-44", "combined_keywords_stem": ["time machin", "text search", "invert file index", "tempor search", "approxim tempor coalesc", "web archiv", "version document collect", "collabor author environ", "timestamp inform feed", "document-content overlap", "index rang-base valu", "open sourc search-engin nutch", "static indexprun techniqu", "valid time-interv", "sublist materi", "tempor text index", "time-travel text search"], "evaluation": "We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper .The techniques described in this paper were implemented in a prototype system using Java JDK 1.5 .All experiments described below were run on a single SUN V40z machine having four AMD Opteron CPUs , 16GB RAM , a large network-attached RAID-5 disk array , and running Microsoft Windows Server 2003 .All data and indexes are kept in an Oracle 10g database that runs on the same machine .For our experiments we used two different datasets .The English Wikipedia revision history referred to as WIKI in the remainder is available for free download as a single XML file .This large dataset , totaling 0.7 TBytes , contains the full editing history of the English Wikipedia from January 2001 to December 2005 the time of our download .We indexed all encyclopedia articles excluding versions that were marked as the result of a minor edit e.g. , the correction of spelling errors etc. .This yielded a total of 892,255 documents with 13,976,915 versions having a mean \u00b5 of 15.67 versions per document at standard deviation \u03c3 of 59.18 .We built a time-travel query workload using the query log temporarily made available recently by AOL Research as follows we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia article for e.g. , `` french revolution '' , `` hurricane season 2005 '' , `` da vinci code '' etc. .The thus extracted queries contained a total of 422 distinct terms .For each extracted query , we randomly picked a time point for each month covered by the dataset .This resulted in a total of 18 , 000 = 300 x 60 time-travel queries .The second dataset used in our experiments was based on a subset of the European Archive 13 , containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data .We filtered out documents not belonging to MIME-types text/plain and text/html , to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper .This included a total of 502,617 documents with 8,687,108 versions \u00b5 = 17.28 and \u03c3 = 13.79 .We built a corresponding query workload as mentioned before , this time choosing keyword queries that led to a site in the .gov.uk domain e.g. , `` minimum wage '' , `` inheritance tax '' , `` citizenship ceremony dates '' etc. , and randomly sampling a time point for every month within the two year period spanned by the dataset .Thus , we obtained a total of 7,200 = 300 x 24 time-travel queries for the UKGOV dataset .In total 522 terms appear in the extracted queries .The collection statistics i.e. , N and avdl and term statistics i.e. , DF were computed at monthly granularity for both datasets .Our first set of experiments is aimed at evaluating the approximate temporal coalescing technique , described in Section 5 , in terms of index-size reduction and its effect on the result quality .For both the WIKI and UKGOV datasets , we compare temporally coalesced indexes for different values of the error threshold e computed using Algorithm 1 with the non-coalesced index as a baseline .Table 1 summarizes the index sizes measured as the total number of postings .As these results demonstrate , approximate temporal coalescing is highly effective in reducing index size .Even a small threshold value , e.g. e = 0.01 , has a considerable effect by reducing the index size almost by an order of magnitude .Note that on the UKGOV dataset , even accurate coalescing e = 0 manages to reduce the index size to less than 38 % of the original size .Index size continues to reduce on both datasets , as we increase the value of E .How does the reduction in index size affect the query results ?In order to evaluate this aspect , we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index , for different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively .We used the following two measures for comparison : i Relative Recall at cutoff level k RR@k , that measures the overlap between Gk and Ck , which ranges in 0 , 1 and is defined asii Kendall 's \u03c4 see 7 , 14 for a detailed definition at cutoff level k KT@k , measuring the agreement between two results in the relative order of items in Gk n Ck , with value 1 or -1 indicating total agreement or disagreement .Figure 3 plots , for cutoff levels 10 and 100 , the mean of RR@k and KT@k along with 5 % and 95 % percentiles , for different values of the threshold a starting from 0.01 .Note that for e = 0 , results coincide with those obtained by the original index , and hence are omitted from the graph .It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results , since RR@k and KT@k are within reasonable limits .For e = 0.01 , the smallest value of a in our experiments , RR@100 for WIKI is 0.98 indicating that the results arealmost indistinguishable from those obtained through the original index .Even the relative order of these common results is quite high , as the mean KT@100 is close to 0.95 .For the extreme value of e = 0.5 , which results in an index size of just 2.35 % of the original , the RR@100 and KT@100 are about 0.8 and 0.6 respectively .On the relatively less dynamic UKGOV dataset as can be seen from the \u03c3 values above , results were even better , with high values of RR and KT seen throughout the spectrum of a values for both cutoff values .We now turn our attention towards evaluating the sublist materialization techniques introduced in Section 6 .For both datasets , we started with the coalesced index produced by a moderate threshold setting of e = 0.10 .In order to reduce the computational effort , boundaries of elementary time intervals were rounded to day granularity before computing the sublist materializations .However , note that the postings in the materialized sublists still retain their original timestamps .For a comparative evaluation of the four approaches Poet , Soet , PG , and SB we measure space and performance as follows .The required space S M , as defined earlier , is equal to the total number of postings in the materialized sublists .To assess performance we compute the expected processing cost EPC for all terms in the respective query workload assuming a uniform probability distribution among query time-points .We report the mean EPC , as well as the 5 % and 95 % percentile .In other words , the mean EPC reflects the expected length of the index list in terms of index postings that needs to be scanned for a random time point and a random term from the query workload .The Soet and Poet approaches are , by their definition , parameter-free .For the PG approach , we varied its parameter y , which limits the maximal performance degradation , between 1.0 and 3.0 .Analogously , for the SB approach the parameter r , , as an upper-bound on the allowed space blowup , was varied between 1.0 and 3.0 .Solutions for the SB approach were obtained running simulated annealing for R = 50 , 000 rounds .Table 2 lists the obtained space and performance figures .Note that EPC values are smaller on WIKI than on UKGOV , since terms in the query workload employed for WIKI are relatively rarer in the corpus .Based on the depicted results , we make the following key observations .i As expected , Poet achieves optimal performance at the cost of an enormous space consumption .Soet , to the contrary , while consuming an optimal amount of space , provides only poor expected processing cost .The PG and SB methods , for different values of their respective parameter , produce solutions whose space and performance lie in between the extremes that Poet and Soet represent .ii For the PG method we see that for an acceptable performance degradation of only 10 % i.e. , y = 1.10 the required space drops by more than one order of magnitude in comparison to Poet on both datasets .iii The SB approach achieves close-to-optimal performance on both datasets , if allowed to consume at most three times the optimal amount of space i.e. , r , = 3.0 , which on our datasets still corresponds to a space reduction over Poet by more than one order of magnitude .We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12 .", "combined_keywords": ["time machine", "text search", "inverted file index", "temporal search", "approximate temporal coalescing", "web archive", "versioned document collection", "collaborative authoring environment", "timestamped information feed", "document-content overlap", "indexing range-based value", "open source search-engine nutch", "static indexpruning technique", "validity time-interval", "sublist materialization", "temporal text index", "time-travel text search"], "author_keywords": ["temporal text index", "time-travel text search", "web archive"], "method": "In the present work , we deal with a temporally versioned document collection D that is modeled as described in the following .Each document d E D is a sequence of its versions d = dt1 , dt2 , ... .Each version d ti has an associated timestamp ti reflecting when the version was created .Each version is a vector of searchable terms or features .Any modification to a document version results in the insertion of a new version with corresponding timestamp .We employ a discrete definition of time , so that timestamps are non-negative integers .The deletion of a document at time ti , i.e. , its disappearance from the current state of the collection , is modeled as the insertion of a special `` tombstone '' version 1 .The validity time-interval val d ti of a version d ti is ti , ti +1 , if a newer version with associated timestamp ti +1 exists , and ti , now otherwise where now points to the greatest possible value of a timestamp i.e. , ` dt : t < now .Putting all this together , we define the state D t of the collection at time t i.e. , the set of versions valid at t that are not deletions asAs mentioned earlier , we want to enrich a keyword query q with a timestamp t , so that q be evaluated over D t , i.e. , the state of the collection at time t .The enriched time-travel query is written as q t for brevity .As a retrieval model in this work we adopt Okapi BM25 27 , but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf 4 or language models 26 as well .For our considered setting , we slightly adapt Okapi BM25 as w q t , d ti = X wtf v , d ti .widf v , t .v \u2208 q In the above formula , the relevance w q t , d ti of a document version dti to the time-travel query q t is defined .We reiterate that q t is evaluated over D t so that only the version d ti valid at time t is considered .The first factor wtf v , d ti in the summation , further referred to as the tfscore is defined asIt considers the plain term frequency tf v , d ti of term v in version d ti normalizing it , taking into account both the length dl d ti of the version and the average document length avdl ti in the collection at time ti .The length-normalization parameter b and the tf-saturation parameter k1 are inherited from the original Okapi BM25 and are commonly set to values 1.2 and 0.75 respectively .The second factor widf v , t , which we refer to as the idf-score in the remainder , conveys the inverse document frequency of term v in the collection at time t and is defined aswhere N t = ID tI is the collection size at time t and df v , t gives the number of documents in the collection that contain the term v at time t .While the idf-score depends on the whole corpus as of the query time t , the tf-score is specific to each version .The inverted file index is a standard technique for text indexing , deployed in many systems .In this section , we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search .An inverted file index consists of a vocabulary , commonly organized as a B + Tree , that maps each term to its idfscore and inverted list .The index list Lv belonging to term v contains postings of the formwhere d is a document-identifier and p is the so-called payload .The payload p contains information about the term frequency of v in d , but may also include positional information about where the term appears in the document .The sort-order of index lists depends on which queries are to be supported efficiently .For Boolean queries it is favorable to sort index lists in document-order .Frequencyorder and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant documents 1 , 2 , 9 , 15 , 31 .A variety of compression techniques , such as encoding document identifiers more compactly , have been proposed 33 , 35 to reduce the size of index lists .For an excellent recent survey about inverted file indexes we refer to 35 .In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information .The main idea for inverted lists is that we include a validity timeinterval tb , te in postings to denote when the payload information was valid .The postings in our time-travel inverted file index are thus of the form d , p , tb , te where d and p are defined as in the standard inverted file index above and tb , te is the validity time-interval .As a concrete example , in our implementation , for a version d ti having the Okapi BM25 tf-score wtf v , d ti for term v , the index list Lv contains the posting d , wtf v , d ti , ti , ti +1 .Similarly , the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B+T ree .Unlike the tf-score , the idf-score of every term could vary with every change in the corpus .Therefore , we take a simplified approach to idf-score maintenance , by computing idf-scores for all terms in the corpus at specific possibly periodic times .During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary .Then , index lists are sequentially read from disk , thereby accumulating the information contained in the postings .We transparently extend the sequential reading , which is to the best of our knowledge common to all query processing techniques on inverted file indexes , thus making them suitable for time-travel queryprocessing .To this end , sequential reading is extended by skipping all postings whose validity time-interval does not contain t i.e. , t \u2208 6 tb , te .Whether a posting can be skipped can only be decided after the posting has been transferred from disk into memory and therefore still incurs significant I/O cost .As a remedy , we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly .We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of index lists .As a consequence , existing query-processing techniques and most optimizations e.g. , compression techniques remain equally applicable .If we employ the time-travel inverted index , as described in the previous section , to a versioned document collection , we obtain one posting per term per document version .For frequent terms and large highly-dynamic collections , thisleads to extremely long index lists with very poor queryprocessing performance .The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size .It builds on the observation that most changes in a versioned document collection are minor , leaving large parts of the document untouched .As a consequence , the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all .Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads , while keeping the maximal error bounded .This idea is illustrated in Figure 1 , which plots non-coalesced and coalesced scores of postings belonging to a single document .Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of postings from 9 to 3 in the example .The notion of temporal coalescing was originally introduced in temporal database research by B \u00a8 ohlen et al. 6 , where the simpler problem of coalescing only equal information was considered .We next formally state the problem dealt with in approximate temporal coalescing , and discuss the computation of optimal and approximate solutions .Note that the technique is applied to each index list separately , so that the following explanations assume a fixed term v and index list Lv .As an input we are given a sequence of temporally adjacent postingsEach sequence represents a contiguous time period during which the term was present in a single document d .If a term disappears from d but reappears later , we obtain multiple input sequences that are dealt with separately .We seek to generate the minimal length output sequence of postingsthat adheres to the following constraints : First , O and I must cover the same time-range , i.e. , ti = tj and tn = tm .Second , when coalescing a subsequence of postings of the input into a single posting of the output , we want the approximation error to be below a threshold ~ .In other words , if d , pi , ti , ti +1 and d , pj , tj , tj +1 are postings of I and O respectively , then the following must hold for a chosen error function and a threshold ~ : tj < ti n ti +1 < tj +1 = : > .error pi , pj < ~ .In this paper , as an error function we employ the relative error between payloads i.e. , tf-scores of a document in I and O , defined as : errrel pi , pj = Ipi pjI / IpiI .Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points ti , pi that uses a minimal number of segments while retaining the above approximation guarantee .Similar problems occur in time-series segmentation 21 , 30 and histogram construction 19 , 20 .Typically dynamic programming is scoreapplied to obtain an optimal solution in O n2 m * 20 , 30 time with m * being the number of segments in an optimal sequence .In our setting , as a key difference , only a guarantee on the local error is retained in contrast to a guarantee on the global error in the aforementioned settings .Exploiting this fact , an optimal solution is computable by means of induction 24 in O n2 time .Details of the optimal algorithm are omitted here but can be found in the accompanying technical report 5 .The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work .As an alternative , we introduce a linear-time approximate algorithm that is based on the sliding-window algorithm given in 21 .This algorithm produces nearly-optimal output sequences that retain the bound on the relative error , but possibly require a few additional segments more than an optimal solution .Algorithm 1 makes one pass over the input sequence I .While doing so , it coalesces sequences of postings having maximal length .The optimal representative for a sequence of postings depends only on their minimal and maximal payload pmin and pmax and can be looked up using optrep in O 1 see 16 for details .When reading the next posting , the algorithm tries to add it to the current sequence of postings .It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee .If this test fails , a coalesced posting bearing the old representative is added to the output sequence O and , following that , the bookkeeping is reinitialized .The time complexity of the algorithm is in O n .Note that , since we make no assumptions about the sort order of index lists , temporal-coalescing algorithms have an additional preprocessing cost in O | Lv | log | Lv | for sorting the index list and chopping it up into subsequences for each document .Efficiency of processing a query q t on our time-travel inverted index is influenced adversely by the wasted I/O due to read but skipped postings .Temporal coalescing implicitly addresses this problem by reducing the overall index list size , but still a significant overhead remains .In this section , we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinterval of time spanned by the full index .Each of these sublists contains all coalesced postings that overlap with the corresponding time interval of the sublist .Note that all those postings whose validity time-interval spans across the temporal boundaries of several sublists are replicated in each of the spanned sublists .Thus , in order to process the query q tand demandi.e. , the time intervals in M must completely cover the time interval t1 , tn , so that time-travel queries q t for all t \u2208 t1 , tn can be processed .We also assume that intervals in M are disjoint .We can make this assumption without ruling out any optimal solution with regard to space or performance defined below .The space required for the materialization of sublists in a set M is defined asit is sufficient to scan any materialized sublist whose timeinterval contains t .We illustrate the idea of sublist materialization using an example shown in Figure 2 .The index list Lv visualized in the figure contains a total of 10 postings from three documents d1 , d2 , and d3 .For ease of description , we have numbered boundaries of validity time-intervals , in increasing time-order , as t1 , ... , t10 and numbered the postings themselves as 1 , ... , 10 .Now , consider the processing of a query q t with t \u2208 t1 , t2 using this inverted list .Although only three postings postings 1 , 5 and 8 are valid at time t , the whole inverted list has to be read in the worst case .Suppose that we split the time axis of the list at time t2 , forming two sublists with postings 1 , 5 , 8 and 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 respectively .Then , we can process the above query with optimal cost by reading only those postings that existed at this t .At a first glance , it may seem counterintuitive to reduce index size in the first step using temporal coalescing , and then to increase it again using the sublist materialization techniques presented in this section .However , we reiterate that our main objective is to improve the efficiency of processing queries , not to reduce the index size alone .The use of temporal coalescing improves the performance by reducing the index size , while the sublist materialization improves performance by judiciously replicating entries .Further , the two techniques , can be applied separately and are independent .If applied in conjunction , though , there is a synergetic effect sublists that are materialized from a temporally coalesced index are generally smaller .We employ the notation Lv : ti , tj to refer to the materialized sublist for the time interval ti , tj , that is formally defined as ,To aid the presentation in the rest of the paper , we first provide some definitions .Let T = t1 ... tn be the sorted sequence of all unique time-interval boundaries of an inverted list Lv .Then we defineto be the set of elementary time intervals .We refer to the set of time intervals for which sublists are materialized asi.e. , the total length of all lists in M. Given a set M , we let \u03c0 ti , ti +1 = tj , tk \u2208 M : ti , ti +1 \u2286 tj , tk denote the time interval that is used to process queries q t with t \u2208 ti , ti +1 .The performance of processing queries q t for t \u2208 ti , ti +1 inversely depends on its processing costwhich is assumed to be proportional to the length of the list Lv : \u03c0 ti , ti +1 .Thus , in order to optimize the performance of processing queries we minimize their processing costs .One strategy to eliminate the problem of skipped postings is to eagerly materialize sublists for all elementary time intervals , i.e. , to choose M = E .In doing so , for every query q t only postings valid at time t are read and thus the best possible performance is achieved .Therefore , we will refer to this approach as Popt in the remainder .The initial approach described above that keeps only the full list Lv and thus picks M = t1 , tn is referred to as Sopt in the remainder .This approach requires minimal space , since it keeps each posting exactly once .Popt and Sopt are extremes : the former provides the best possible performance but is not space-efficient , the latter requires minimal space but does not provide good performance .The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the configuration spectrum between the Popt and the Sopt approach .The Popt approach clearly wastes a lot of space materializing many nearly-identical sublists .In the example illustrated in Figure 2 materialized sublists for t1 , t2 and t2 , t3 differ only by one posting .If the sublist for t1 , t3 was materialized instead , one could save significant space while incurring only an overhead of one skipped posting for all t \u2208 t1 , t3 .The technique presented next is driven by the idea that significant space savings over Popt are achievable , if an upper-bounded loss on the performance can be tolerated , or to put it differently , if a performance guarantee relative to the optimum is to be retained .In detail , the technique , which we refer to as PG Performance Guarantee in the remainder , finds a set M that has minimal required space , but guarantees for any elementary time interval ti , ti +1 and thus for any query q t with t \u2208 ti , ti +1 that performance is worse than optimal by at most a factor of y \u2265 1 .Formally , this problem can be stated asAn optimal solution to the problem can be computed by means of induction using the recurrencewhere C t1 , tj is the optimal cost i.e. , the space required for the prefix subproblemIntuitively , the recurrence states that an optimal solution for t1 , tk +1 be combined from an optimal solution to a prefix subproblem C t1 , tj and a time interval tj , tk +1 that can be materialized without violating the performance guarantee .Pseudocode of the algorithm is omitted for space reasons , but can be found in the accompanying technical report 5 .The time complexity of the algorithm is in O n 2 for each prefix subproblem the above recurrence must be evaluated , which is possible in linear time if list sizes | L : ti , tj | are precomputed .The space complexity is in O n 2 the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems .So far we considered the problem of materializing sublists that give a guarantee on performance while requiring minimal space .In many situations , though , the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not exceeding a given space limit .The technique presented next , which is named SB , tackles this very problem .The space restriction is modeled by means of a user-specified parameter r , \u2265 1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by Sopt .The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost and thus optimizes the expected performance .In the definition of the expected processing cost , P ti , ti +1 denotes the probability of a query time-point being in ti , ti +1 .Formally , this space-bound sublist-materialization problem can be stated asThe problem can be solved by using dynamic programming over an increasing number of time intervals : At each time interval in E the algorithms decides whether to start a new materialization time-interval , using the known best materialization decision from the previous time intervals , and keeping track of the required space consumption for materialization .A detailed description of the algorithm is omitted here , but can be found in the accompanying technical report 5 .Unfortunately , the algorithm has time complexity in O n3 | Lv | and its space complexity is in O n2 | Lv | , which is not practical for large data sets .We obtain an approximate solution to the problem using simulated annealing 22 , 23 .Simulated annealing takes a fixed number R of rounds to explore the solution space .In each round a random successor of the current solution is looked at .If the successor does not adhere to the space limit , it is always rejected i.e. , the current solution is kept .A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution .If it achieves higher expected processing cost , it is randomly accepted with probability e \u2212 o/r where \u0394 is the increase in expected processing cost and R \u2265 r \u2265 1 denotes the number of remaining rounds .In addition , throughout all rounds , the method keeps track of the best solution seenso far .The solution space for the problem at hand can be efficiently explored .As we argued above , we solely have to look at sets M that completely cover the time interval t1 , tn and do not contain overlapping time intervals .We represent such a set M as an array of n boolean variables b1 ... bn that convey the boundaries of time intervals in the set .Note that b1 and bn are always set to `` true '' .Initially , all n \u2212 2 intermediate variables assume `` false '' , which corresponds to the set M = t1 , tn .A random successor can now be easily generated by switching the value of one of the n \u2212 2 intermediate variables .The time complexity of the method is in O n 2 the expected processing cost must be computed in each round .Its space complexity is in O n for keeping the n boolean variables .As a side remark note that for \u03ba = 1.0 the SB method does not necessarily produce the solution that is obtained from Soft , but may produce a solution that requires the same amount of space while achieving better expected performance .", "conclusions": "In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections .Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results .The present work opens up many interesting questions for future research , e.g. : How can we even further improve performance by applying and possibly extending encoding , compression , and skipping techniques 35 ?.How can we extend the approach for queries q tb , te specifying a time interval instead of a time point ?How can the described time-travel text search functionality enable or speed up text mining along the time axis e.g. , tracking sentiment changes in customer opinions ?", "related work": "We can classify the related work mainly into the following two categories : i methods that deal explicitly with collections of versioned documents or temporal databases , and ii methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index .We briefly review work under these categories here .To the best of our knowledge , there is very little prior work dealing with historical search over temporally versioned documents .Anick and Flynn 3 , while pioneering this research , describe a help-desk system that supports historical queries .Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past .Burrows and Hisgen 10 , in a patent description , delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents .Recent work by N\u00f8rv\u02daag and Nyb\u00f8 25 and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results .Stack 29 reports practical experiences made when adapting the open source search-engine Nutch to search web archives .This adaptation , however , does not provide the intended time-travel text search functionality .In contrast , research in temporal databases has produced several index structures tailored for time-evolving databases ; a comprehensive overview of the state-of-art is available in 28 .Unlike the inverted file index , their applicability to text search is not well understood .Moving on to the second category of related work , Broder et al. 8 describe a technique that exploits large content overlaps between documents to achieve a reduction in index size .Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context .More recent approaches by Hersovici et al. 17 and Zhang and Suel 34 exploit arbitrary content overlaps between documents to reduce index size .None of the approaches , however , considers time explicitly or provides the desired time-travel text search functionality .Static indexpruning techniques 11 , 12 aim to reduce the effective index size , by removing portions of the index that are expected to have low impact on the query result .They also do not consider temporal aspects of documents , and thus are technically quite different from our proposal despite having a shared goal of index-size reduction .It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here ."}