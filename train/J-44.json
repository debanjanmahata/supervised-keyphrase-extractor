{"reader_keywords": ["nearest neighbor", "collaborative filtering algorithm", "aggregation process", "recommender", "rating", "purchase", "opinion", "scout", "promoter", "connector", "list rank accuracy", "neighborhood"], "reader_keywords_stem": ["nearest neighbor", "collabor filter algorithm", "aggreg process", "recommend", "rate", "purchas", "opinion", "scout", "promot", "connector", "list rank accuraci", "neighborhood"], "introduction": "Recommender systems have become integral to e-commerce , providing technology that suggests products to a visitor based on previous purchases or rating history .Collaborative filtering , a common form of recommendation , predicts a user 's rating for an item by combining other ratings of that user with other users ' ratings .Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms 2 , 7 , designing interfaces for presenting recommendations to users 1 , and studying the robustness of these algorithms 8 .However , with the exception of a few studies on the influence of users 10 , little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making good recommendations .Such an understanding will give an important handle to monitoring and managing a recommender system , to engineer mechanisms to sustain the recommender , and thereby ensure its continued success .Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating , allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering .We identify three possible roles : scouts to connect the user into the system to receive recommendations , promoters to connect an item into the system to be recommended , and connectors to connect ratings of these two kinds .Viewing ratings in this way , we can define the contribution of a rating in each role , both in terms of allowing recommendations to occur , and in terms of influence on the quality of recommendations .In turn , this capability helps support scenarios such as :items , shrinking with users leaving the system , items becoming irrelevant , and parts of the system under attack .Tracking the roles of a rating and its evolution over time provides many insights into the health of the system , and how it could be managed and improved .These include being able to identify rating subspaces that do not contribute or contribute negatively to system performance , and could be removed ; to enumerate users who are in danger of leaving , or have left the system ; and to assess the susceptibility of the system to attacks such as shilling 5 .As we show , the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community .The rest of the paper is organized as follows .Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2 .Section 3 defines and discusses the roles of a rating , and Section 4 defines measures of the contribution of a rating in each of these roles .In Section 5 , we illustrate the use of these roles to address the goals outlined above .", "title": "Scouts , Promoters , and Connectors : The Roles of Ratings in Nearest Neighbor Collaborative Filtering", "author_keywords_stem": ["recommender system", "collaborative filter", "neighborhood", "user-base and item-base algorithm", "scout", "promoter", "connector"], "background": "Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction .An algorithm of the first kind is called user-based , and one of the second kind is called itembased 12 .In both families of algorithms , neighborhoods are formed by first computing the similarity between all pairs of users for user-based or items for item-based .Predictions are then computed by aggregating ratings , which in a user-based algorithm involves aggregating the ratings of the target item by the user 's neighbors and , in an item-based algorithm , involves aggregating the user 's ratings of items that are neighbors of the target item .Algorithms within these families differ in the definition of similarity , formation of neighborhoods , and the computation of predictions .We consider a user-based algorithm based on that defined for GroupLens 11 with variations from Herlocker et al. 2 , and an item-based algorithm similar to that of Sarwar et al. 12 .The algorithm used by Resnick et al. 11 defines the similarity of two users u and v as the Pearson correlation of their common ratings :where Iu is the set of items rated by user u , ru , i is user u 's rating for item i , and \u00af ru is the average rating of user u similarly for v .Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings , to reduce the chance of making a recommendation made on weak connections : sim ' u , v = max | Iu \u2229 Iv | , \u03b3 \u00b7 sim u , v , \u03b3 where \u03b3 \u2248 5 is a constant used as a lower limit in scaling 2 .These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u .A prediction for user u and item i is computed by a weighted average of the ratings by the neighborswhere V = Nu \u2229 Ui is the set of users most similar to u who have rated i .The item-based algorithm we use is the one defined by Sarwar et al. 12 .In this algorithm , similarity is defined as the adjusted cosine measure where Ui is the set of users who have rated item i .As for the user-based algorithm , the similarity weights are adjusted proportionally to the number of users that have rated the items in common Given the similarities , the neighborhood Ni of an item i is defined as the top K most similar items for that item .A prediction for user u and item i is computed as the weighted averagewhere J = Ni \u2229 Iu is the set of items rated by u that are most similar to i.Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage 3 .Studies on recommender algorithms , notably Herlocker et al. 2 and Sarwar et al. 12 , typically compute predictive accuracy by dividing a set of ratings into training and test sets , and compute the prediction for an item in the test set using the ratings in the training set .A standard measure of predictive accuracy is mean absolute error MAE , which for a test setCoverage has a number of definitions , but generally refers to the proportion of items that can be predicted by the algorithm 3 .A practical issue with predictive accuracy is that users typically are presented with recommendation lists , and not individual numeric predictions .Recommendation lists are lists of items in decreasing order of prediction sometimes stated in terms of star-ratings , and so predictive accuracy may not be reflective of the accuracy of the list .So , instead we can measure recommendation or rank accuracy , which indicates the extent to which the list is in the correct order .Herlocker et al. 3 discuss a number of rank accuracy measures , which range from Kendall 's Tau to measures that consider the fact that users tend to only look at a prefix of the list 5 .Kendall 's Tau measures the number of inversions when comparing ordered pairs in the true user ordering ofwhere C is the number of pairs that the system predicts in the correct order , D the number of pairs the system predicts in the wrong order , TR the number of pairs in the true ordering that have the same ratings , and TP is the number of pairs in the predicted ordering that have the same ratings 3 .A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs 3 .For instance , an inversion toward the end of the list is given the same weight as one in the beginning .One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list .", "abstract": "Recommender systems aggregate individual user ratings into predictions of products or services that might interest visitors .The quality of this aggregation process crucially affects the user experience and hence the effectiveness of recommenders in e-commerce .We present a novel study that disaggregates global recommender performance metrics into contributions made by each individual rating , allowing us to characterize the many roles played by ratings in nearestneighbor collaborative filtering .In particular , we formulate three roles scouts , promoters , and connectors that capture how users receive recommendations , how items get recommended , and how ratings of these two types are themselves connected resp ..These roles find direct uses in improving recommendations for users , in better targeting of items and , most importantly , in helping monitor the health of the system as a whole .For instance , they can be used to track the evolution of neighborhoods , to identify rating subspaces that do not contribute or contribute negatively to system performance , to enumerate users who are in danger of leaving , and to assess the susceptibility of the system to attacks such as shilling .We argue that the three rating roles presented here provide broad primitives to manage a recommender system and its community .", "id": "J-44", "combined_keywords_stem": ["nearest neighbor", "collabor filter algorithm", "aggreg process", "recommend", "rate", "purchas", "opinion", "scout", "promot", "connector", "list rank accuraci", "neighborhood", "recommend system", "collabor filter", "user-base and item-base algorithm"], "evaluation": "As we have seen , we can assign role values to each rating when evaluating a collaborative filtering system .In this section , we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset .In particular , we discuss results relating to identifying good scouts , promoters , and connectors ; the evolution of rating roles ; and the characterization of user neighborhoods .Our experiments use the MovieLens million rating dataset , which consists of ratings by 6040 users of 3952 movies .The ratings are in the range 1 to 5 , and are labeled with the time the rating was given .As discussed before , we consider only the item-based algorithm here with item neighborhoods of size 30 and , due to space considerations , only present role value results for rank accuracy .Since we are interested in the evolution of the rating role values over time , the model of the recommender system is built by processing ratings in their arrival order .The timestamping provided by MovieLens is hence crucial for the analyses presented here .We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset giving rise to 20 snapshots .We incrementally update the role values as the time ordered ratings are merged into the model .To keep the experiment computationally manageable , we define a test dataset for each user .As the time ordered ratings are merged into the model , we label a small randomly selected percentage 20 % as test data .At discrete epochs , i.e. , after processing every 10,000 ratings , we compute the predictions for the ratings in the test data , and then compute the role values for the ratings used in the predictions .One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles .We overcome this concern by repeating the experiment , using different random seeds .The probability that every rating is considered for evaluation is then considerably high : 1 \u2212 0.2 n , where n is the number of times the experiment is repeated with different random seeds .The results here are based on n = 4 repetitions .The item-based collaborative filtering algorithm 's performance was ordinary with respect to rank accuracy .Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model .The recall was always high , but the average precision was just about 53 % .The ratings of a user that serve as scouts are those that allow the user to receive recommendations .We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values .Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the user 's preference .Whether all of these discordant pairs are observable by the user is unclear , however , certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists .The distribution of the scout values for most users ' ratings are Gaussian with mean zero .Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot .We observe that a large number of ratings never serve as scouts for their users .A relatable scenario is when Amazon 's recommender makes suggestions of books or items based on other items that were purchased as gifts .With simple relevance feedback from the user , such ratings can be isolated as bad scouts and discounted from future predictions .Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal .An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. 9 .They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE .Following their intuition , we would expect to see a higher correlation between popularityentropy and good scouts .We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie number of times it is rated ; and with its popularity * variance measure at different snapshots of the system .Note that the scout values were initially anti-correlated with popularity Fig. 7 , but became moderately correlated as the system evolved .Both popularity and popularity * variance performed similarly .A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings .By studying the evolution of scout values , we can identify movies that consistently feature in good scouts over time .We claim these movies will make viable scouts for other users .We found the aggregated scout values for all movies in intervals of 10,000 ratings each .A movie is said to induce a good scout if the movie was in the top 100 of the sorted list , and to induce a bad scout if it was in bottom 100 of the same list .Movies appearing consistently high over time are expected to remain up there in the future .The effective confidence in a movie m iswhere Tm is the number of times it appeared in the top 100 , Bm the number of times it appeared in the bottom 100 , and N is the number of intervals considered .Using this measure , the top few movies expected to induce the best scouts are shown in Table 1 .Movies that would be bad scout choices are shown in Table 2 with their associated confidences .The popularities of the movies are also displayed .Although more popular movies appear in the list of good scouts , these tables show that a blind choice of scout based on popularity alone can be potentially dangerous .Interestingly , the best scout ` Being John Malkovich ' is about a puppeteer who discovers a portal into a movie star , a movie that has been described variously on amazon.com as ` makes you feel giddy , ' ` seriously weird , ' ` comedy with depth , ' ` silly , ' ` strange , ' and ` inventive . 'Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood , with similar preferences .On the other hand , several factors may have made a movie a bad scout , like the sharp variance in user preferences in the neighborhood of a movie .Two users may have the same opinion about Lawrence of Arabia , but they may differ sharply about how they felt about the other movies they saw .Bad scouts ensue when there is deviation in behavior around a common synchronization point .Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users .So , inducing good promoters is important for cold-start recommendation .We note that the frequency distribution of promoter values for a sample movie 's ratings is also Gaussian similar to Fig. 6 .This indicates that the promotion of a movie is benefited most by the ratings of a few users , and are unaffected by the ratings of most users .We find a strong correlation between a user 's number of ratings and his aggregated promoter value .Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user number of ratings versus his aggregated promoter value .We expect that conspicuous shills , by recommending wrong movies to users , will be discredited with negative aggregate promoter values and should be identifiable easily .Given this observation , the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values .A new movie is thus cast into the neighborhood of many other movies improving its visibility .Note , though , that a user may have long stopped using the system .Tracking promoter values consistently allows only the most active recent users to be considered .Given the way scouts , connectors , and promoters are characterized , it follows that the movies that are part of the best scouts are also part of the best connectors .Similarly , the users that constitute the best promoters are also part of the best connectors .Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value .In our experiments , we find that a rating 's longest standing role is often as a connector .A rating with a poor connector value is often seen due to its user being a bad promoter , or its movie being a bad scout .Such ratings can be removed from the prediction process to bring marginal improvements to recommendations .In some selected experiments , we observed that removing a set of badly behaving connectors helped improve the system 's overall performance by 1.5 % .The effect was even higher on a few select users who observed an improvement of above 10 % in precision without much loss in recall .One of the more significant contributions of our work is the ability to model the evolution of recommender systems , by studying the changing roles of ratings over time .The role and value of a rating can change depending on many factors like user behavior , redundancy , shilling effects or properties of the collaborative filtering algorithm used .Studying the dynamics of rating roles in terms of transitions between good , bad , and negligible values can provide insights into the functioning of the recommender system .We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system .We classify different rating states as good , bad , or negligible .Consider a user who has rated 100 movies in a particular interval , of which 20 are part of the test set .If a scout has a value greater than 0.005 , it indicates that it is uniquely involved in at least 2 concordant predictions , which we will say is good .Thus , a threshold of 0.005 is chosen to bin a rating as good , bad or negligible in terms of its scout , connector and promoter value .For instance , a rating r , at time t with role value triple 0.1 , 0.001 , \u2212 0.01 is classified as scout + , connector 0 , promoter \u2212 , where + indicates good , 0 indicates negligible , and \u2212 indicates bad .The positive credit held by a rating is a measure of its contribution to the betterment of the system , and the discredit is a measure of its contribution to the detriment of the system .Even though the positive roles and the negative roles make up a very small percentage of all ratings , their contribution supersedes their size .For example , even though only 1.7 % of all ratings were classified as good scouts , they hold 79 % of all positive credit in the system !Similarly , the bad scouts were just 1.4 % of all ratings but hold 82 % of all discredit .Note that good and bad scouts , together , comprise only 1.4 % + 1.7 % = 3.1 % of the ratings , implying that the majority of the ratings are negligible role players as scouts more on this later .Likewise , good connectors were 1.2 % of the system , and hold 30 % of all positive credit .The bad connectors 0.8 % of the system hold 36 % of all discredit .Good promoters 3 % of the system hold 46 % of all credit , while bad promoters 2 % hold 50 % of all discredit .This reiterates that a few ratings influence most of the system 's performance .Hence it is important to track transitions between them regardless of their small numbers .Across different snapshots , a rating can remain in the same state or change .A good scout can become a bad scout , a good promoter can become a good connector , good and bad scouts can become vestigial , and so on .It is not practical to expect a recommender system to have no ratings in bad roles .However , it suffices to see ratings in bad roles either convert to good or vestigial roles .Similarly , observing a large number of good roles become bad ones is a sign of imminent failure of the system .We employ the principle of non-overlapping episodes 6 to count such transitions .A sequence such as : + , 0 , 0 + + , 0 , 0 + 0 , + , 0 + 0 , 0 , 0 is interpreted as the transitions + , 0 , 0 ^ .* 0 , + , 0 : 1 + , 0 , 0 ^ .* 0 , 0 , 0 : 1 0 , + , 0 ^ .* 0 , 0 , 0 : 1 instead of + , 0 , 0 ^ .* 0 , + , 0 : 2 + , 0 , 0 ^ .* 0 , 0 , 0 : 2 0 , + , 0 ^ .* 0 , 0 , 0 : 1 .See 6 for further details about this counting procedure .Thus , a rating can be in one of 27 possible states , and there are about 272 possible transitions .We make a further simplification and utilize only 9 states , indicating whether the rating is a scout , promoter , or connector , and whether it has a positive , negative , or negligible role .Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states .In this model , a transition such as + , 0 , + ^ .* 0 , + , 0 : 1 is counted asOf these , transitions like pX ^ .* q0 where p = q , X \u2208 + , 0 , \u2212 are considered uninteresting , and only the rest are counted .Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset .Only transitions with frequency greater than or equal to 3 % are shown .The percentages for each state indicates the number of ratings that were found to be in those states .We consider transitions from any state to a good state as healthy , from any state to a bad state as unhealthy , and from any state to a vestigial state as decaying .From Fig. 9 , we can observe : \u2022 The bulk of the ratings have negligible values , irrespective of their role .The majority of the transitions involve both good and bad ratings becoming negligible .\u2022 The number of good ratings is comparable to the bad ratings , and ratings are seen to switch states often , except in the case of scouts see below .Note that these results are conditioned by the static nature of the dataset , which is a set of ratings over a fixed window of time .However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system .For instance , acceptable limits can be imposed on different types of transitions and , if a transition fails to meet the threshold , the recommender system or a part of it can be brought under closer scrutiny .Furthermore , the role state transition diagram would also be the ideal place to study the effects of shilling , a topic we will consider in future research .Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L fora user .In our experiment , we consider lists of length 30 , and sample the lists of about 5 % of users through the evolution of the model at intervals of 10,000 ratings each and compute their neighborhood characteristics .To simplify our presentation , we consider the percentage of the sample that fall into one of the following categories :From our sample set of 561 users , we found that 476 users were inactive .Of the remaining 85 users , we found 26 users had good scouts and a good neighborhood , 6 had bad scouts and a good neighborhood , 29 had good scouts and a bad neighborhood , and 24 had bad scouts and a bad neighborhood .Thus , we conjecture that 59 users 29 +24 +6 are in danger of leaving the system .As a remedy , users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the system 's recommendations .The system can be expected to deliver more if they engineer some good scouts .Users with good scouts and a bad neighborhood are harder to address ; this situation might entail selectively removing some connector-promoter pairs that are causing the damage .Handling users with bad scouts and bad neighborhoods is a more difficult challenge .Such a classification allows the use of different strategies to better a user 's experience with the system depending on his context .In future work , we intend to conduct field studies and study the improvement in performance of different strategies for different contexts .", "combined_keywords": ["nearest neighbor", "collaborative filtering algorithm", "aggregation process", "recommender", "rating", "purchase", "opinion", "scout", "promoter", "connector", "list rank accuracy", "neighborhood", "recommender system", "collaborative filter", "user-base and item-base algorithm"], "author_keywords": ["recommender system", "collaborative filter", "neighborhood", "user-base and item-base algorithm", "scout", "promoter", "connector"], "method": "Our basic observation is that each rating plays a different role in each prediction in which it is used .Consider a simplified movie recommender system with three users Jim , Jeff , and Tom and their ratings for a few movies , as shown in Fig. 1 .For this initial discussion we will not consider the rating values involved .The recommender predicts whether Tom will like The Mask using the other already available ratings .How this is done depends on the algorithm : 1 .An item-based collaborative filtering algorithm constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly e.g. , Jim 's ratings of The Matrix and The Mask ; and Jeff 's ratings of Star Wars and The Mask .Tom 's ratings of those movies are then used to make a prediction for The Mask .2 .A user-based collaborative filtering algorithm would construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Tom 's e.g. , Tom and Jeff have rated Star Wars ; Tom and Jim have rated The Matrix .The prediction of Tom 's rating for The Mask is then based on the ratings of Jeff and Tim .Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions , we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings .So , irrespective of the collaborative filtering algorithm used , we can visualize the prediction of Tom 's rating of The Mask as walking through a sequence of ratings .Inthis example , two paths were used for this prediction as depicted in Fig. 2 : p1 , p2 , p3 and q1 , q2 , q3 .Note that these paths are undirected , and are all of length 3 .Only the order in which the ratings are traversed is different between the item-based algorithm e.g. , p3 , p2 , p1 , q3 , q2 , q1 and the user-based algorithm e.g. , p1 , p2 , p3 , q1 , q2 , q3 .A rating can be part of many paths for a single prediction as shown in Fig. 3 , where three paths are used for a prediction , two of which follow p1 : p1 , p2 , p3 and p1 , r2 , r3 .Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel , each playing a part in influencing the predicted value .Each prediction path consists of three ratings , playing roles that we call scouts , promoters , and connectors .To illustrate these roles , consider the path p1 , p2 , p3 in Fig. 2 used to make a prediction of The Mask for Tom : 1 .The rating p1 Tom > Star Wars makes a connection from Tom to other ratings that can be used to predict Tom 's rating for The Mask .This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask .2 .The rating p2 Jeff > Star Wars helps the system recommend The Mask to Tom by connecting the scout to the promoter .3 .The rating p3 Jeff > The Mask allows connections to The Mask , and , therefore , promotes this movie to Tom .Formally , given a prediction pu , a of a target item a for user u , a scout for pu , a is a rating ru , i such that there exists a user v with ratings rv , a and rv , i for some item i ; a promoter for pu , a is a rating rv , a for some user v , such that there exist ratings rv , i and ru , i for an item i , and ; a connector for pu , ais a rating rv , i by some user v and rating i , such that there exists ratings ru , i and rv , a .The scouts , connectors , and promoters for the prediction of Tom 's rating of The Mask are p1 and q1 , p2 and q2 , and p3 and q3 respectively .Each of these roles has a value in the recommender to the user , the user 's neighborhood , and the system in terms of allowing recommendations to be made .Ratings that act as scouts tend to help the recommender system suggest more movies to the user , though the extent to which this is true depends on the rating behavior of other users .For example , in Fig. 4 the rating Tom \u2192 Star Wars helps the system recommend only The Mask to him , while Tom \u2192 The Matrix helps recommend The Mask , Jurassic Park , and My Cousin Vinny .Tom makes a connection to Jim who is a prolific user of the system , by rating The Matrix .However , this does not make The Matrix the best movie to rate for everyone .For example , Jim is benefited equally by both The Mask and The Matrix , which allow the system to recommend Star Wars to him .His rating of The Mask is the best scout for Jeff , and Jerry 's only scout is his rating of Star Wars .This suggests that good scouts allow a user to build similarity with prolific users , and thereby ensure they get more from the system .While scouts represent beneficial ratings from the perspective of a user , promoters are their duals , and are of benefit to items .In Fig. 4 , My Cousin Vinny benefits from Jim 's rating , since it allows recommendations to Jeff and Tom .The Mask is not so dependent on just one rating , since the ratings by Jim and Jeff help it .On the other hand , Jerry 's rating of Star Wars does not help promote it to any other user .We conclude that a good promoter connects an item to a broader neighborhood of other items , and thereby ensures that it is recommended to more users .Connectors serve a crucial role in a recommender system that is not as obvious .The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff , Jerry and Tom based on the linkage structure illustrated in Fig. 4 .Beside the fact that Jim rated these movies , these recommendations are possible only because of the ratings Jim \u2192 The Matrix and Jim \u2192 The Mask , which are the best connectors .A connector improves the system 's ability to make recommendations with no explicit gain for the user .Note that every rating can be of varied benefit in each of these roles .The rating Jim \u2192 My Cousin Vinny is a poor scout and connector , but is a very good promoter .The rating Jim \u2192 The Mask is a reasonably good scout , a very good connector , and a good promoter .Finally , the rating Jerry \u2192 Star Wars is a very good scout , but is of no value as a connector or promoter .As illustrated here , a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not .We could measure this value by simply counting the number of times a rating is used in each role , which alone would be helpful in characterizing the behavior of a system .But we can also measure the contribution of each rating to the quality of recommendations or health of the system .Since every prediction is a combined effort of several recommendation paths , we are interested in discerning the influence of each rating and , hence , each path in the system towards the system 's overall error .We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played .The next section describes the approach to measuring the values of a rating in each role .As we 've seen , a rating may play different roles in different predictions and , in each prediction , contribute to the quality of a prediction in different ways .Our approach can use any numeric measure of a property of system health , and assigns credit or blame to each rating proportional to its influence in the prediction .By tracking the role of each rating in a prediction , we can accumulate the credit for a rating in each of the three roles , and also track the evolution of the roles of rating over time in the system .This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating , and then instantiating the approach for predictive accuracy , and then rank accuracy .We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a user 's recommendations .Note that although our general formulation for rating influence is algorithm independent , due to space considerations , we present the approach for only item-based collaborative filtering .The definition for user-based algorithms is similar and will be presented in an expanded version of this paper .Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user .As described earlier , similarity is defined by the adjusted cosine measure Equations 2 and 3 .A set of the top K neighbors is maintained for all items for space and computational efficiency .A prediction of item i for a user u is computed as the weighted deviation from the item 's mean rating as shown in Equation 4 .The list of recommendations for a user is then the list of items sorted in descending order of their predicted values .We first define impact a , i , j , the impact a user a has in determining the similarity between two items i and j .This is the change in the similarity between i and j when a 's rating is removed , and is defined asof items i and j users who rate both i and j , R u is the set of ratings provided by user u , and sim ' \u00af a i , j is the similarity of i and j when the ratings of user a are removedand adjusted for the number of ratersThe influence of each path u , j , v , i = ru , j , rv , j , rv , i in the prediction of pu , i is given byIt follows that the sum of influences over all such paths , for a given set of endpoints , is 1 .The value of a rating in each role is computed from the influence depending on the evaluation measure employed .Here we illustrate the approach using predictive accuracy as the evaluation metric .In general , the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role .For predictive accuracy , the error in prediction e = | pu , i \u2212 ru , i | is mapped to a comfort level using a mapping function M e .Anecdotal evidence suggests that users are unable to discern errors less than 1.0 for a rating scale of 1 to 5 4 , and so an error less than 1.0 is considered acceptable , but anything larger is not .We hence define M e as 1 \u2212 e binned to an appropriate value in \u2212 1 , \u2212 0.5 , 0.5 , 1 .For each prediction pu , i , M e is attributed to all the paths that assisted the computation of pu , i , proportional to their influences .This tribute , M e \u2217 influence u , j , v , i , is in turn inherited by each of the ratings in the path ru , j , rv , j , rv , i , with the credit/blame accumulating to the respective roles of ru , j as a scout , rv , j as a connector , and rv , i as a promoter .In other words , the scout value SF ru , j , the connector value CF rv , j and the promoter value PF rv , i are all incremented by the tribute amount .Over a large number of predictions , scouts that have repeatedly resulted in big error rates have a big negative scout value , and vice versa similarly with the other roles .Every rating is thus summarized by its triple SF , CF , PF .We now define the computation of the contribution of ratings to observed rank accuracy .For this computation , we must know the user 's preference order for a set of items for which predictions can be computed .We assume that we have a test set of the users ' ratings of the items presented in the recommendation list .For every pair of items rated by a user in the test data , we check whether the predicted order is concordant with his preference .We say a pair i , j is concordant with error e whenever one of the following holds :where t is the number of items in the user 's test set whose ratings could be predicted , T is the number of items rated by user u in the test set , C is the number of concordances and D is the number of discordances .The credit c is then divided among all paths responsible for predicting pu , i and pu , j proportional to their influences .We again add the role values obtained from all the experiments to form a triple SF , CF , PF for each rating .After calculating the role values for individual ratings , we can also use these values to study neighborhoods and the system .Here we consider how we can use the role values to characterize the health of a neighborhood .Consider the list of top recommendations presented to a user at a specific point in time .The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations .We call these ratings the recommender neighborhood of the user .The user implicitly chooses this neighborhood of ratings through the items he rates .Apart from the collaborative filtering algorithm , the health of this neighborhood completely influences a user 's satisfaction with the system .We can characterize a user 's recommender neighborhood by aggregating the individual role values of the ratings involved , weighted by the influence of individual ratings in determining his recommended list .Different sections of the user 's neighborhood wield varied influence on his recommendation list .For instance , ratings reachable through highly rated items have a bigger say in the recommended items .Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood .A user can have a good set of scouts , but may be exposed to a neighborhood with bad connectors and promoters .He can have a good neighborhood , but his bad scouts may ensure the neighborhood 's potential is rendered useless .We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future .A user 's neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations .Consider a user u and his ordered list of recommendations L .An item iin the list is weighted inversely , as K i , depending on its popsition in the list .In our studies we use K i = position i .Several paths of ratings ru , j , rv , j , rv , i are involved in predicting pu , i which ultimately decides its position in L , each with inf luence u , j , v , i .The recommender neighborhood of a user u is characterized by the triple , SFN u , CFN u , PFN u whereCFN u and PFN u are defined similarly .This triple estimates the quality of u 's recommendations based on the past track record of the ratings involved in their respective roles .", "conclusions": "To further recommender system acceptance and deployment , we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings .A fine-grained characterization in terms of rating roles such as scouts , promoters , and connectors , as done here , helps such an endeavor .Although we have presented results on only the item-based algorithm with list rank accuracy as the metric , the same approach outlined here applies to user-based algorithms and other metrics .In future research , we plan to systematically study the many algorithmic parameters , tolerances , and cutoff thresholds employed here and reason about their effects on the downstream conclusions .We also aim to extend our formulation to other collaborative filtering algorithms , study the effect of shilling in altering rating roles , conduct field studies , and evaluate improvements in user experience by tweaking ratings based on their role values .Finally , we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health ."}