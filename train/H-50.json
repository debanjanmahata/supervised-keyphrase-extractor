{"reader_keywords": ["rank aggregation", "information retrieval", "datum fusion problem", "multiple criterion framework", "decision rule", "outranking approach", "metasearch engine", "combsum and combmnz strategy", "majoritarian method", "ir model"], "reader_keywords_stem": ["rank aggreg", "inform retriev", "data fusion problem", "multipl criterion framework", "decis rule", "outrank approach", "metasearch engin", "combsum and combmnz strategi", "majoritarian method", "ir model"], "introduction": "A wide range of current Information Retrieval IR approaches are based on various search models Boolean , Vector Space , Probabilistic , Language , etc. 2 in order to retrieve relevant documents in response to a user request .The result lists produced by these approaches depend on the exact definition of the relevance concept .Rank aggregation approaches , also called data fusion approaches , consist in combining these result lists in order to produce a new and hopefully better ranking .Such approaches give rise to metasearch engines in the Web context .We consider , in the following , cases where only ranks areavailable and no other additional information is provided such as the relevance scores .This corresponds indeed to the reality , where only ordinal information is available .Data fusion is also relevant in other contexts , such as when the user writes several queries of his/her information need e.g. , a boolean query and a natural language query 4 , or when many document surrogates are available 16 .Several studies argued that rank aggregation has the potential of combining effectively all the various sources of evidence considered in various input methods .For instance , experiments carried out in 16 , 30 , 4 and 19 showed that documents which appear in the lists of the majority of the input methods are more likely to be relevant .Moreover , Lee 19 and Vogt and Cottrell 31 found that various retrieval approaches often return very different irrelevant documents , but many of the same relevant documents .Bartell et al. 3 also found that rank aggregation methods improve the performances w.r.t. those of the input methods , even when some of them have weak individual performances .These methods also tend to smooth out biases of the input methods according to Montague and Aslam 22 .Data fusion has recently been proved to improve performances for both the ad hoc retrieval and categorization tasks within the TREC genomics track in 2005 1 .The rank aggregation problem was addressed in various fields such as i in social choice theory which studies voting algorithms which specify winners of elections or winners of competitions in tournaments 29 , ii in statistics when studying correlation between rankings , iii in distributed databases when results from different databases must be combined 12 , and iv in collaborative filtering 23 .Most current rank aggregation methods consider each input ranking as a permutation over the same set of items .They also give rigid interpretation to the exact ranking of the items .Both of these assumptions are rather not valid in the IR context , as will be shown in the following sections .The remaining of the paper is organized as follows .We first review current rank aggregation methods in Section 2 .Then we outline the specificities of the data fusion problem in the IR context Section 3 .In Section 4 , we present a new aggregation method which is proven to best fit the IR context .Experimental results are presented in Section 5 and conclusions are provided in a final section .", "title": "An Outranking Approach for Rank Aggregation in Information Retrieval", "author_keywords_stem": ["datum fusion", "metasearch engine", "multiple criterium approach", "outrank method", "rank aggregation"], "abstract": "Research in Information Retrieval usually shows performance improvement when many sources of evidence are combined to produce a ranking of documents e.g. , texts , pictures , sounds , etc. .In this paper , we focus on the rank aggregation problem , also called data fusion problem , where rankings of documents , searched into the same collection and provided by multiple methods , are combined in order to produce a new ranking .In this context , we propose a rank aggregation method within a multiple criteria framework using aggregation mechanisms based on decision rules identifying positive and negative reasons for judging whether a document should get a better rank than another .We show that the proposed method deals well with the Information Retrieval distinctive features .Experimental results are reported showing that the suggested method performs better than the well-known CombSUM and CombMNZ operators .", "id": "H-50", "combined_keywords_stem": ["rank aggreg", "inform retriev", "data fusion problem", "multipl criterion framework", "decis rule", "outrank approach", "metasearch engin", "combsum and combmnz strategi", "majoritarian method", "ir model", "data fusion", "multipl criterium approach", "outrank method"], "evaluation": "To facilitate empirical investigation of the proposed methodology , we developed a prototype metasearch engine that implements a version of our outranking approach for rank aggregation .In this paper , we apply our approach to the Topic Distillation TD task of TREC-2004 Web track 10 .In this task , there are 75 topics where only a short description of each is given .For each query , we retained the rankings of the 10 best runs of the TD task which are provided by TREC-2004 participating teams .The performances of these runs are reported in table 3 .For each query , each run provides a ranking of about 1000 documents .The number of documents retrieved by all these runs ranges from 543 to 5769 .Their average median number is 3340 3386 .It is worth noting that we found similar distributions of the documents among the rankings as in 11 .For evaluation , we used the ` trec eval ' standard tool which is used by the TREC community to calculate the standard measures of system effectiveness which are Mean Average Precision MAP and Success@n S0n for n = 1 , 5 and 10 .Our approach effectiveness is compared against some high performing official results from TREC-2004 as well as against some standard rank aggregation algorithms .In the experiments , significance testing is mainly based on the t-student statistic which is computed on the basis of the MAP values of the compared runs .In the tables of the following section , statistically significant differences are marked with an asterisk .Values between brackets of the first column of each table , indicate the parameter value of the corresponding run .We carried out several series of runs in order to i study performance variations of the outranking approach when tuning the parameters and working assumptions , ii compare performances of the outranking approach vs standard rank aggregation strategies , and iii check whether rank aggregation performs better than the best input rankings .We set our basic run mcm with the following parameters .We considered that each input ranking is a complete order sp = 0 and that an input ranking strongly refutes di\u03c3di , when the difference of both document positions is large enough sv = 75 % .Preference and veto thresholds are computed proportionally to the number of documents retained in each input ranking .They consequently may vary from one ranking to another .In addition , to accept the assertion di\u03c3dii , we supposed that the majority of the rankings must be concordant cmin = 50 % and that every input ranking can impose its veto dmax = 0 .Concordance and discordance thresholds are computed for each tuple di , dii as the percentage of the input rankings of PRi \u2229 PRii .Thus , our choice of parameters leads to the definition of the outranking relation S 0,75 % ,50 % ,0 .To test the run mcm , we had chosen the following assumptions .We retained the top 100 best documents from each input ranking H1100 , only considered documents which are present in at least half of the input rankings H25 and assumed H3no and H4 new .In these conditions , the number of successful documents was about 100 on average , and the computation time per query was less than one second .Obviously , modifying the working assumptions should have deeper impact on the performances than tuning our model parameters .This was validated by preliminary experiments .Thus , we hereafter begin by studying performance variation when different sets of assumptions are considered .Afterwards , we study the impact of tuning parameters .Finally , we compare our model performances w.r.t. the input rankings as well as some standard data fusion algorithms .Table 4 summarizes the performance variation of the outranking approach under different working hypotheses .Inthis table , we first show that run mcm22 , in which missing documents are all put in the same last position of each input ranking , leads to performance drop w.r.t. run mcm .Moreover , S01 moves from 41.33 % to 34.67 % -16.11 % .This shows that several relevant documents which were initially put at the first position of the consensus ranking in mcm , lose this first position but remain ranked in the top 5 documents since S05 did not change .We also conclude that documents which have rather good positions in some input rankings are more likely to be relevant , even though they are missing in some other rankings .Consequently , when they are missing in some rankings , assigning worse ranks to these documents is harmful for performance .Also , from Table 4 , we found that the performances of runs mcm and mcm23 are similar .Therefore , the outranking approach is not sensitive to keeping the initial positions of candidate documents or recomputing them by discarding excluded ones .From the same Table 4 , performance of the outranking approach increases significantly for runs mcm24 and mcm25 .Therefore , whether we consider all the documents which are present in half of the rankings mcm24 or we consider all the documents which are ranked in the first 100 positions in one or more rankings mcm25 , increases performances .This result was predictable since in both cases we have more detailed information on the relative importance of documents .Tables 5 and 6 confirm this evidence .Table 5 , where values between brackets of the first column give the number of documents which are retained from each input ranking , shows that selecting more documents from each input ranking leads to performance increase .It is worth mentioning that selecting more than 600 documents from each input ranking does not improve performance .the run mcm32 , only documents which are present in 3 or more input rankings , were considered successful .This table shows that performance is significantly better when rare documents are considered , whereas it decreases significantly when these documents are discarded .Therefore , we conclude that many of the relevant documents are retrieved by a rather small set of IR models .For both runs mcm24 and mcm25 , the number of successful documents was about 1000 and therefore , the computation time per query increased and became around 5 seconds .Table 7 shows performance variation of the outranking approach when different preference thresholds are considered .We found performance improvement up to threshold values of about 5 % , then there is a decrease in the performance which becomes significant for threshold values greater than 10 % .Moreover , S@1 improves from 41.33 % to 46.67 % when preference threshold changes from 0 to 5 % .We can thus conclude that the input rankings are semi orders rather than complete orders .Table 8 shows the evolution of the performance measures w.r.t. the concordance threshold .We can conclude that in order to put document di before di , in the consensus ranking ,at least half of the input rankings of PRi \u2229 PRii should be concordant .Performance drops significantly for very low and very high values of the concordance threshold .In fact , for such values , the concordance condition is either fulfilled rather always by too many document pairs or not fulfilled at all , respectively .Therefore , the outranking relation becomes either too weak or too strong respectively .In the experiments , varying the veto threshold as well as the discordance threshold within reasonable intervals does not have significant impact on performance measures .In fact , runs with different veto thresholds sv \u2208 50 % ; 100 % had similar performances even though there is a slight advantage for runs with high threshold values which means that it is better not to allow the input rankings to put their veto easily .Also , tuning the discordance threshold was carried out for values 50 % and 75 % of the veto threshold .For these runs we did not get any noticeable performance variation , although for low discordance thresholds dmax < 20 % , performance slightly decreased .To study performance evolution when different sets of input rankings are considered , we carried three more runs where 2 , 4 , and 6 of the best performing sets of the input rankings are considered .Results reported in Table 9 are seemingly counter-intuitive and also do not support previous findings regarding rank aggregation research 3 .Nevertheless , this result shows that low performing rankings bring more noise than information to the establishment of the consensus ranking .Therefore , when they are considered , performance decreases .In this set of runs , we compare the outranking approach with some standard rank aggregation methods which wereproven to have acceptable performance in previous studies : we considered two positional methods which are the CombSUM and the CombMNZ strategies .We also examined the performance of one majoritarian method which is the Markov chain method MC4 .For the comparisons , we considered a specific outranking relation S \u2217 = S 5 % ,50 % ,50 % ,30 % which results in good overall performances when tuning all the parameters .The first row of Table 10 gives performances of the rank aggregation methods w.r.t. a basic assumption set A1 = H1100 , H25 , H4new : we only consider the 100 first documents from each ranking , then retain documents present in 5 or more rankings and update ranks of successful documents .For positional methods , we place missing documents at the queue of the ranking H3 yes whereas for our method as well as for MC4 , we retained hypothesis Hn3o .The three following rows of Table 10 report performances when changing one element from the basic assumption set : the second row corresponds to the assumption set A2 = H11000 , H25 , H4new , i.e. changing the number of retained documents from 100 to 1000 .The third row corresponds to the assumption set A3 = H1100 , H2 all , Hn4ew , i.e. considering the documents present in at least one ranking .The fourth row corresponds to the assumption set A4 = H1100 , H25 , H4init , i.e. keeping the original ranks of successful documents .The fifth row of Table 10 , labeled A5 , gives performance when all the 225 queries of the Web track of TREC-2004 are considered .Obviously , performance level can not be compared with previous lines since the additional queries are different from the TD queries and correspond to other tasks Home Page and Named Page tasks 10 of TREC-2004 Web track .This set of runs aims to show whether relative performance of the various methods is task-dependent .The last row of Table 10 , labeled A6 , reports performance of the various methods considering the TD task of TREC2002 instead of TREC-2004 : we fused the results of input rankings of the 10 best official runs for each of the 50 TD queries 9 considering the set of assumptions A1 of the first row .This aims to show whether relative performance of the various methods changes from year to year .Values between brackets of Table 10 are variations of performance of each rank aggregation method w.r.t. performance of the outranking approach .From the analysis of table 10 the following can be established : \u2022 for all the runs , considering all the documents in each input ranking A2 significantly improves performance MAP increases by 11.62 % on average .This is predictable since some initially unreported relevant documents would receive better positions in the consensus ranking .proves performance .For mcm , combSUM and combMNZ , performance improvement is more important MAP increases by 20.27 % on average than for the markov run MAP increases by 3.86 % .2002 , performances of all the data fusion methods are lower than that of the best performing input ranking for which the MAP value equals 18.58 % .This is because most of the fused input rankings have very low performances compared to the best one , which brings more noise to the consensus ranking .ter than positional methods combSUM and combMNZ .It has also better performances than the Markov chain method , especially under assumption H2 all where difference of performances becomes significant .", "combined_keywords": ["rank aggregation", "information retrieval", "datum fusion problem", "multiple criterion framework", "decision rule", "outranking approach", "metasearch engine", "combsum and combmnz strategy", "majoritarian method", "ir model", "datum fusion", "multiple criterium approach", "outrank method"], "author_keywords": ["datum fusion", "metasearch engine", "multiple criterium approach", "outrank method", "rank aggregation"], "method": "The exact positions of documents in one input ranking have limited significance and should not be overemphasized .For instance , having three relevant documents in the first three positions , any perturbation of these three items will have the same value .Indeed , in the IR context , the complete order provided by an input method may hide ties .In this case , we call such rankings semi orders .This was outlined in 13 as the problem of aggregation with ties .It is therefore important to build the consensus ranking based on robust information :In real world applications , such as metasearch engines , rankings provided by the input methods are often partial lists .This was outlined in 14 as the problem of having to merge top-k results from various input lists .For instance , in the experiments carried out by Dwork et al. 11 , authors found that among the top 100 best documents of 7 input search engines , 67 % of the documents were present in only one search engine , whereas less than two documents were present in all the search engines .Rank aggregation of partial lists raises four major difficulties which we state hereafter , proposing for each of them various working assumptions :Hereafter , we call documents which will be retained in the consensus ranking , candidate documents , and documents that will be excluded from the consensus ranking , excluded documents .We also call a candidate document which is missing in one or more rankings , a missing document .ument are that it was not indexed or it was indexed but deemed irrelevant ; usually this information is not available .We consider the following two working hypotheses : H3 yes : Each missing document in each > j is assigned a position .H3no : No assumption is made , that is each missing document is considered neither better nor worse than any other document .4 .When assumption H2k holds , each input ranking may contain documents which will not be considered in the consensus ranking .Regarding the positions of the candidate documents , we can consider the following working hypotheses : H4init : The initial positions of candidate documents are kept in each input ranking .H4 new : Candidate documents receive new positions in each input ranking , after discarding excluded ones .In the IR context , rank aggregation methods need to decide more or less explicitly which assumptions to retain w.r.t. the above-mentioned difficulties .Positional methods consider implicitly that the positions of the documents in the input rankings are scores giving thus a cardinal meaning to an ordinal information .This constitutes a strong assumption that is questionable , especially when the input rankings have different lengths .Moreover , for positional methods , assumptions H3 and H4 , which are often arbitrary , have a strong impact on the results .For instance , let us consider an input ranking of 500 documents out of 1000 candidate documents .Whether we assign to each of the missing documents the position 1 , 501 , 750 or 1000 corresponding to variations of H3 yes will give rise to very contrasted results , especially regarding the top of the consensus ranking .Majoritarian methods do not suffer from the above-mentioned drawbacks of the positional methods since they build consensus rankings exploiting only ordinal information contained in the input rankings .Nevertheless , they suppose that such rankings are complete orders , ignoring that they may hide ties .Therefore , majoritarian methods base consensus rankings on illusory discriminant information rather than less discriminant but more robust information .Trying to overcome the limits of current rank aggregation methods , we found that outranking approaches , which were initially used for multiple criteria aggregation problems 26 , can also be used for the rank aggregation purpose , where each ranking plays the role of a criterion .Therefore , in order to decide whether a document di should be ranked better than di , in the consensus ranking \u03c3 , the two following conditions should be met :Formally , the concordance coalition with diadi ' is Csp diadi ' = ~ j \u2208 PR : rji \u2264 rji ' \u2212 sp where sp is a preference threshold which is the variation of document positions whether it is absolute or relative to the ranking length which draws the boundaries between an indifference and a preference situation between documents .The discordance coalition with diadi ' is Dsv diadi ' = ~ j \u2208 PR : rji \u2265 rji ' + sv where sv is a veto threshold which is the variation of document positions whether it is absolute or relative to the ranking length which draws the boundaries between a weak and a strong opposition to diadi ' .Depending on the exact definition of the preceding concordance and discordance coalitions leading to the definition of some decision rules , several outranking relations can be defined .They can be more or less demanding depending on i the values of the thresholds sp and sv , ii the importance or minimal size cmin required for the concordance coalition , and iii the importance or maximum size dmax of the discordance coalition .A generic outranking relation can thus be defined as follows :This expression defines a family of nested outranking relations since S sp , sv , cmin , dmax \u2286 S s ' p , s ' v , c 'm in , d'max when cmin \u2265 c ~ min and/or dmax \u2264 d ~ max and/or sp \u2265 s ~ p and/or sv \u2264 s ~ v .This expression also generalizes the majority rule which corresponds to the particular relation S 0 , \u221e , n 2 , n .It also satisfies important properties of rank aggregation methods , called neutrality , Pareto-optimality , Condorcet property and Extended Condorcet property , in the social choice literature 29 .Outranking relations are not necessarily transitive and do not necessarily correspond to rankings since directed cycles may exist .Therefore , we need specific procedures in order to derive a consensus ranking .We propose the following procedure which finds its roots in 27 .It consists in partitioning the set of documents into r ranked classes .Each class Ch contains documents with the same relevance and results from the application of all relations if possible to the set of documents remaining after previous classes are computed .Documents within the same equivalence class are ranked arbitrarily .Formally , letEach class Ch results from a distillation process .It corresponds to the last distillate of a series of sets E0 \u2287 E1 \u2287 ... where E0 = R \\ C1 \u222a ... \u222a Ch \u2212 1 and Ek is a reduced subset of Ek \u2212 1 resulting from the application of the following procedure :When one outranking relation is used , the distillation process stops after the first application of the previous procedure , i.e. , Ch corresponds to distillate E1 .When different outranking relations are used , the distillation process stops when all the pre-defined outranking relations have been used or when | Ek | = 1 .This section illustrates the concepts and procedures of section 4.1 .Let us consider a set of candidate documentsLet us suppose that the preference and veto thresholds are set to values 1 and 4 respectively , and that the concordance and discordance thresholds are set to values 2 and 1 respectively .The following tables give the concordance , discordance and outranking matrices .Each entry csp di , di ' dsv di , di ' in the concordance discordance matrix gives the number of rankings that are concordant discordant with diadi ' , i.e. csp di , di ' = | Csp diadi ' | and dsv di , di ' = | Dsv diadi ' | .For instance , the concordance coalition for the assertion d1ad4 is C1 d1ad4 = ~ 1 , ~ 2 , ~ 3 and the discordance coalition for the same assertion is D4 d1ad4 = \u2205 .Therefore , c1 d1 , d4 = 3 , d4 d1 , d4 = 0 and d1S1d4 holds .Notice that Fk di , R fk di , R is given by summing the values of the ith row column of the outranking matrix .Theconsensus ranking is obtained as follows : to get the first class C1 , we compute the qualifications of all the documents of E0 = R with respect to S1 .They are respectively 2 , 2 , 2 , -2 and -4 .Therefore smax equals 2 and C1 = E1 = d1 , d2 , d3 .Observe that , if we had used a second outranking relation S2 \u2287 S1 , these three documents could have been possibly discriminated .At this stage , we remove documents of C1 from the outranking matrix and compute the next class C2 : we compute the new qualifications of the documents of E0 = R \\ C1 = d4 , d5 .They are respectively 1 and -1 .So C3 = E1 = d4 .The last document d5 is the only document of the last class C3 .Thus , the consensus ranking is d1 , d2 , d3 d4 d5 .", "conclusions": "In this paper , we address the rank aggregation problem where different , but not disjoint , lists of documents are to be fused .We noticed that the input rankings can hide ties , so they should not be considered as complete orders .Only robust information should be used from each input ranking .Current rank aggregation methods , and especially positional methods e.g. combSUM 15 , are not initially designed to work with such rankings .They should be adapted by considering specific working assumptions .We propose a new outranking method for rank aggregation which is well adapted to the IR context .Indeed , it ranks two documents w.r.t. the intensity of their positions difference in each input ranking and also considering the number of the input rankings that are concordant and discordant in favor of a specific document .There is also no need to make specific assumptions on the positions of the missing documents .This is an important feature since the absence of a document from a ranking should not be necessarily interpreted negatively .Experimental results show that the outranking method significantly out-performs popular classical positional data fusion methods like combSUM and combMNZ strategies .It also out-performs a good performing majoritarian methods which is the Markov chain method .These results are tested against different test collections and queries .From the experiments , we can also conclude that in order to improve the performances , we should fuse result lists of well performingIR models , and that majoritarian data fusion methods perform better than positional methods .The proposed method can have a real impact on Web metasearch performances since only ranks are available from most primary search engines , whereas most of the current approaches need scores to merge result lists into one single list .Further work involves investigating whether the outranking approach performs well in various other contexts , e.g. using the document scores or some combination of document ranks and scores .", "related work": "As pointed out by Riker 25 , we can distinguish two families of rank aggregation methods : positional methods which assign scores to items to be ranked according to the ranksthey receive and majoritarian methods which are based on pairwise comparisons of items to be ranked .These two families of methods find their roots in the pioneering works of Borda 5 and Condorcet 7 , respectively , in the social choice literature .We first introduce some basic notations to present the rank aggregation methods in a uniform way .Let D = d1 , d2 , ... , dnd be a set of nd documents .A list or a ranking ~ j is an ordering defined on Dj \u2286 D j = 1 , ... , n .Thus , di ~ j di , means di ` is ranked better than ' di , in ~ j .When Dj = D , ~ j is said to be a full list .Otherwise , it is a partial list .If di belongs to Dj , rji denotes the rank or position of di in ~ j .We assume that the best answer document is assigned the position 1 and the worst one is assigned the position | Dj | .Let ~ D be the set of all permutations on D or all subsets of D .A profile is a n-tuple of rankings PR = ~ 1 , ~ 2 , ... , ~ n .Restricting PR to the rankings containing document di defines PRi .We also call the number of rankings which contain document di the rank hits of di 19 .The rank aggregation or data fusion problem consists of finding a ranking function or mechanism \u03a8 also called a social welfare function in the social choice theory terminology defined by :where \u03c3 is called a consensus ranking .This method 5 first assigns a score Enj = 1 rji to each document di .Documents are then ranked by increasing order of this score , breaking ties , if any , arbitrarily .This family of methods basically combine scores of documents .When used for the rank aggregation problem , ranks are assumed to be scores or performances to be combined using aggregation operators such as the weighted sum or some variation of it 3 , 31 , 17 , 28 .For instance , Callan et al. 6 used the inference networks model 30 to combine rankings .Fox and Shaw 15 proposed several combination strategies which are CombSUM , CombMIN , CombMAX , CombANZ and CombMNZ .The first three operators correspond to the sum , min and max operators , respectively .CombANZ and CombMNZ respectively divides and multiplies the CombSUM score by the rank hits .It is shown in 19 that the CombSUM and CombMNZ operators perform better than the others .Metasearch engines such as SavvySearch and MetaCrawler use the CombSUM strategy to fuse rankings .In this method , a consensus ranking minimizes the Spearman footrule distance from the input rankings 21 .Formally , given two full lists ~ j and ~ j , , this distance is given by F ~ j , ~ j , = ndas follows .Given a profile PR and a consensus ranking \u03c3 , the Spearman footrule distance of \u03c3 to PR is given byi , i , | , where rji , i , = rji , \u2212 rji .This formulation has the advantage that it considers the intensity of preferences .This kind of methods assume that the performance of the input methods on a number of training queries is indicative of their future performance .During the training process , probabilities of relevance are calculated .For subsequent queries , documents are ranked based on these probabilities .For instance , in 20 , each input ranking ~ j is divided into a number of segments , and the conditional probability of relevance R of each document di depending on the segment k it occurs in , is computed , i.e. prob R | di , k , ~ j .For subsequent queries , the score of each document di is given by En prob R | di , k , Yj .Le Calve and Savoy 18 suggest using j = 1 k a logistic regression approach for combining scores .Training data is needed to infer the model parameters .The original Condorcet rule 7 specifies that a winner of the election is any item that beats or ties with every other item in a pairwise contest .Formally , let C di\u03c3di , = ~ j \u2208 PR : di ~ j di , be the coalition of rankings that are concordant with establishing di\u03c3di , , i.e. with the proposition di ` should be ranked better than ' di , in the final ranking \u03c3 .di beats or ties with di , iff | C di\u03c3di , | \u2265 | C di , \u03c3di | .The repetitive application of the Condorcet algorithm can produce a ranking of items in a natural way : select the Condorcet winner , remove it from the lists , and repeat the previous two steps until there are no more documents to rank .Since there is not always Condorcet winners , variations of the Condorcet procedure have been developed within the multiple criteria decision aid theory , with methods such as ELECTRE 26 .As in section 2.2.3 , a consensus ranking minimizes a geometric distance from the input rankings , where the Kendall tau distance is used instead of the Spearman footrule distance .Formally , given two full lists ~ j and ~ j , , the Kendall tau distance is given by K ~ j , ~ j , = | di , di , : i < i ~ , rji < rji , , rj ,i , | , i.e. the number of pairwise disagreements between the two lists .It is easy to show that the consensus ranking corresponds to the geometric median of the input rankings and that the Kemeny optimal aggregation problem corresponds to the minimum feedback edge set problem .Markov chains MCs have been used by Dwork et al. 11 as a ` natural ' method to obtain a consensus ranking where states correspond to the documents to be ranked and the transition probabilities vary depending on the interpretation of the transition event .In the same reference , the authors proposed four specific MCs and experimental testing had shown that the following MC is the best performing one see also 24 :The consensus ranking corresponds to the stationary distribution of MC4 ."}