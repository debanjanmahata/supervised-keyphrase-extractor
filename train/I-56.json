{"reader_keywords": ["dcsp", "constraint", "shared environment", "algorithm", "backtracking", "mediation", "resource restriction", "privacy requirement", "negotiation", "bdi", "uma"], "reader_keywords_stem": ["dcsp", "constraint", "share environ", "algorithm", "backtrack", "mediat", "resourc restrict", "privaci requir", "negoti", "bdi", "uma"], "introduction": "At the core of many emerging distributed applications is the distributed constraint satisfaction problem DCSP one which involves finding a consistent combination of actions abstracted as domain values to satisfy the constraints among multiple agents in a shared environment .Important application examples includedistributed resource allocation 1 and distributed scheduling 2 .Many important algorithms , such as distributed breakout DBO 3 , asynchronous backtracking ABT 4 , asynchronous partial overlay APO 5 and asynchronous weak-commitment AWC 4 , have been developed to address the DCSP and provide the agent solution basis for its applications .Broadly speaking , these algorithms are based on two different approaches , either extending from classical backtracking algorithms 6 or introducing mediation among the agents .While there has been no lack of efforts in this promising research field , especially in dealing with outstanding issues such as resource restrictions e.g. , limits on time and communication 7 and privacy requirements 8 , there is unfortunately no conceptually clear treatment to prise open the model-theoretic workings of the various agent algorithms that have been developed .As a result , for instance , a deeper intellectual understanding on why one algorithm is better than the other , beyond computational issues , is not possible .In this paper , we present a novel , unified distributed constraint satisfaction framework based on automated negotiation 9 .Negotiation is viewed as a process of several agents searching for a solution called an agreement .The search can be realized via a negotiation mechanism or algorithm by which the agents follow a high level protocol prescribing the rules of interactions , using a set of strategies devised to select their own preferences at each negotiation step .Anchoring the DCSP search on automated negotiation , we show in this paper that several well-known DCSP algorithms 3 are actually mechanisms that share the same Belief-DesireIntention BDI interaction protocol to reach agreements , but use different action or value selection strategies .The proposed framework provides not only a clearer understanding of existing DCSP algorithms from a unified BDI agent perspective , but also opens up the opportunities to extend and develop new strategies for DCSP .To this end , a new strategy called Unsolicited Mutual Advice UMA is proposed .Our performance evaluation shows that UMA can outperform ABT and AWC in terms of the average number of computational cycles for both the sparse and critical coloring problems 6 .The rest of this paper is organized as follows .In Section 2 , we provide a formal overview of DCSP .Section 3 presents a BDI negotiation model by which a DCSP agent reasons .Section 4 presents the existing algorithms ABT , AWC and DBO as different strategies formalized on a common protocol .A new strategy called Unsolicited Mutual Advice is proposed in Section 5 ; our empirical results and discussion attempt to highlight the merits of the new strategy over existing ones .Section 6 concludes the paper and points to some future work .", "title": "Unifying Distributed Constraint Algorithms in a BDI Negotiation Framework", "author_keywords_stem": ["distribute constraint satisfaction problem", "belief-desireintention model", "agent negotiation"], "abstract": "This paper presents a novel , unified distributed constraint satisfaction framework based on automated negotiation .The Distributed Constraint Satisfaction Problem DCSP is one that entails several agents to search for an agreement , which is a consistent combination of actions that satisfies their mutual constraints in a shared environment .By anchoring the DCSP search on automated negotiation , we show that several well-known DCSP algorithms are actually mechanisms that can reach agreements through a common Belief-Desire-Intention BDI protocol , but using different strategies .A major motivation for this BDI framework is that it not only provides a conceptually clearer understanding of existing DCSP algorithms from an agent model perspective , but also opens up the opportunities to extend and develop new strategies for DCSP .To this end , a new strategy called Unsolicited Mutual Advice UMA is proposed .Performance evaluation shows that the UMA strategy can outperform some existing mechanisms in terms of computational cycles .", "id": "I-56", "combined_keywords_stem": ["dcsp", "constraint", "share environ", "algorithm", "backtrack", "mediat", "resourc restrict", "privaci requir", "negoti", "bdi", "uma", "distribut constraint satisfact problem", "belief-desireintent model", "agent negoti"], "combined_keywords": ["dcsp", "constraint", "shared environment", "algorithm", "backtracking", "mediation", "resource restriction", "privacy requirement", "negotiation", "bdi", "uma", "distribute constraint satisfaction problem", "belief-desireintention model", "agent negotiation"], "author_keywords": ["distribute constraint satisfaction problem", "belief-desireintention model", "agent negotiation"], "method": "The DCSP 4 considers the following environment .k , which have values in domains D1 , D2 , \u00b7 \u00b7 \u00b7 , Dk , respectively .We define a partial function B over the productrange 10 , 1 , ... , n \u2212 1 x 10 , 1 , ... , k \u2212 1 such that , thatvariable xj belongs to agent i is denoted by B i , j !.The exclamation mark ` ! 'means ` is defined ' .The DCSP may be formally stated as follows .m where E l , j !, cl is satisfied .A constraint may consist of different variables belonging to different agents .An agent can not change or modify the assignment values of other agents ' variables .Therefore , in cooperatively searching for a DCSP solution , the agents would need to communicate with one another , and adjust and re-adjust their own variable assignments in the process .In general , all DCSP agents must cooperatively interact , and essentially perform the assignment and reassignment of domain values to variables to resolve all constraint violations .If the agents succeed in their resolution , a solution is found .In order to engage in cooperative behavior , a DCSP agent needs five fundamental parameters , namely , i a variable 4 or a variable set 10 , ii domains , iii priority , iv a neighbor list and v a constraint list .Each variable assumes a range of values called a domain .A domain value , which usually abstracts an action , is a possible option that an agent may take .Each agent has an assigned priority .These priority values help decide the order in which they revise or modify their variable assignments .An agent 's priority may be fixed static or changing dynamic when searching for a solution .If an agent has more than one variable , each variable can be assigned a different priority , to help determine which variable assignment the agent should modify first .An agent which shares the same constraint with another agent is called the latter 's neighbor .Each agent needs to refer to its list of neighbors during the search process .This list may also be kept unchanged or updated accordingly in runtime .Similarly , each agent maintains a constraint list .The agent needs to ensure that there is no violation of the constraints in this list .Constraints can be added or removed from an agent 's constraint list in runtime .As with an agent , a constraint can also be associated with a priority value .Constraints with a high priority are said to be more important than constraints with a lower priority .To distinguish it from the priority of an agent , the priority of a constraint is called its weight .The BDI model originates with the work of M. Bratman 11 .According to 12 , Ch .1 , the BDI architecture is based on a philosophical model of human practical reasoning , and draws out the process of reasoning by which an agent decides which actions to perform at consecutive moments when pursuing certain goals .Grounding the scope to the DCSP framework , the common goal of all agents is finding a combination of domain values to satisfy a set of predefined constraints .In automated negotiation 9 , such a solution is called an agreement among the agents .Within this scope , we found that we were able to unearth the generic behavior of a DCSP agent and formulate it in a negotiation protocol , prescribed using the powerful concepts of BDI .Thus , our proposed negotiation model can be said to combine the BDI concepts with automated negotiation in a multiagent framework , allowing us to conceptually separate DCSP mechanisms into a common BDI interaction protocol and the adopted strategies .component or transition which may or may not appear depending on the adopted strategy .Two types of messages are exchanged through this protocol , namely , the info message and the negotiation message .An info message perceived is a message sent by another agent .The message will contain the current selected values and priorities of the variables of that sending agent .The main purpose of this message is to update the agent about the current environment .Info message is sent out at the end of one negotiation round also called a negotiation cycle , and received at the beginning of next round .A negotiation message is a message which may be sent within a round .This message is for mediation purposes .The agent may put different contents into this type of message as long as it is agreed among the group .The format of the negotiation message and when it is to be sent out are subject to the strategy .A negotiation message can be sent out at the end of one reasoning step and received at the beginning of the next step .Mediation is a step of the protocol that depends on whether the agent 's interaction with others is synchronous or asynchronous .In synchronous mechanism , mediation is required in every negotiation round .In an asynchronous one , mediation is needed only in a negotiation round when the agent receives a negotiation message .A more in-depth view of this mediation step is provided later in this section .The BDI protocol prescribes the skeletal structure for DCSP negotiation .We will show in Section 4 that several well-known DCSP mechanisms all inherit this generic model .The details of the six main reasoning steps for the protocol see Figure 1 are described as follows for a DCSP agent .For a conceptually clearer description , we assume that there is only one variable per agent .ment is found .In case the agent finds its value in conflict with some of its neighbors ' , i.e. , the combination of values assigned to the variables leads to a constraint violation , the agent will first try to reassign its own variable using a specific strategy .If it finds a suitable option which meets some criteria of the adopted strategy , the agent will believe it should change to the new option .However it does not always happen that an agent can successfully find such an option .If no option can be found , the agent will believe it has no option , and therefore will request its neighbors to reconsider their variable assignments .To summarize , there are three types of beliefs that a DCSP agent can form : i it can change its variable assignment to improve the current situation , ii it can not change its variable assignment and some constraints violations can not be resolved and iii it need not change its variable assignment as all the constraints are satisfied .Once the beliefs are formed , the agent will determine its desires , which are the options that attempt to resolve the current constraint violations .Since , if the agent executes its intention without performing intention mediation with its neighbors , the constraint violation between the agents may not be resolved .Take for example , suppose two agents have variables , x1 and x2 , associated with the same domain 1 , 2 , and their shared constraint is x1 + x2 = 3 .Then if both the variables are initialized with value 1 , they will both concurrently switch between the values 2 and 1 in the absence of mediation between them .There are two types of mediation : local mediation and group mediation .In the former , the agents exchange their intentions .When an agent receives another 's intention which conflicts with its own , the agent must mediate between the intentions , by either changing its own intention or informing the other agent to change its intention .In the latter , there is an agent which acts as a group mediator .This mediator will collect the intentions from the group a union of the agent and its neighbors and determine which intention is to be executed .The result of this mediation is passed back to the agents in the group .Following mediation , the agent may proceed to the next reasoning step to execute its intention or begin a new negotiation round .\u2022 Execution .This is the last step of a negotiation round .The agent will execute by updating its variable assignment if the intention obtained at this step is its own .Following execution , the agent will inform its neighbors about its new variable assignment and updated priority .To do so , the agent will send out an info message .A strategy plays an important role in the negotiation process .Within the protocol , it will often determine the efficiency of thesearch process in terms of computational cycles and message communication costs .The design space when devising a strategy is influenced by the following dimensions : i asynchronous or synchronous , ii dynamic or static priority , iii dynamic or static constraint weight , iv number of negotiation messages to be communicated , v the negotiation message format and vi the completeness property .In other words , these dimensions provide technical considerations for a strategy design .In this section , we apply the proposed BDI negotiation model presented in Section 3 to expose the BDI protocol and the different strategies used for three well-known algorithms , ABT , AWC and DBO .All these algorithms assume that there is only one variable per agent .Under our framework , we call the strategies applied the ABT , AWC and DBO strategies , respectively .To describe each strategy formally , the following mathematical notations are used :Figure 2 presents the BDI negotiation model incorporating the Asynchronous Backtracking ABT strategy .As mentioned in Section 3 , for an asynchronous mechanism that ABT is , the mediation step is needed only in a negotiation round when an agent receives a negotiation message .For agent i , beginning initially with wl = 1 , 0 < l < m ; pi = i , 0 < i < n and Fi contains all the agents who share the constraints with agent i , its BDI-driven ABT strategy is described as follows .Step 1 Percept : Update Pi upon receiving the info messages from the neighbors in Fi .Update Ci to be the list ofconstraints which only consists of agents in Fi that have equal or higher priority than this agent .Step 2 Belief : The belief function GB Pi , Ci will return a value bi E 0 , 1 , 2 , decided as follows :Step 3 Desire : The desire function GD bi will return a desire set denoted by DS , decided as follows :Following this step , agent i proceeds to the next negotiation round .Figure 3 presents the BDI negotiation model incorporating the Asynchronous Weak Commitment AWC strategy .The model is similar to that of incorporating the ABT strategy see Figure 2 .This is not surprising ; AWC and ABT are found to be strategically similar , differing only in the details of some reasoning steps .The distinguishing point of AWC is that when the agent can not find a suitable variable assignment , it will change its priority to the highest among its group members i U Fi .For agent i , beginning initially with wl = 1 , 0 < l < m ; pi = i , 0 < i < n and Fi contains all the agents who share the constraints with agent i , its BDI-driven AWC strategy is described as follows .Step 1 Percept : This step is identical to the Percept step of ABT .Step 2 Belief : The belief function GB Pi , Ci will return a value bi E 0 , 1 , 2 , decided as follows :Step 3 Desire : The desire function GD bi will return a desire set DS , decided as follows :Following , if bi = 1 , agent i will find a list Ki of higher priority neighbors , defined by Ki = k | agent k E Fi and pk > pi .Step 4 Intention : This step is similar to the Intention step of ABT .However , for this strategy , the negotiation message will contain the variable assignments of the current image Pi for all the agents in Ki .This list of assignment is considered as a nogood .If the same negotiation message had been sent out before , agent i will have nil intention .Otherwise , the agent will send the message and save the nogood in the nogood list .Step 5 Execution :Mediation : This step is identical to the Mediation step of ABT , except that agent i will now add the nogood contained in the negotiation message received to its own nogood list .Figure 4 presents the BDI negotiation model incorporating the Distributed Breakout DBO strategy .Essentially , by this synchronous strategy , each agent will search iteratively for improvement by reducing the total weight of the violated constraints .The iteration will continue until no agent can improve further , at which time if some constraints remain violated , the weights ofthese constraints will be increased by 1 to help ` breakout ' from a local minimum .For agent i , beginning initially with wl = 1 , 0 < l < m , pi = i , 0 < i < n and Fi contains all the agents who share the constraints with agent i , its BDI-driven DBO strategy is described as follows .Step 1 Percept : Update Pi upon receiving the info messages from the neighbors in Fi .Update Ci to be the list of its relevant constraints .Step 2 Belief : The belief function GB Pi , Ci will return a value bi \u2208 0 , 1 , 2 , decided as follows :Step 3 Desire : The desire function GD bi will return a desire set DS , decided as follows :Step 4 Intention : The intention function GI DS will return an intention , decided as follows :Following , agent i will send its intention to all its neighbors .In return , it will receive intentions from these agents before proceeding to Mediation step .Mediation : Agent i receives all the intentions from its neighbors .If it finds that the intention received from a neighbor agent j is associated with hmax j > hmax i , the agent will automatically cancel its current intention .Step 5 Execution :Unlike when using the strategies of the previous section , a DCSP agent using UMA will not only send out a negotiation message when concluding its Intention step , but also when concluding its Desire step .The negotiation message that it sends out to conclude the Desire step constitutes an unsolicited advice for all its neighbors .In turn , the agent will wait to receive unsolicited advices from all its neighbors , before proceeding on to determine its intention .For agent i , beginning initially with wl = 1 , 0 < l < m , pi = i , 0 < i < n and Fi contains all the agents who share the constraints with agent i , its BDI-driven UMA strategy is described as follows .Step 1 Percept : Update Pi upon receiving the info messages from the neighbors in Fi .Update Ci to be the list of constraints relevant to agent i. Step 2 Belief : The belief function GB Pi , Ci will return a value bi \u2208 0 , 1 , 2 , decided as follows :Step 3 Desire : The desire function GD bi will return a desire set DS , decided as follows :i in subsequent steps , and it definesthe maximal reduction in constraint violations .It is also referred to as an improvement .i to all its neighbors .This message is called a voluntary advice .If bi = 1 , agent i will send a negotiation message called change advice to the neighbors in Fi who share the violated constraints with agent i. Agent i receives advices from all its neighbors and stores them in a list called A , before proceeding to the next step .Step 4 Intention : The intention function GI DS , A will return an intention , decided as follows :Following , if the improvement hmaxi is the biggest improvement and equal to some improvements associated with the received voluntary advices , agent i will send its computed intention to all its neighbors .If agent i has a reluctant intention , it will also send this intention to all its neighbors .In both cases , agent i will attach the number of received change advices in the current negotiation round with its intention .In return , agent i will receive the intentions from its neighbors before proceeding to Mediation step .Mediation : If agent i does not send out its intention before this step , i.e. , the agent has either a nil intention or a voluntary intention with biggest improvement , it will proceed to next step .Otherwise , agent i will select the best intention among all the intentions received , including its own if any .The criteria to select the best intention are listed , applied in descending order of importance as follows .Step 5 Execution : If agent i does not cancel its intention , it will update its variable assignment with the intended value .Termination Condition : Since each agent does not have full information about the global state , it may not know when it has reached a solution , i.e. , when all the agents are in a global stable state .Hence an observer is needed that will keep track of the negotiation messages communicated in the environment .Following a certain period of time when there is no more message communication and this happens when all the agents have no more intention to update their variable assignments , the observer will inform the agents in the environment that a solution has been found .To illustrate how UMA works , consider a 2-color graph problem 6 as shown in Figure 6 .In this example , each agent has a color variable representing a node .There are 10 color variables sharing the same domain Black , White .The following records the outcome of each step in every negotiation round executed .Step 1 Percept : Each agent obtains the current color assignments of those nodes agents adjacent to it , i.e. , its neighbors ' .Step 2 Belief : Agents which have positive improvements are agent 1 this agent believes it should change its color to White , agent 2 this believes should change its color to White , agent 7 this agent believes it should change its color to Black and agent 10 this agent believes it should change its value to Black .In this negotiation round , the improvements achieved by these agents are 1 .Agents which do not have any improvements are agents 4 , 5 and 8 .Agents 3 , 6 and 9 need not change as all their relevant constraints are satisfied .Step 3 Desire : Agents 1 , 2 , 7 and 10 have the voluntary desire White color for agents 1 , 2 and Black color for agents 7 , 10 .These agents will send the voluntary advices to all their neighbors .Meanwhile , agents 4 , 5 and 8 have the reluctant desires White color for agent 4 and Black color for agents 5 , 8 .Agent 4 will send a change advice to agent 2 as agent 2 is sharing the violated constraint with it .Similarly , agents 5 and 8 will send change advices to agents 7 and 10 respectively .Agents 3 , 6 and 9 do not have any desire to update their color assignments .Step 4 Intention : Agents 2 , 7 and 10 receive the change advices from agents 4 , 5 and 8 , respectively .They form their voluntary intentions .Agents 4 , 5 and 8 receive the voluntary advices from agents 2 , 7 and 10 , hence they will not have any intention .Agents 3 , 6 and 9 do not have any intention .Following , the intention from the agents will be sent to all their neighbors .Mediation : Agent 1 finds that the intention from agent 2 is better than its intention .This is because , although both agents have voluntary intentions with improvement of 1 , agent 2 has received one change advice from agent 4 while agent 1 has not received any .Hence agent 1 cancels its intention .Agent 2 will keep its intention .Agents 7 and 10 keep their intentions since none of their neighbors has an intention .The rest of the agents do nothing in this step as they do not have any intention .Step 5 Execution : Agent 2 changes its color to White .Agents 7 and 10 change their colors to Black .The new state after round 1 is shown in Figure 7 .Step 1 Percept : The agents obtain the current color assignments of their neighbors .Step 2 Belief : Agent 3 is the only agent who has a positive improvement which is 1 .It believes it should change itscolor to Black .Agent 2 does not have any positive improvement .The rest of the agents need not make any change as all their relevant constraints are satisfied .They will have no desire , and hence no intention .Step 3 Desire : Agent 3 desires to change its color to Black voluntarily , hence it sends out a voluntary advice to its neighbor , i.e. , agent 2 .Agent 2 does not have any value for its reluctant desire set as the only option , Black color , will bring agent 2 and its neighbors to the previous state which is known to be a bad state .Since agent 2 is sharing the constraint violation with agent 3 , it sends a change advice to agent 3 .Step 4 Intention : Agent 3 will have a voluntary intention while agent 2 will not have any intention as it receives the voluntary advice from agent 3 .Mediation : Agent 3 will keep its intention as its only neighbor , agent 2 , does not have any intention .Step 5 Execution : Agent 3 changes its color to Black .The new state after round 2 is shown in Figure 8 .Round 3 : In this round , every agent finds that it has no desire and hence no intention to revise its variable assignment .Following , with no more negotiation message communication in the environment , the observer will inform all the agents that a solution has been found .To facilitate credible comparisons with existing strategies , we measured the execution time in terms of computational cycles as defined in 4 , and built a simulator that could reproduce the published results for ABT and AWC .The definition of a computational cycle is as follows .Four benchmark problems 6 were considered , namely , n-queens and node coloring for sparse , dense and critical graphs .For each problem , a finite number of test cases were generated for various problem sizes n .The maximum execution time was set to10000 cycles for node coloring for critical graphs and 1000 cycles for other problems .The simulator program was terminated after this period and the algorithm was considered to fail a test case if it did not find a solution by then .In such a case , the execution time for the test was counted as 1000 cycles .The n-queens problem is a traditional problem of constraint satisfaction .10 test cases were generated for each problem size n \u2208 10 , 50 and 100 .Figure 9 shows the execution time for different problem sizes when ABT , AWC and UMA were run .The graph coloring problem can be characterized by three parameters : i the number of colors k , the number of nodes/agents n and the number of links m. Based on the ratio m/n , the problem can be classified into three types 3 : i sparse with m/n = 2 , ii critical with m/n = 2.7 or 4.7 and iii dense with m/n = n \u2212 1 / 4 .For this problem , we did not include ABT in our empirical results as its failure rate was found to be very high .This poor performance of ABT was expected since the graph coloring problem is more difficult than the n-queens problem , on which ABT already did not perform well see Figure 9 .The sparse and dense coloring problem types are relatively easy while the critical type is difficult to solve .In the experiments , we fix k = 3 .10 test cases were created using the method described in 13 for each value of n \u2208 60 , 90 , 120 , for each problem type .The simulation results for each type of problem are shown in Figures 10 12 .Figure 10 : Comparison between AWC and UMA sparse graph coloringFigure 10 shows that the average performance of UMA is slightly better than AWC for the sparse problem .UMA outperforms AWC in solving the critical problem as shown in Figure 11 .It was observed that the latter strategy failed in some test cases .However , as seen in Figure 12 , both the strategies are very efficient when solving the dense problem , with AWC showing slightly better performance .The performance of UMA , in the worst time complexity case , is similar to that of all evaluated strategies .The worst case occurs when all the possible global states of the search are reached .Since only a few agents have the right to change their variable assignments in a negotiation round , the number of redundant computational cycles and info messages is reduced .As we observe from the backtracking in ABT and AWC , the difference in the ordering of incoming messages can result in a different number of computational cycles to be executed by the agents .The computational performance of UMA is arguably better than DBO for the following reasons :of sending a message and awaiting a reply than DBO , which occurs due to the need to communicate unsolicited advices .Although this increases the communication cost per negotiation round , we observed from our simulations that the overall communication cost incurred by UMA is lower due to the significantly lower number of negotiation rounds .\u2022 Using UMA , in the worst case , an agent will only take 2 or 3 communication round trips per negotiation round , following which the agent or its neighbor will do a variable assignment update .Using DBO , this number of round trips is uncertain as each agent might have to increase the weights of the violated constraints until an agent has a positive improvement ; this could result in a infinite loop 3 .", "conclusions": "Applying automated negotiation to DCSP , this paper has proposed a protocol that prescribes the generic reasoning of a DCSP agent in a BDI architecture .Our work shows that several wellknown DCSP algorithms , namely ABT , AWC and DBO , can be described as mechanisms sharing the same proposed protocol , and only differ in the strategies employed for the reasoning steps per negotiation round as governed by the protocol .Importantly , this means that it might furnish a unified framework for DCSP that not only provides a clearer BDI agent-theoretic view of existing DCSP approaches , but also opens up the opportunities to enhance or develop new strategies .Towards the latter , we have proposed and formulated a new strategy the UMA strategy .Empirical results and our discussion suggest that UMA is superior to ABT , AWC and DBO in some specific aspects .It was observed from our simulations that UMA possesses the completeness property .Future work will attempt to formally establish this property , as well as formalize other existing DSCP algorithms as BDI negotiation mechanisms , including the recent endeavor that employs a group mediator 5 .The idea of DCSP agents using different strategies in the same environment will also be investigated ."}