{"reader_keywords": ["resource selection of distributed text information retrieval", "distributed text information retrieval resource selection", "federated search", "hidden web content", "resource representation", "resource selection", "retrieval and result merging", "database recommendation", "distributed document retrieval", "logistic transformation model", "semi-supervised learning", "unified utility maximization model"], "reader_keywords_stem": ["resourc select of distribut text inform retriev", "distribut text inform retriev resourc select", "feder search", "hidden web content", "resourc represent", "resourc select", "retriev and result merg", "databas recommend", "distribut document retriev", "logist transform model", "semi-supervis learn", "unifi util maxim model"], "introduction": "Conventional search engines such as Google or AltaVista use ad-hoc information retrieval solution by assuming all the searchable documents can be copied into a single centralized database for the purpose of indexing .Distributed information retrieval , also known as federated search 1,4,7,11,14,22 is different from ad-hoc information retrieval as it addresses the cases when documents can not be acquired and stored in a single database .For example , `` Hidden Web '' contents also called `` invisible '' or `` deep '' Web contents are information on the Webthat can not be accessed by the conventional search engines .Hidden web contents have been estimated to be 2-50 19 times larger than the contents that can be searched by conventional search engines .Therefore , it is very important to search this type of valuable information .The architecture of distributed search solution is highly influenced by different environmental characteristics .In a small local area network such as small company environments , the information providers may cooperate to provide corpus statistics or use the same type of search engines .Early distributed information retrieval research focused on this type of cooperative environments 1,8 .On the other side , in a wide area network such as very large corporate environments or on the Web there are many types of search engines and it is difficult to assume that all the information providers can cooperate as they are required .Even if they are willing to cooperate in these environments , it may be hard to enforce a single solution for all the information providers or to detect whether information sources provide the correct information as they are required .Many applications fall into the latter type of uncooperative environments such as the Mind project 16 which integrates non-cooperating digital libraries or the QProber system 9 which supports browsing and searching of uncooperative hidden Web databases .In this paper , we focus mainly on uncooperative environments that contain multiple types of independent search engines .There are three important sub-problems in distributed information retrieval .First , information about the contents of each individual database must be acquired resource representation 1,8,21 .Second , given a query , a set of resources must be selected to do the search resource selection 5,7,21 .Third , the results retrieved from all the selected resources have to be merged into a single final list before it can be presented to the end user retrieval and results merging 1,5,20,22 .Many types of solutions exist for distributed information retrieval .Invisible-web .net1 provides guided browsing of hidden Web databases by collecting the resource descriptions of these databases and building hierarchies of classes that group them by similar topics .A database recommendation system goes a step further than a browsing system like Invisible-web .net by recommending most relevant information sources to users ' queries .It is composed of the resource description and theresource selection components .This solution is useful when the users want to browse the selected databases by themselves instead of asking the system to retrieve relevant documents automatically .Distributed document retrieval is a more sophisticated task .It selects relevant information sources for users ' queries as the database recommendation system does .Furthermore , users ' queries are forwarded to the corresponding selected databases and the returned individual ranked lists are merged into a single list to present to the users .The goal of a database recommendation system is to select a small set of resources that contain as many relevant documents as possible , which we call a high-recall goal .On the other side , the effectiveness of distributed document retrieval is often measured by the Precision of the final merged document result list , which we call a high-precision goal .Prior research indicated that these two goals are related but not identical 4,21 .However , most previous solutions simply use effective resource selection algorithm of database recommendation system for distributed document retrieval system or solve the inconsistency with heuristic methods 1,4,21 .This paper presents a unified utility maximization framework to integrate the resource selection problem of both database recommendation and distributed document retrieval together by treating them as different optimization goals .First , a centralized sample database is built by randomly sampling a small amount of documents from each database with query-based sampling 1 ; database size statistics are also estimated 21 .A logistic transformation model is learned off line with a small amount of training queries to map the centralized document scores in the centralized sample database to the corresponding probabilities of relevance .Second , after a new query is submitted , the query can be used to search the centralized sample database which produces a score for each sampled document .The probability of relevance for each document in the centralized sample database can be estimated by applying the logistic model to each document 's score .Then , the probabilities of relevance of all the mostly unseen documents among the available databases can be estimated using the probabilities of relevance of the documents in the centralized sample database and the database size estimates .For the task of resource selection for a database recommendation system , the databases can be ranked by the expected number of relevant documents to meet the high-recall goal .For resource selection for a distributed document retrieval system , databases containing a small number of documents with large probabilities of relevance are favored over databases containing many documents with small probabilities of relevance .This selection criterion meets the high-precision goal of distributed document retrieval application .Furthermore , the Semi-supervised learning SSL 20,22 algorithm is applied to merge the returned documents into a final ranked list .The unified utility framework makes very few assumptions and works in uncooperative environments .Two key features make it a more solid model for distributed information retrieval : i It formalizes the resource selection problems of different applications as various utility functions , and optimizes the utility functions to achieve the optimal results accordingly ; and ii It shows an effective and efficient way to estimate the probabilities of relevance of all documents across databases .Specifically , the framework builds logistic models on the centralized sample database to transform centralized retrieval scores to the corresponding probabilities of relevance and uses the centralized sample database as the bridge between individual databases and the logistic model .The human effort relevance judgment required to train the single centralized logistic model does not scale with the number of databases .This is a large advantage over previous research , which required the amount of human effort to be linear with the number of databases 7,15 .The unified utility framework is not only more theoretically solid but also very effective .Empirical studies show the new model to be at least as accurate as the state-of-the-art algorithms in a variety of configurations .The next section discusses related work .Section 3 describes the new unified utility maximization model .Section 4 explains our experimental methodology .Sections 5 and 6 present our experimental results for resource selection and document retrieval .Section 7 concludes .", "title": "Unified Utility Maximization Framework for Resource Selection", "author_keywords_stem": ["distribute information retrieval", "resource selection"], "abstract": "This paper presents a unified utility framework for resource selection of distributed text information retrieval .This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases .With the estimated relevance information , resource selection can be made by explicitly optimizing the goals of different applications .Specifically , when used for database recommendation , the selection is optimized for the goal of highrecall include as many relevant documents as possible in the selected databases ; when used for distributed document retrieval , the selection targets the high-precision goal high precision in the final merged list of documents .This new model provides a more solid framework for distributed information retrieval .Empirical studies show that it is at least as effective as other state-of-the-art algorithms .", "id": "H-73", "combined_keywords_stem": ["resourc select of distribut text inform retriev", "distribut text inform retriev resourc select", "feder search", "hidden web content", "resourc represent", "resourc select", "retriev and result merg", "databas recommend", "distribut document retriev", "logist transform model", "semi-supervis learn", "unifi util maxim model", "distribut inform retriev"], "combined_keywords": ["resource selection of distributed text information retrieval", "distributed text information retrieval resource selection", "federated search", "hidden web content", "resource representation", "resource selection", "retrieval and result merging", "database recommendation", "distributed document retrieval", "logistic transformation model", "semi-supervised learning", "unified utility maximization model", "distribute information retrieval"], "author_keywords": ["distribute information retrieval", "resource selection"], "method": "There has been considerable research on all the sub-problems of distributed information retrieval .We survey the most related work in this section .The first problem of distributed information retrieval is resource representation .The STARTS protocol is one solution for acquiring resource descriptions in cooperative environments 8 .However , in uncooperative environments , even the databases are willing to share their information , it is not easy to judge whether the information they provide is accurate or not .Furthermore , it is not easy to coordinate the databases to provide resource representations that are compatible with each other .Thus , in uncooperative environments , one common choice is query-based sampling , which randomly generates and sends queries to individual search engines and retrieves some documents to build the descriptions .As the sampled documents are selected by random queries , query-based sampling is not easily fooled by any adversarial spammer that is interested to attract more traffic .Experiments have shown that rather accurate resource descriptions can be built by sending about 80 queries and downloading about 300 documents 1 .Many resource selection algorithms such as gGlOSS/vGlOSS 8 and CORI 1 have been proposed in the last decade .The CORI algorithm represents each database by its terms , the document frequencies and a small number of corpus statistics details in 1 .As prior research on different datasets has shown the CORI algorithm to be the most stable and effective of the three algorithms 1,17,18 , we use it as a baseline algorithm in this work .The relevant document distribution estimation ReDDE 21 resource selection algorithm is a recent algorithm that tries to estimate the distribution of relevant documents across the available databases and ranks the databases accordingly .Although the ReDDE algorithm has been shown to be effective , it relies on heuristic constants that are set empirically 21 .The last step of the document retrieval sub-problem is results merging , which is the process of transforming database-specificdocument scores into comparable database-independent document scores .The semi supervised learning SSL 20,22 result merging algorithm uses the documents acquired by querybased sampling as training data and linear regression to learn the database-specific , query-specific merging models .These linear models are used to convert the database-specific document scores into the approximated centralized document scores .The SSL algorithm has been shown to be effective 22 .It serves as an important component of our unified utility maximization framework Section 3 .In order to achieve accurate document retrieval results , many previous methods simply use resource selection algorithms that are effective of database recommendation system .But as pointed out above , a good resource selection algorithm optimized for high-recall may not work well for document retrieval , which targets the high-precision goal .This type of inconsistency has been observed in previous research 4,21 .The research in 21 tried to solve the problem with a heuristic method .The research most similar to what we propose here is the decision-theoretic framework DTF 7,15 .This framework computes a selection that minimizes the overall costs e.g. , retrieval quality , time of document retrieval system and several methods 15 have been proposed to estimate the retrieval quality .However , two points distinguish our research from the DTF model .First , the DTF is a framework designed specifically for document retrieval , but our new model integrates two distinct applications with different requirements database recommendation and distributed document retrieval into the same unified framework .Second , the DTF builds a model for each database to calculate the probabilities of relevance .This requires human relevance judgments for the results retrieved from each database .In contrast , our approach only builds one logistic model for the centralized sample database .The centralized sample database can serve as a bridge to connect the individual databases with the centralized logistic model , thus the probabilities of relevance of documents in different databases can be estimated .This strategy can save large amount of human judgment effort and is a big advantage of the unified utility maximization framework over the DTF especially when there are a large number of databases .The Unified Utility Maximization UUM framework is based on estimating the probabilities of relevance of the mostly unseen documents available in the distributed search environment .In this section we describe how the probabilities of relevance are estimated and how they are used by the Unified Utility Maximization model .We also describe how the model can be optimized for the high-recall goal of a database recommendation system and the high-precision goal of a distributed document retrieval system .As pointed out above , the purpose of resource selection is highrecall and the purpose of document retrieval is high-precision .In order to meet these diverse goals , the key issue is to estimate the probabilities of relevance of the documents in various databases .This is a difficult problem because we can only observe a sample of the contents of each database using query-based sampling .Our strategy is to make full use of all the available information to calculate the probability estimates .In the resource description step , the centralized sample database is built by query-based sampling and the database sizes are estimated using the sample-resample method 21 .At the same time , an effective retrieval algorithm Inquery 2 is applied on the centralized sample database with a small number e.g. , 50 of training queries .For each training query , the CORI resource selection algorithm 1 is applied to select some number e.g. , 10 of databases and retrieve 50 document ids from each database .The SSL results merging algorithm 20,22 is used to merge the results .Then , we can download the top 50 documents in the final merged list and calculate their corresponding centralized scores using Inquery and the corpus statistics of the centralized sample database .The centralized scores are further normalized divided by the maximum centralized score for each query , as this method has been suggested to improve estimation accuracy in previous research 15 .Human judgment is acquired for those documents and a logistic model is built to transform the normalized centralized document scores to probabilities of relevance as follows :is the normalized centralized document score and ac and bc are the two parameters of the logistic model .These two parameters are estimated by maximizing the probabilities of relevance of the training queries .The logistic model provides us the tool to calculate the probabilities of relevance from centralized document scores .When the user submits a new query , the centralized document scores of the documents in the centralized sample database are calculated .However , in order to calculate the probabilities of relevance , we need to estimate centralized document scores for all documents across the databases instead of only the sampled documents .This goal is accomplished using : the centralized scores of the documents in the centralized sample database , and the database size statistics .We define the database scale factor for the ith database as the ratio of the estimated database size and the number of documents sampled from this database as follows :where Ndbi is the estimated database size and Ndbi _ samp is the number of documents from the ith database in the centralized sample database .The intuition behind the database scale factor is that , for a database whose scale factor is 50 , if one document from this database in the centralized sample database has a centralized document score of 0.5 , we may guess that there are about 50 documents in that database which have scores of about 0.5 .Actually , we can apply a finer non-parametric linear interpolation method to estimate the centralized document score curve for each database .Formally , we rank all the sampled documents from the ith database by their centralized documentscores to get the sampled centralized document score list Sc dsi1 , Sc dsi2 , Sc dsi3 , ... for the ith database ; we assume that if we could calculate the centralized document scores for all the documents in this database and get the complete centralized document score list , the top document in the sampled list would have rank SFdbi/2 , the second document in the sampled list would rank SFdbi3/2 , and so on .Therefore , the data points of sampled documents in the complete list are : SFdbi/2 , Sc dsi1 , SFdbi3/2 , Sc dsi2 , SFdbi5/2 , Sc dsi3 , ... .Piecewise linear interpolation is applied to estimate the centralized document score curve , as illustrated in Figure 1 .The complete centralized document score list can be estimated by calculating the values of different ranks on the centralized document curve as :It can be seen from Figure 1 that more sample data points produce more accurate estimates of the centralized document score curves .However , for databases with large database scale ratios , this kind of linear interpolation may be rather inaccurate , especially for the top ranked e.g. , 1 , SFdbi/2 documents .Therefore , an alternative solution is proposed to estimate the centralized document scores of the top ranked documents for databases with large scale ratios e.g. , larger than 100 .Specifically , a logistic model is built for each of these databases .The logistic model is used to estimate the centralized document score of the top 1 document in the corresponding database by using the two sampled documents from that database with highest centralized scores .\u03b1i0 , \u03b1i1 and \u03b1i2 are the parameters of the logistic model .For each training query , the top retrieved document of each database is downloaded and the corresponding centralized document score is calculated .Together with the scores of the top two sampled documents , these parameters can be estimated .After the centralized score of the top document is estimated , an exponential function is fitted for the top part 1 , SFdbi/2 of the centralized document score curve as :The two parameters \u03b2i0 and \u03b2i1 are fitted to make sure the ^ exponential function passes through the two points 1 , Sc di1 and SFdbi/2 , Sc dsi1 .The exponential function is only used to adjust the top part of the centralized document score curve and the lower part of the curve is still fitted with the linear interpolation method described above .The adjustment by fitting exponential function of the top ranked documents has been shown empirically to produce more accurate results .From the centralized document score curves , we can estimate the complete centralized document score lists accordingly for all the available databases .After the estimated centralized document scores are normalized , the complete lists of probabilities of relevance can be constructed out of the complete centralized document score lists by Equation 1 .Formally for the ith database , the complete list of probabilities of relevance is :In this section , we formally define the new unified utility maximization model , which optimizes the resource selection problems for two goals of high-recall database recommendation and high-precision distributed document retrieval in the same framework .In the task of database recommendation , the system needs to decide how to rank databases .In the task of document retrieval , the system not only needs to select the databases but also needs to decide how many documents to retrieve from each selected database .We generalize the database recommendation selection process , which implicitly recommends all documents in every selected database , as a special case of the selection decision for the document retrieval task Formally , we denote di as the number of documents we would like to retrieve from the ith database and d = d1 , d2 , as a selection action for all the databases .The database selection decision is made based on the complete lists of probabilities of relevance for all the databases .The complete lists of probabilities of relevance are inferred from all the available information specifically Rs , which stands for the resource descriptions acquired by query-based sampling and the database size estimates acquired by sample-resample ; Sc stands for the centralized document scores of the documents in the centralized sample database .If the method of estimating centralized document scores and probabilities of relevance in Section 3.1 is acceptable , then the most probable complete lists of probabilities of relevance can bederived and we denote them asHigh-recall is the goal of the resource selection algorithm in federated search tasks such as database recommendation .The goal is to select a small set of resources e.g. , less than Nsdb databases that contain as many relevant documents as possible , which can be formally defined as :I di is the indicator function , which is 1 when the ith database is selected and 0 otherwise .Plug this equation into the basic model in Equation 8 and associate the selected database number constraint to obtain the following : ^ ^ probabilities of relevance \u03b8 , we associate a utility function U \u03b8 , d which indicates the benefit from making the d selection when the true complete lists of probabilities of relevance are \u03b8 .Therefore , the selection decision defined by the Bayesian framework is :One common approach to simplify the computation in the Bayesian framework is to only calculate the utility function at the most probable parameter values instead of calculating the whole expectation .In other words , we only need to calculateThe solution of this optimization problem is very simple .We can calculate the expected number of relevant documents for each database as follows :The Nsdb databases with the largest expected number of relevant documents can be selected to meet the high-recall goal .We call this the UUM/HR algorithm Unified Utility Maximization for High-Recall .High-Precision is the goal of resource selection algorithm in federated search tasks such as distributed document retrieval .It is measured by the Precision at the top part of the final merged document list .This high-precision criterion is realized by the following utility function , which measures the Precision of retrieved documents from the selected databases .Note that the key difference between Equation 12 and Equation 9 is that Equation 9 sums up the probabilities of relevance of all the documents in a database , while Equation 12 only considers a much smaller part of the ranking .Specifically , we can calculate the optimal selection decision by :Different kinds of constraints caused by different characteristics of the document retrieval tasks can be associated with the above optimization problem .The most common one is to select a fixed number Nsdb of databases and retrieve a fixed number Nrdoc of documents from each selected database , formally defined as :This optimization problem can be solved easily by calculating the number of expected relevant documents in the top part of the each database 's complete list of probabilities of relevance : Then the databases can be ranked by these values and selected .We call this the UUM/HP-FL algorithm Unified Utility Maximization for High-Precision with Fixed Length document rankings from each selected database .A more complex situation is to vary the number of retrieved documents from each selected database .More specifically , we allow different selected databases to return different numbers of documents .For simplification , the result list lengths are required to be multiples of a baseline number 10 .This value can also be varied , but for simplification it is set to 10 in this paper .This restriction is set to simulate the behavior of commercial search engines on the Web .Search engines such as Google and AltaVista return only 10 or 20 document ids for every result page .This procedure saves the computation time of calculating optimal database selection by allowing the step of dynamic programming to be 10 instead of 1 more detail is discussed latterly .For further simplification , we restrict to select at mostNTotal_rdoc is the total number of documents to be retrieved .dynamic programming algorithm can be applied to calculate the optimal solution .The basic steps of this dynamic programming method are described in Figure 2 .As this algorithm allows retrieving result lists of varying lengths from each selected database , it is called UUM/HP-VL algorithm .After the selection decisions are made , the selected databases are searched and the corresponding document ids are retrieved from each database .The final step of document retrieval is to merge the returned results into a single ranked list with the semisupervised learning algorithm .It was pointed out before that the SSL algorithm maps the database-specific scores into the centralized document scores and builds the final ranked list accordingly , which is consistent with all our selection procedures where documents with higher probabilities of relevance thus higher centralized document scores are selected .It is desirable to evaluate distributed information retrieval algorithms with testbeds that closely simulate the real world applications .The TREC Web collections WT2g or WT10g 4,13 provide a way to partition documents by different Web servers .In this way , a large number O 1000 of databases with rather diversecontents could be created , which may make this testbed a good candidate to simulate the operational environments such as open domain hidden Web .However , two weakness of this testbed are : i Each database contains only a small amount of document 259 documents by average for WT2g 4 ; and ii The contents of WT2g or WT10g are arbitrarily crawled from the Web .It is not likely for a hidden Web database to provide personal homepages or web pages indicating that the pages are under construction and there is no useful information at all .These types of web pages are contained in the WT2g/WT10g datasets .Therefore , the noisy Web data is not similar with that of high-quality hidden Web database contents , which are usually organized by domain experts .Another choice is the TREC news/government data 1,15,17 , 18,21 .TREC news/government data is concentrated on relatively narrow topics .Compared with TREC Web data : i The news/government documents are much more similar to the contents provided by a topic-oriented database than an arbitrary web page , ii A database in this testbed is larger than that of TREC Web data .By average a database contains thousands of documents , which is more realistic than a database of TREC Web data with about 250 documents .As the contents and sizes of the databases in the TREC news/government testbed are more similar with that of a topic-oriented database , it is a good candidate to simulate the distributed information retrieval environments of large organizations companies or domainspecific hidden Web sites , such as West that provides access to legal , financial and news text databases 3 .As most current distributed information retrieval systems are developed for the environments of large organizations companies or domainspecific hidden Web other than open domain hidden Web , TREC news/government testbed was chosen in this work .Trec123-100col-bysource testbed is one of the most used TREC news/government testbed 1,15,17,21 .It was chosen in this work .Three testbeds in 21 with skewed database size distributions and different types of relevant document distributions were also used to give more thorough simulation for real environments .Trec123-100col-bysource : 100 databases were created from TREC CDs 1 , 2 and 3 .They were organized by source and publication date 1 .The sizes of the databases are not skewed .Details are in Table 1 .Three testbeds built in 21 were based on the trec123-100colbysource testbed .Each testbed contains many `` small '' databases and two large databases created by merging about 10-20 small databases together .decision d xyz , which represents the best selection decision in the condition : only databases from number 1 to number x are considered for selection ; totally y * 10 documents will be retrieved ; only z databases are selected out of the x database candidates .And Sel x , y , z is the corresponding utility value by choosing the best selection .ii Initialize Sel 1 , 1 ..NTotal_rdoc / 10 , 1 ..Nsdb with only the estimated relevance information of the 1st database .iii Iterate the current database candidate i from 2 to | DB |from the ith database , otherwise we should not select this database and the previous best solution Sel i-1 , y , z should be kept .Then set the value of d iyz and Sel i , y , z accordingly .environment , three different types of search engines were used in the experiments : INQUERY 2 , a unigram statistical language model with linear smoothing 12,20 and a TFIDF retrieval algorithm with `` ltc '' weight 12,20 .All these algorithms were implemented with the Lemur toolkit 12 .These three kinds of search engines were assigned to the databases among the four testbeds in a round-robin manner .All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system .The resource descriptions were created using query-based sampling .About 80 queries were sent to each database to download 300 unique documents .The database size statistics were estimated by the sample-resample method 21 .Fifty queries 101-150 were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases details in Section 3.1 .Another 50 queries 51-100 were used as test data .Trec123-2ldb-60col `` representative '' : The databases in the trec123-100col-bysource were sorted with alphabetical order .Two large databases were created by merging 20 small databases with the round-robin method .Thus , the two large databases have more relevant documents due to their large sizes , even though the densities of relevant documents are roughly the same as the small databases .Trec123-AP-WSJ-60col `` relevant '' : The 24 Associated Press collections and the 16 Wall Street Journal collections in the trec123-100col-bysource testbed were collapsed into two large databases APall and WSJall .The other 60 collections were left unchanged .The APall and WSJall databases have higher densities of documents relevant to TREC queries than the small databases .Thus , the two large databases have many more relevant documents than the small databases .Trec123-FR-DOE-81col `` nonrelevant '' : The 13 Federal Register collections and the 6 Department of Energy collections in the trec123-100col-bysource testbed were collapsed into two large databases FRall and DOEall .The other 80 collections were left unchanged .The FRall and DOEall databases have lower densities of documents relevant to TREC queries than the small databases , even though they are much larger .100 queries were created from the title fields of TREC topics 51-150 .The queries 101-150 were used as training queries and the queries 51-100 were used as test queries details in Table 2 .In the uncooperative distributed information retrieval environments of large organizations companies or domainspecific hidden Web , different databases may use different types of search engine .To simulate the multiple type-engine Resource selection algorithms of database recommendation systems are typically compared using the recall metric Rn 1,17,18,21 .Let B denote a baseline ranking , which is often the RBR relevance based ranking , and E as a ranking provided by a resource selection algorithm .And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E .Then Rn is defined as follows :Usually the goal is to search only a few databases , so our figures only show results for selecting up to 20 databases .The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms , namely the CORI , ReDDE and UUM/HR .The UUM/HR algorithm is described in Section 3.3 .It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective on the representative , relevant and nonrelevant testbeds or as good as on the Trec123-100Col testbed the CORI resource selection algorithm .The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the Trec123100Col and the nonrelevant testbeds .This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm .It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds , the ReDEE algorithm has a small advantage over the UUM/HR algorithm .We attribute this to two causes : i The ReDDE algorithm was tuned on the Trec123-100Col testbed ; and ii Although the difference is small , this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough .More training data or a more sophisticated model may help to solve this minor puzzle .For document retrieval , the selected databases are searched and the returned results are merged into a single final list .In all of the experiments discussed in this section the results retrieved from individual databases were combined by the semisupervised learning results merging algorithm .This version of the SSL algorithm 22 is allowed to download a small number of returned document texts `` on the fly '' to create additional training data in the process of learning the linear models which map database-specific document scores into estimated centralized document scores .It has been shown to be very effective in environments where only short result-lists are retrieved from each selected database 22 .This is a common scenario in operational environments and was the case for our experiments .Document retrieval effectiveness was measured by Precision at the top part of the final document list .The experiments in this section were conducted to study the document retrieval effectiveness of five selection algorithms , namely the CORI , ReDDE , UUM/HR , UUM/HP-FL and UUM/HP-VL algorithms .The last three algorithms were proposed in Section 3 .All the first four algorithms selected 3 or 5 databases , and 50 documents were retrieved from each selected database .The UUM/HP-FL algorithm also selected 3 or 5 databases , but it was allowed to adjust the number of documents to retrieve from each selected database ; the number retrieved was constrained to be from 10 to 100 , and a multiple of 10 .The Trec123-100Col and representative testbeds were selected for document retrieval as they represent two extreme cases of resource selection effectiveness ; in one case the CORI algorithm is as good as the other algorithms and in the other case it is quitea lot worse than the other algorithms .Tables 3 and 4 show the results on the Trec123-100Col testbed , and Tables 5 and 6 show the results on the representative testbed .On the Trec123-100Col testbed , the document retrieval effectiveness of the CORI selection algorithm is roughly the same or a little bit better than the ReDDE algorithm but both of them are worse than the other three algorithms Tables 3 and 4 .The UUM/HR algorithm has a small advantage over the CORI and ReDDE algorithms .One main difference between the UUM/HR algorithm and the ReDDE algorithm was pointed out before : The UUM/HR uses training data and linear interpolation to estimate the centralized document score curves , while the ReDDE algorithm 21 uses a heuristic method , assumes the centralized document score curves are step functions and makes no distinction among the top part of the curves .This difference makes UUM/HR better than the ReDDE algorithm at distinguishing documents with high probabilities of relevance from low probabilities of relevance .Therefore , the UUM/HR reflects the high-precision retrieval goal better than the ReDDE algorithm and thus is more effective for document retrieval .The UUM/HR algorithm does not explicitly optimize the selection decision with respect to the high-precision goal as the UUM/HP-FL and UUM/HP-VL algorithms are designed to do .It can be seen that on this testbed , the UUM/HP-FL and UUM/HP-VL algorithms are much more effective than all the other algorithms .This indicates that their power comes from explicitly optimizing the high-precision goal of document retrieval in Equations 14 and 16 .On the representative testbed , CORI is much less effective than other algorithms for distributed document retrieval Tables 5 and 6 .The document retrieval results of the ReDDE algorithm are better than that of the CORI algorithm but still worse than the results of the UUM/HR algorithm .On this testbed the three UUM algorithms are about equally effective .Detailed analysis shows that the overlap of the selected databases between the UUM/HR , UUM/HP-FL and UUM/HP-VL algorithms is much larger than the experiments on the Trec123-100Col testbed , since all of them tend to select the two large databases .This explains why they are about equally effective for document retrieval .In real operational environments , databases may return no document scores and report only ranked lists of results .As the unified utility maximization model only utilizes retrieval scores of sampled documents with a centralized retrieval algorithm to calculate the probabilities of relevance , it makes database selection decisions without referring to the document scores from individual databases and can be easily generalized to this case of rank lists without document scores .The only adjustment is that the SSL algorithm merges ranked lists without document scores by assigning the documents with pseudo-document scores normalized for their ranks In a ranked list of 50 documents , the first one has a score of 1 , the second has a score of 0.98 etc , which has been studied in 22 .The experiment results on trec123-100Col-bysource testbed with 3 selected databases are shown in Table 7 .The experiment setting was the same as before except that the document scores were eliminated intentionally and the selected databases only return ranked lists of document ids .It can be seen from the results that the UUM/HP-FL and UUM/HP-VL work well with databases returning no document scores and are still more effective than other alternatives .Other experiments with databases that return no document scores are not reported but they show similar results to prove the effectiveness of UUM/HP-FL and UUM/HPVL algorithms .The above experiments suggest that it is very important to optimize the high-precision goal explicitly in document retrieval .The new algorithms based on this principle achieve better or at least as good results as the prior state-of-the-art algorithms in several environments .", "conclusions": "Distributed information retrieval solves the problem of finding information that is scattered among many text databases on local area networks and Internets .Most previous research use effective resource selection algorithm of database recommendation system for distributed document retrieval application .We argue that the high-recall resource selection goal of database recommendation and high-precision goal of document retrieval are related but not identical .This kind of inconsistency has also been observed in previous work , but the prior solutions either used heuristic methods or assumed cooperation by individual databases e.g. , all the databases used the same kind of search engines , which is frequently not true in the uncooperative environment .In this work we propose a unified utility maximization model to integrate the resource selection of database recommendation and document retrieval tasks into a single unified framework .In this framework , the selection decisions are obtained by optimizing different objective functions .As far as we know , this is the first work that tries to view and theoretically model the distributed information retrieval task in an integrated manner .The new framework continues a recent research trend studying the use of query-based sampling and a centralized sample database .A single logistic model was trained on the centralized 40 sample database to estimate the probabilities of relevance of documents by their centralized retrieval scores , while the centralized sample database serves as a bridge to connect the individual databases with the centralized logistic model .Therefore , the probabilities of relevance for all the documents across the databases can be estimated with very small amount of human relevance judgment , which is much more efficient than previous methods that build a separate model for each database .This framework is not only more theoretically solid but also very effective .One algorithm for resource selection UUM/HR and two algorithms for document retrieval UUM/HP-FL and UUM/HP-VL are derived from this framework .Empirical studies have been conducted on testbeds to simulate the distributed search solutions of large organizations companies or domain-specific hidden Web .Furthermore , the UUM/HP-FL and UUM/HP-VL resource selection algorithms are extended with a variant of SSL results merging algorithm to address the distributed document retrieval task when selected databases do not return document scores .Experiments have shown that these algorithms achieve results that are at least as good as the prior state-of-the-art , and sometimes considerably better .Detailed analysis indicates that the advantage of these algorithms comes from explicitly optimizing the goals of the specific tasks .The unified utility maximization framework is open for different extensions .When cost is associated with searching the online databases , the utility framework can be adjusted to automatically estimate the best number of databases to search so that a large amount of relevant documents can be retrieved with relatively small costs .Another extension of the framework is to consider the retrieval effectiveness of the online databases , which is an important issue in the operational environments .All of these are the directions of future research ."}