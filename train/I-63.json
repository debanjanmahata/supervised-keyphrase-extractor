{"reader_keywords": ["combinatorial resource scheduling", "optimal resource scheduling", "multiagent system", "resource", "markov decision process", "resource allocation", "scheduling", "optimization problem", "utility function", "optimal allocation", "discrete-time scheduling problem", "resource-scheduling algorithm", "resource-scheduling"], "reader_keywords_stem": ["combinatori resourc schedul", "optim resourc schedul", "multiag system", "resourc", "markov decis process", "resourc alloc", "schedul", "optim problem", "util function", "optim alloc", "discret-time schedul problem", "resourc-schedul algorithm", "resourc-schedul"], "introduction": "The tasks of optimal resource allocation and scheduling are ubiquitous in multiagent systems , but solving such optimization problems can be computationally difficult , due to a number of factors .In particular , when the value of a set of resources to an agent is not additive as is often the case withresources that are substitutes or complements , the utility function might have to be defined on an exponentially large space of resource bundles , which very quickly becomes computationally intractable .Further , even when each agent has a utility function that is nonzero only on a small subset of the possible resource bundles , obtaining optimal allocation is still computationally prohibitive , as the problem becomes NP-complete 14 .Such computational issues have recently spawned several threads of work in using compact models of agents ' preferences .One idea is to use any structure present in utility functions to represent them compactly , via , for example , logical formulas 15 , 10 , 4 , 3 .An alternative is to directly model the mechanisms that define the agents ' utility functions and perform resource allocation directly with these models 9 .A way of accomplishing this is to model the processes by which an agent might utilize the resources and define the utility function as the payoff of these processes .In particular , if an agent uses resources to act in a stochastic environment , its utility function can be naturally modeled with a Markov decision process , whose action set is parameterized by the available resources .This representation can then be used to construct very efficient resource-allocation algorithms that lead to an exponential speedup over a straightforward optimization problem with flat representations of combinatorial preferences 6 , 7 , 8 .However , this existing work on resource allocation with preferences induced by resource-parameterized MDPs makes an assumption that the resources are only allocated once and are then utilized by the agents independently within their infinite-horizon MDPs .This assumption that no reallocation of resources is possible can be limiting in domains where agents arrive and depart dynamically .In this paper , we extend the work on resource allocation under MDP-induced preferences to discrete-time scheduling problems , where agents are present in the system for finite time intervals and can only use resources within these intervals .In particular , agents arrive and depart at arbitrary predefined times and within these intervals use resources to execute tasks in finite-horizon MDPs .We address the problem of globally optimal resource scheduling , where the objective is to find an allocation of resources to the agents across time that maximizes the sum of the expected rewards that they obtain .In this context , our main contribution is a mixed-integerprogramming formulation of the scheduling problem that chooses globally optimal resource assignments , starting times , and execution horizons for all agents within their arrivaldeparture intervals .We analyze and empirically compare two flavors of the scheduling problem : one , where agents have static resource assignments within their finite-horizon MDPs , and another , where resources can be dynamically reallocated between agents at every time step .In the rest of the paper , we first lay down the necessary groundwork in Section 2 and then introduce our model and formal problem statement in Section 3 .In Section 4.2 , we describe our main result , the optimization program for globally optimal resource scheduling .Following the discussion of our experimental results on a job-scheduling problem in Section 5 , we conclude in Section 6 with a discussion of possible extensions and generalizations of our method .", "title": "Combinatorial Resource Scheduling for Multiagent MDPs", "author_keywords_stem": ["task and resource allocation in agent system", "multiagent plan"], "background": "Similarly to the model used in previous work on resourceallocation with MDP-induced preferences 6 , 7 , we define the value of a set of resources to an agent as the value of the best MDP policy that is realizable , given those resources .However , since the focus of our work is on scheduling problems , and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times , we model the agents ' planning problems as finite-horizon MDPs , in contrast to previous work that used infinite-horizon discounted MDPs .In the rest of this section , we first introduce some necessary background on finite-horizon MDPs and present a linear-programming formulation that serves as the basis for our solution algorithm developed in Section 4 .We also outline the standard methods for combinatorial resource scheduling with flat resource values , which serve as a comparison benchmark for the new model developed here .A stationary , finite-domain , discrete-time MDP see , for example , 13 for a thorough and detailed development can be described as S , A , p , r , where : S is a finite set of system states ; A is a finite set of actions that are available to the agent ; p is a stationary stochastic transition function , where p \u03c3 | s , a is the probability of transitioning to state \u03c3 upon executing action a in state s ; r is a stationary reward function , where r s , a specifies the reward obtained upon executing action a in state s. Given such an MDP , a decision problem under a finite horizon T is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agent 's finite lifetime .The agent 's optimal policy is then a function of current state s and the time until the horizon .An optimal policy for such a problem is to act greedily with respect to the optimal value function , defined recursively by the following system of finite-time Bellman equations 2 :This optimal value function can be easily computed using dynamic programming , leading to the following optimal policy \u03c0 , where \u03c0 s , a , t is the probability of executing actionThe above is the most common way of computing the optimal value function and therefore an optimal policy for a finite-horizon MDP .However , we can also formulate the problem as the following linear program similarly to the dual LP for infinite-horizon discounted MDPs 13 , 6 , 7 :Note that the standard unconstrained finite-horizon MDP , as described above , always has a uniformly-optimal solution optimal for any initial distribution \u03b1 s .Therefore , an optimal policy can be obtained by using an arbitrary constant \u03b1 s > 0 in particular , \u03b1 s = 1 will result in x s , a , t = \u03c0 s , a , t .However , for MDPs with resource constraints as defined below in Section 3 , uniformly-optimal policies do not in general exist .In such cases , \u03b1 becomes a part of the problem input , and a resulting policy is only optimal for that particular \u03b1 .This result is well known for infinite-horizon MDPs with various types of constraints 1 , 6 , and it also holds for our finite-horizon model , which can be easily established via a line of reasoning completely analogous to the arguments in 6 .A straightforward approach to resource scheduling for a set of agents M , whose values for the resources are induced by stochastic planning problems in our case , finite-horizon MDPs would be to have each agent enumerate all possible resource assignments over time and , for each one , compute its value by solving the corresponding MDP .Then , each agent would provide valuations for each possible resource bundle over time to a centralized coordinator , who would compute the optimal resource assignments across time based on these valuations .When resources can be allocated at different times to different agents , each agent must submit valuations for every combination of possible time horizons .Let each agent m E M execute its MDP within the arrival-departure time interval \u03c4 E \u03c4am , \u03c4dm .Hence , agent m will execute an MDP with time horizon no greater than Tm = \u03c4d m \u2212 \u03c4 a m +1 .Let b\u03c4 be the global time horizon for the problem , before which all of the agents ' MDPs must finish .We assume \u03c4dm < b\u03c4 , Vm E M.For the scheduling problem where agents have static resource requirements within their finite-horizon MDPs , the agents provide a valuation for each resource bundle for each possible time horizon from 1 , Tm that they may use .Let \u03a9 be the set of resources to be allocated among the agents .An agent will get at most one resource bundle for one of the time horizons .Let the variable \u03c8 \u2208 \u03a8m enumerate all possible pairs of resource bundles and time horizons for agent m , so there are 2 | \u03a9 | \u00d7 Tm values for \u03c8 the space of bundles is exponential in the number of resource types | \u03a9 | .The agent m must provide a value v\u03c8m for each \u03c8 , and the coordinator will allocate at most one \u03c8 resource , time horizon pair to each agent .This allocation is expressed as an indicator variable z\u03c8m \u2208 0 , 1 that shows whether \u03c8 is assigned to agent m. For time \u03c4 and resource \u03c9 , the function nm \u03c8 , \u03c4 , \u03c9 \u2208 0 , 1 indicates whether the bundle in \u03c8 uses resource \u03c9 at time \u03c4 we make the assumption that agents have binary resource requirements .This allocation problem is NP-complete , even when considering only a single time step , and its difficulty increases significantly with multiple time steps because of the increasing number of values of \u03c8 .The problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource \u03c9 allocated to all agents does not exceed the available amount b\u03d5 \u03c9 can be expressed as the following integer program : 3 The first constraint in equation 3 says that no agent can receive more than one bundle , and the second constraint ensures that the total assignment of resource \u03c9 does not , at any time , exceed the resource bound .For the scheduling problem where the agents are able to dynamically reallocate resources , each agent must specify a value for every combination of bundles and time steps within its time horizon .Let the variable \u03c8 \u2208 \u03a8m in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step .Therefore , in this case there are Pt \u2208 1 , T 2 | \u03a9 | t \u223c 2 | \u03a9 | Tpossibilities of resource bundles assigned to different time slots , for the Tm different time horizons .The same set of equations 3 can be used to solve this dynamic scheduling problem , but the integer program is different because of the difference in how \u03c8 is defined .In this case , the number of \u03c8 values is exponential in each agent 's planning horizon Tm , resulting in a much larger program .This straightforward approach to solving both of these of either 2 | \u03a9 | Tm static allocation or P scheduling problems requires an enumeration and solution t \u2208 1 , T 2 | \u03a9 | t dynamic reallocation MDPs for each agent , which very quickly becomes intractable with the growth of the number of resources | \u03a9 | or the time horizon Tm .", "abstract": "Optimal resource scheduling in multiagent systems is a computationally challenging task , particularly when the values of resources are not additive .We consider the combinatorial problem of scheduling the usage of multiple resources among agents that operate in stochastic environments , modeled as Markov decision processes MDPs .In recent years , efficient resource-allocation algorithms have been developed for agents with resource values induced by MDPs .However , this prior work has focused on static resource-allocation problems where resources are distributed once and then utilized in infinite-horizon MDPs .We extend those existing models to the problem of combinatorial resource scheduling , where agents persist only for finite periods between their predefined arrival and departure times , requiring resources only for those time periods .We provide a computationally efficient procedure for computing globally optimal resource assignments to agents over time .We illustrate and empirically analyze the method in the context of a stochastic jobscheduling domain .", "id": "I-63", "combined_keywords_stem": ["combinatori resourc schedul", "optim resourc schedul", "multiag system", "resourc", "markov decis process", "resourc alloc", "schedul", "optim problem", "util function", "optim alloc", "discret-time schedul problem", "resourc-schedul algorithm", "resourc-schedul", "task and resourc alloc in agent system", "multiag plan"], "evaluation": "Although the complexity of solving MILPs is in the worst case exponential in the number of integer variables , there are many efficient methods for solving MILPs that allow our algorithm to scale well for parameters common to resource allocation and scheduling problems .In particular , this section introduces a problem domain the repairshop problem used to empirically evaluate our algorithm 's scalability in terms of the number of agents IMI , the number of shared resources I\u03a9I , and the varied lengths of global time b-r during which agents may enter and exit the system .The repairshop problem is a simple parameterized MDP adopting the metaphor of a vehicular repair shop .Agents in the repair shop are mechanics with a number of independent tasks that yield reward only when completed .In our MDP model of this system , actions taken to advance through the state space are only allowed if the agent holds certain resources that are publicly available to the shop .These resources are in finite supply , and optimal policies for the shop will determine when each agent may hold the limited resources to take actions and earn individual rewards .Each task to be completed is associated with a single action , although the agent is required to repeat the action numerous times before completing the task and earning a reward .This model was parameterized in terms of the number of agents in the system , the number of different types of resources that could be linked to necessary actions , a global time during which agents are allowed to arrive and depart , and a maximum length for the number of time steps an agent may remain in the system .All datapoints in our experiments were obtained with 20 evaluations using CPLEX to solve the MILPs on a Pentium4 computer with 2Gb of RAM .Trials were conducted on both the static and the dynamic version of the resourcescheduling problem , as defined earlier .Figure 3 shows the runtime and policy value for independent modifications to the parameter set .The top row shows how the solution time for the MILP scales as we increase the number of agents IMI , the global time horizon b-r , and the number of resources I\u03a9I .Increasing the number of agents leads to exponential complexity scaling , which is to be expected for an NP-complete problem .However , increasing the global time limit b-r or the total number of resource types I\u03a9I while holding the number of agents constant does not lead to decreased performance .This occurs because the problems get easier as they become under-constrained , which is also a common phenomenon for NP-complete problems .We also observe that the solution to the dynamic version of the problem can often be computed much faster than the static version .The bottom row of Figure 3 shows the joint policy value of the policies that correspond to the computed optimal resource-allocation schedules .We can observe that the dynamic version yields higher reward as expected , since the reward for the dynamic version is always no less than the reward of the static version .We should point out that these graphs should not be viewed as a measure of performance of two different algorithms both algorithms produce optimal solutions but to different problems , but rather as observations about how the quality of optimal solutions change as more flexibility is allowed in the reallocation of resources .Figure 4 shows runtime and policy value for trials in which common input variables are scaled together .This allowsus to explore domains where the total number of agents scales proportionally to the total number of resource types or the global time horizon , while keeping constant the average agent density per unit of global time or the average number of resources per agent which commonly occurs in real-life applications .Overall , we believe that these experimental results indicate that our MILP formulation can be used to effectively solve resource-scheduling problems of nontrivial size .", "combined_keywords": ["combinatorial resource scheduling", "optimal resource scheduling", "multiagent system", "resource", "markov decision process", "resource allocation", "scheduling", "optimization problem", "utility function", "optimal allocation", "discrete-time scheduling problem", "resource-scheduling algorithm", "resource-scheduling", "task and resource allocation in agent system", "multiagent plan"], "author_keywords": ["task and resource allocation in agent system", "multiagent plan"], "method": "We now formally introduce our model of the resourcescheduling problem .The problem input consists of the following components :Given the above input , the optimization problem we consider is to find the globally optimal maximizing the sum of expected rewards mapping of resources to agents for all time steps : \u0394 : \u03c4 \u00d7 M \u00d7 \u03a9 ~ \u2192 0 , 1 .A solution is feasible if the corresponding assignment of resources to the agents does not violate the global resource constraint :We consider two flavors of the resource-scheduling problem .The first formulation restricts resource assignments to the space where the allocation of resources to each agent is static during the agent 's lifetime .The second formulation allows reassignment of resources between agents at every time step within their lifetimes .Figure 1 depicts a resource-scheduling problem with three agents M = m1 , m2 , m3 , three resources \u03a9 = \u03c91 , \u03c92 , \u03c93 , and a global problem horizon of b\u03c4 = 11 .The agents ' arrival and departure times are shown as gray boxes and are 1 , 6 , 3 , 7 , and 2 , 11 , respectively .A solution to this problem is shown via horizontal bars within each agents ' box , where the bars correspond to the allocation of the three resource types .Figure 1a shows a solution to a static scheduling problem .According to the shown solution , agent m1 begins the execution of its MDP at time \u03c4 = 1 and has a lock on all three resources until it finishes execution at time \u03c4 = 3 .Note that agent m1 relinquishes its hold on the resources before its announced departure time of \u03c4dm1 = 6 , ostensibly because other agents can utilize the resources more effectively .Thus , at time \u03c4 = 4 , resources \u03c91 and \u03c93 are allocated to agent m2 , who then uses them to execute its MDP using only actions supported by resources \u03c91 and \u03c93 until time \u03c4 = 7 .Agent m3 holds resource \u03c93 during the interval \u03c4 \u2208 4 , 10 .Figure 1b shows a possible solution to the dynamic version of the same problem .There , resources can be reallocated between agents at every time step .For example , agent m1 gives up its use of resource \u03c92 at time \u03c4 = 2 , although it continues the execution of its MDP until time \u03c4 = 6 .Notice that an agent is not allowed to stop and restart its MDP , so agent m1 is only able to continue executing in the interval \u03c4 \u2208 3 , 4 if it has actions that do not require any resources \u03d5m a , \u03c9 = 0 .Clearly , the model and problem statement described above make a number of assumptions about the problem and the desired solution properties .We discuss some of those assumptions and their implications in Section 6 .Our resource-scheduling algorithm proceeds in two stages .First , we perform a preprocessing step that augments the agent MDPs ; this process is described in Section 4.1 .Second , using these augmented MDPs we construct a global optimization problem , which is described in Section 4.2 .In the model described in the previous section , we assume that if an agent does not possess the necessary resources to perform actions in its MDP , its execution is halted and the agent leaves the system .In other words , the MDPs can not be `` paused '' and `` resumed '' .For example , in the problem shown in Figure 1a , agent m1 releases all resources after time \u03c4 = 3 , at which point the execution of its MDP is halted .Similarly , agents m2 and m3 only execute their MDPs in the intervals \u03c4 E 4 , 6 and \u03c4 E 4 , 10 , respectively .Therefore , an important part of the global decision-making problem is to decide the window of time during which each of the agents is `` active '' i.e. , executing its MDP .To accomplish this , we augment each agent 's MDP with two new states `` start '' and `` finish '' states sb , sf , respectively and a new `` start/stop '' action a ` , as illustrated in Figure 2 .The idea is that an agent stays in the start state sb until it is ready to execute its MDP , at which point it performs the start/stop action a ` and transitions into the state space of the original MDP with the transition probability that corresponds to the original initial distribution \u03b1 s .For example , in Figure 1a , for agent m2 this would happen at time \u03c4 = 4 .Once the agent gets to the end of its activity window time \u03c4 = 6 for agent m2 in Figure 1a , it performs the start/stop action , which takes it into the sink finish state sf at time \u03c4 = 7 .More precisely , given an MDP S , A , pm , rm , \u03b1m , we define an augmented MDP S ' , A ' , p ' m , r ' m , \u03b1 'm as follows :where all non-specified transition probabilities are assumed to be zero .Further , in order to account for the new starting state , we begin the MDP one time-step earlier , setting \u03c4am + \u03c4am \u2212 1 .This will not affect the resource allocation due to the resource constraints only being enforced for the original MDP states , as will be discussed in the next section .For example , the augmented MDPs shown in Figure 2b which starts in state sb at time \u03c4 = 2 would be constructed from an MDP with original arrival time \u03c4 = 3 .Figure 2b also shows a sample trajectory through the state space : the agent starts in state sb , transitions into the state space S of the original MDP , and finally exists into the sink state sf .Note that if we wanted to model a problem where agents could pause their MDPs at arbitrary time steps which might be useful for domains where dynamic reallocation is possible , we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward .Given a set of augmented MDPs , as defined above , the goal of this section is to formulate a global optimization program that solves the resource-scheduling problem .In this section and below , all MDPs are assumed to be the augmented MDPs as defined in Section 4.1 .Our approach is similar to the idea used in 6 : we begin with the linear-program formulation of agents ' MDPs 1 and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid .The resulting optimization problem then simultaneously solves the agents ' MDPs and resource-scheduling problems .In the rest of this section , we incrementally develop a mixed integer program MILP that achieves this .In the absence of resource constraints , the agents ' finitehorizon MDPs are completely independent , and the globally optimal solution can be trivially obtained via the following LP , which is simply an aggregation of single-agent finitehorizon LPs :where xm s , a , t is the occupation measure of agent m , andTm = \u03c4dm \u2212 \u03c4am + 1 is the time horizon for the agent 's MDP .Using this LP as a basis , we augment it with constraints that ensure that the resource usage implied by the agents ' occupation measures xm does not violate the global resource requirements \u03d5b at any time step \u03c4 \u2208 0 , b\u03c4 .To formulate these resource constraints , we use the following binary variables :indicators 0 , as shown in 6 in Table 1 .Another constraint we have to add because the activity indicators 0 are defined on the global timeline r is to enforce the fact that the agent is inactive outside of its arrivaldeparture window .This is accomplished by constraint 7 in Table 1 .Furthermore , agents should not be using resources while they are inactive .This constraint can also be enforced via a linear inequality on 0 and \u0394 , as shown in 8 .Constraint 6 sets the value of 0 to match the policy defined by the occupation measure x _ .In a similar fashion , we have to make sure that the resource-usage variables \u0394 are also synchronized with the occupation measure x _ .This is done via constraint 9 in Table 1 , which is nearly identical to the analogous constraint from 6 .After implementing the above constraint , which enforces the meaning of \u0394 , we add a constraint that ensures that the agents ' resource usage never exceeds the amounts of available resources .This condition is also trivially expressed as a linear inequality 10 in Table 1 .Finally , for the problem formulation where resource assignments are static during a lifetime of an agent , we add a constraint that ensures that the resource-usage variables \u0394 do not change their value while the agent is active 0 = 1 .This is accomplished via the linear constraint 11 , where Z > 2 is a constant that is used to turn off the constraints when 0 _ r = 0 or 0 _ r + 1 = 0 .This constraint is not used for the dynamic problem formulation , where resources can be reallocated between agents at every time step .To summarize , Table 1 together with the conservationof-flow constraints from 12 defines the MILP that simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon MDP policies that are valid under that resource assignment .As a rough measure of the complexity of this MILP , let us consider the number of optimization variables and constraints .Let TM = ET _ = E _ r ' _ rd _ + 1 be the sum of the lengths of the arrival-departure windows across all agents .Then , the number of optimization variables is :TM of which are continuous x _ , and b-rIMII\u03a9I + b-rIMI are binary \u0394 and 0 .However , notice that all but TMIMI of the 0 are set to zero by constraint 7 , which also immediately forces all but TMIMII\u03a9I of the \u0394 to be zero via the constraints 8 .The number of constraints not including the degenerate constraints in 7 in the MILP is :Despite the fact that the complexity of the MILP is , in the worst case , exponential1 in the number of binary variables , the complexity of this MILP is significantly exponentially lower than that of the MILP with flat utility functions , described in Section 2.2 .This result echos the efficiency gains reported in 6 for single-shot resource-allocation problems , but is much more pronounced , because of the explosion of the flat utility representation due to the temporal aspect of the problem recall the prohibitive complexity of the combinatorial optimization in Section 2.2 .We empirically analyze the performance of this method in Section 5 ."}